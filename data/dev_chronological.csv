Unnamed: 0.2,Unnamed: 0.1,Unnamed: 0,paper_id,actual_ids,cited_paper_id,citation_text,intent,gen_id,citing_paper_abstracts,cited_paper_abstracts,split,triple_type,proposal,new_split,keyword
0,53,79,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous. Following Gong et al. (2018) , we consider two document collections heterogeneous if their documents differ systematically with respect to vocabulary and / or level of abstraction.",Motivation,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
1,55,81,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"The extent to which this information is used by Gong et al. (2018) is not entirely clear, so we experiment with several setups (cf. Section 4).",Motivation,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
2,56,82,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,We presented a simple method for semantic matching of documents from heterogeneous collections as a solution to the Concept-Project matching task by Gong et al. (2018) .,Motivation,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
3,57,83,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"With these defining differences, there often also comes a difference in length, which, however, by itself does not make document collections heterogeneous. Following Gong et al. (2018) , we consider two document collections heterogeneous if their documents differ systematically with respect to vocabulary and / or level of abstraction.",Background,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
4,58,84,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,**GONG ET AL. (2018)'S APPROACH** The approach by Gong et al. (2018) is based on the idea that the longer document in the pair is reduced to a set of topics which capture the essence of the document in a way that eliminates the effect of a potential length difference.,Background,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
5,59,85,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,66021a920001bc3e6258bffe7076d647614147b7,"They do not, however, provide a much simpler averaging-based baseline.",Background,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ""travel"" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
6,60,86,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"Gong et al. (2018) use two different sets of word embeddings: One (topic wiki) was trained on a full English Wikipedia dump, the other (wiki science) on a smaller subset of the former dump which only contained science articles.",Background,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
7,61,87,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"Gong et al. (2018) motivate their approach mainly with the length mismatch argument, which they claim makes approaches relying on document representations (incl. vector averaging) unsuitable.",Background,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
8,62,88,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,66021a920001bc3e6258bffe7076d647614147b7,"As a second baseline, they use Word Mover's Distance (Kusner et al. (2015) ), which is based on word-level distances, rather than distance of global document representations, but which also fails to be competitive with their topic-based method.",Background,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","We present the Word Mover's Distance (WMD), a novel distance function between text documents. Our work is based on recent results in word embeddings that learn semantically meaningful representations for words from local cooccurrences in sentences. The WMD distance measures the dissimilarity between two text documents as the minimum amount of distance that the embedded words of one document need to ""travel"" to reach the embedded words of another document. We show that this distance metric can be cast as an instance of the Earth Mover's Distance, a well studied transportation problem for which several highly efficient solvers have been developed. Our metric has no hyperparameters and is straight-forward to implement. Further, we demonstrate on eight real world document classification data sets, in comparison with seven state-of-the-art baselines, that the WMD metric leads to unprecedented low k-nearest neighbor document classification error rates.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
9,63,89,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,Parameter tuning experiments were performed on a random subset of 20% of our data set (54% positive). Note that Gong et al. (2018) used only 10% of their 537 instances data set as tuning data.,Difference,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
10,64,90,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"Since the original data split used by Gong et al. (2018) is unknown, we cannot exactly replicate their settings, but we also perform ten runs using randomly selected 10% of our 408 instances test data set, and report average P, R, F, and standard deviation.",Difference,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
11,65,91,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"We demonstrate our method with the Concept-Project matching task (Gong et al. (2018) ), which is described in the next section.",Uses,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,1,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
12,66,92,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"The annotation was done by undergrad engineering students. Gong et al. (2018) do not provide any specification, or annotation guidelines, of the semantics of the 'matches' relation to be annotated. Instead, they create gold standard annotations based on a majority vote of three manual annotations.",Difference,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
13,67,93,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"8 This is the more important as we exclusively employ off-the-shelf, general-purpose embeddings, while Gong et al. (2018) reach their best results with a much more sophisticated system and with embeddings that were custom-trained for the science domain.",Difference,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
14,68,94,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"Thus, while the performance of our proposed TOP n COS SIM AVG method is superior to the approach by Gong et al. (2018) , it is itself outperformed by the 'baseline' AVG COS SIM method with appropriate weighting.",Difference,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
15,69,95,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"Another result is that, contrary to the claim made by Gong et al. (2018) , the standard averaging approach does indeed work very well even for heterogeneous document collections, if appropriate weighting is applied.",Difference,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
16,70,96,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"This result corroborates our findings on the tuning data, and clearly contradicts the (implicit) claim made by Gong et al. (2018) regarding the infeasibility of document-level matching for documents of different lengths.",Difference,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
17,71,97,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"The extent to which this information is used by Gong et al. (2018) is not entirely clear, so we experiment with several setups (cf. Section 4).",Extention,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,1,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
18,72,98,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"The extent to which this information is used by Gong et al. (2018) is not entirely clear, so we experiment with several setups (cf. Section 4).",Unsure,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
19,73,99,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,We implement this standard measure (AVG COS SIM) as a baseline for both our method and for the method by Gong et al. (2018) .,Extention,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,1,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
20,74,100,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,Note that our Both setting is probably the one most similar to the concept input used by Gong et al. (2018) .,Similar,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
21,75,101,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,4824c992c58d9375fe3a7991eeacdd7d68032541,"The second, more important finding is that our proposed TOP n COS SIM AVG measure is also very competitive, as it also outperforms both systems by Gong et al. (2018) in two out of three settings.",Similar,s2,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","Measuring similarity between texts is an important task for several applications. Available approaches to measure document similarity are inadequate for document pairs that have non-comparable lengths, such as a long document and its summary. This is because of the lexical, contextual and the abstraction gaps between a long document of rich details and its concise summary of abstract information. In this paper, we present a document matching approach to bridge this gap, by comparing the texts in a common space of hidden topics. We evaluate the matching algorithm on two matching tasks and find that it consistently and widely outperforms strong baselines. We also highlight the benefits of the incorporation of domain knowledge to text matching.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
22,81,126,ABC_cf2cc67035107f5bdaab85a760e56e_1,ARXIV:1906.09912,8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731,"Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens.",Background,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.",train,0,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
23,82,127,ABC_cf2cc67035107f5bdaab85a760e56e_1,ARXIV:1906.09912,0fe73c19513dfd17372d8ef58da0d0149725832c,"We used fastText pretrained embeddings introduced in (Bojanowski et al., 2017 ) and (Grave et al., 2018) , which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively.",Uses,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.",train,1,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
24,83,128,ABC_cf2cc67035107f5bdaab85a760e56e_1,ARXIV:1906.09912,8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731,"Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens.",Motivation,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.",train,0,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
25,250,345,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Figure 1 illustrates the results of (Bolukbasi et al. 2016 ) bias detection algorithm for a list of profession names with word embedding vectors of dimension 256 trained using the Skipgram algorithm (Mikolov et al. 2013 ) on a sample of 23k Wikipedia articles with 50k term vocabulary. As one can observe, this bias measure identifies some profession names such as commander and nurse, which historically were predominantly male and female jobs, respectively. Moreover, we trained word embeddings with differing dimension on the same sampled Wikipedia corpus using Skip-gram algorithm, and calculated Kendall tau rank correlation coefficients for the bias metrics corresponding to terms in the above list of professions. As illustrated in Figure 2 , we found that though rankings for low dimensional embeddings were unstable, for larger dimensions (≥ 128) rankings of the biases of the profession terms achieve superior Kendall tau scores.",Difference,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
26,251,346,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"While it is the case that the bias metrics in (Bolukbasi et al. 2016 ) may provide meaningful rankings of corpora when controlling for model hyper-parameter configuration, publishing the average absolute value of the metric without a complete account for model configuration is suspect.",Difference,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
27,252,347,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Next, we evaluated the stability of the direct bias measure proposed in (Bolukbasi et al. 2016) . The authors assert that the direct bias measure can be used as a metric to conclude how much an embedding is biased. For instance, for word embeddings trained on Google News articles, they reported that direct gender bias on 327 profession names is 0.08 and thus they concluded that this embedding is biased. However, as we have illustrated in Figure 3a , this bias score is not stable, i.e., direct bias measure decays exponentially with increasing word embedding dimension.",Difference,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
28,253,348,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"For instance, using such a bias measure, (Bolukbasi et al. 2016) concluded that ""word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent"".",Background,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
29,254,349,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"In this section, we evaluate the stability of the bias measure developed in (Bolukbasi et al. 2016) which is claimed to measure societal biases in word embeddings.",Background,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
30,255,350,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"We conclude that while meta analyses of the bias metrics proposed by (Bolukbasi et al. 2016) indicate that the metrics capture and somewhat quantify sociologically meaningful biases present in learned embedding spaces, the metrics are highly sensitive to the hyperparameter configurations of the algorithms used to learn them.",Difference,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
31,256,351,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"For the sake of com-pleteness, we repeat the definition of (Bolukbasi et al. 2016 )'s bias metric:",Background,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
32,257,352,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"While it is not difficult to hypothesize various words which are supposed to be either ideally neutral terms W , or ideally bias-axis-aligned G 1 , G 2 , or as in (Bolukbasi et al. 2016) , have these term sets evaluated by a crowd, it is much more difficult to argue the canonicity of a given term set W , G 1 or G 2 . If it is not possible to defend the term set selections used to define the metric as being canonical, it is better for them to be mathematically regarded as a sample of the canonical term set.",Unsure,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
33,258,353,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"Initial attempts have been made to develop metrics which seek to describe the geometric properties of the embedding space with respect to various axes of interest which are empirically determined to correspond to our intuitions of the hypothetical biases under study to quantify the degrees to which various biases exist within the embedding space, and presumably, the underlying text corpus (Bolukbasi et al. 2016; Caliskan, Bryson, and Narayanan 2017; Garg et al. 2018) .",Background,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
34,259,354,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"In this section, we evaluate the stability of the bias measure developed in (Bolukbasi et al. 2016) which is claimed to measure societal biases in word embeddings.",Motivation,s2,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
35,272,486,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"We use the set of occupation words defined in the WinoBias corpus and their assignments as prototypically male or female (Zhao et al., 2018a) .",Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
36,273,487,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"To visualize the gender subspace, we pick a few sentence pairs from WinoBias (Zhao et al., 2018a) .",Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
37,274,488,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"Specifically, we evaluate a state-of-the-art coreference resolution system ) that makes use of ELMo's contextual embeddings on WinoBias (Zhao et al., 2018a) , a coreference diagnostic dataset that evaluates whether systems behave differently on decisions involving male and female entities of stereotyped or anti-stereotyped occupations.",Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
38,275,489,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"Then we split all such instances into training and test, with 539 and 62 instances, respectively and augment these sentences by swapping all the gendered words with words of the opposite gender such that the numbers of male 1 We use the list collected in (Zhao et al., 2018a) and female entities are balanced.",Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
39,276,490,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"We explore two different strategies: (1) a training-time data augmentation technique (Zhao et al., 2018a) , where we augment the corpus for training the coreference system with its genderswapped variant (female entities are swapped to male entities and vice versa) and, afterwards, retrain the coreference system; and (2)",Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
40,277,491,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,ELMo improves the performance on the OntoNotes dataset by 5% but shows stronger bias on the WinoBias dataset.,Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
41,278,492,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,ebe84feeed3cf6a297f5a2fa504647e3eeba05b5,"We evaluate bias with respect to the WinoBias dataset (Zhao et al., 2018a) , a benchmark of paired male and female coreference resolution examples following the Winograd format (Hirst, 1981; Rahman and Ng, 2012; Peng et al., 2015) .",Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Coreference resolution is a key problem in natural language understanding that still escapes reliable solutions. One fundamental difficulty has been that of resolving instances involving pronouns since they often require deep language understanding and use of background knowledge. In this paper we propose an algorithmic solution that involves a new representation for the knowledge required to address hard coreference problems, along with a constrained optimization framework that uses this knowledge in coreference decision making. Our representation, Predicate Schemas, is instantiated with knowledge acquired in an unsupervised way, and is compiled automatically into constraints that impact the coreference decision. We present a general coreference resolution system that significantly improves state-of-the-art performance on hard, Winograd-style, pronoun resolution cases, while still performing at the stateof-the-art level on standard coreference resolution datasets.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
42,279,493,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes.",Background,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
43,280,494,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,8ae1af4a424f5e464d46903bc3d18fe1cf1434ff,"In addition, they find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from Bolukbasi et al. (2016) .",Background,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
44,281,495,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"Previous work (Zhao et al., 2018a) evaluated the systems based on GloVe embeddings but here we evaluate a state-of-the-art system that trained on the OntoNotes corpus with ELMo embeddings .",Background,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
45,282,496,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"For example, Zhao et al. (2018a) and Rudinger et al. (2018) show that coreference resolution systems relying on word embeddings encode such occupational stereotypes. In contrast, we analyze bias in contextualized word representations and its effect on a downstream task.",Difference,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
46,283,497,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,Zhao et al. (2018a) propose a method to reduce gender bias in coreference resolution by augmenting the training corpus for this task.,Background,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
47,285,499,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,babbf74939612ee2f0203c30a190b4b95881415b,"Previous work (Zhao et al., 2018a) evaluated the systems based on GloVe embeddings but here we evaluate a state-of-the-art system that trained on the OntoNotes corpus with ELMo embeddings .",Motivation,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
48,569,1141,ABC_b27150a3506730c61dc78b3034887e_6,ACL:S19-2175,df2b0e26d0599ce3e70df8a9da02e51594e0e992,We use the same tokenization method and embeddings as Devlin et al. (2018) to represent the words.,Uses,s2,"This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",train,1,"The research problem is the detection of hyperpartisan news, specifically within the context of the SemEval-2019 Task 4 competition. The motivation behind this research is not explicitly stated in the abstract, but it can be inferred that the researchers were aiming to participate in and contribute to the SemEval-2019 Task 4 competition. The research is likely driven by an interest in developing and improving techniques for identifying hyperpartisan news.",train,"classification task, held-out test dataset, news domain, pretraining"
49,570,1142,ABC_b27150a3506730c61dc78b3034887e_6,ACL:S19-2175,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"For Masked LM task, we follow the same approach with Devlin et al. (2018) .",Uses,s2,"This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",train,1,"The research problem is the detection of hyperpartisan news, specifically within the context of the SemEval-2019 Task 4 competition. The motivation behind this research is not explicitly stated in the abstract, but it can be inferred that the researchers were aiming to participate in and contribute to the SemEval-2019 Task 4 competition. The research is likely driven by an interest in developing and improving techniques for identifying hyperpartisan news.",train,"classification task, held-out test dataset, news domain, pretraining"
50,571,1143,ABC_b27150a3506730c61dc78b3034887e_6,ACL:S19-2175,3febb2bed8865945e7fddc99efd791887bb7e14f,"In earlier work, people mainly used the ""bag of words"" approach in algorithms such as Naive Bayes, Decision Tree, and SVM. However, recent studies (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018) showed that contextual word embeddings perform quite better than traditional word embeddings in many different NLP tasks as a result of their superior capacity of meaning representation.",Background,s2,"This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.","We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",train,0,"The research problem is the detection of hyperpartisan news, specifically within the context of the SemEval-2019 Task 4 competition. The motivation behind this research is not explicitly stated in the abstract, but it can be inferred that the researchers were aiming to participate in and contribute to the SemEval-2019 Task 4 competition. The research is likely driven by an interest in developing and improving techniques for identifying hyperpartisan news.",train,"classification task, held-out test dataset, news domain, pretraining"
51,572,1144,ABC_b27150a3506730c61dc78b3034887e_6,ACL:S19-2175,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"Devlin et al. (2018) introduced two unsupervised tasks to pretrain this architecture, Next Sentence Prediction and Masked Language Modeling.",Background,s2,"This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",train,0,"The research problem is the detection of hyperpartisan news, specifically within the context of the SemEval-2019 Task 4 competition. The motivation behind this research is not explicitly stated in the abstract, but it can be inferred that the researchers were aiming to participate in and contribute to the SemEval-2019 Task 4 competition. The research is likely driven by an interest in developing and improving techniques for identifying hyperpartisan news.",train,"classification task, held-out test dataset, news domain, pretraining"
52,573,1145,ABC_b27150a3506730c61dc78b3034887e_6,ACL:S19-2175,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"Among those, BERT attracts researchers most because of (i) its transformer based architecture enabling faster training and (ii) state of the art results in many different tasks. Though it is quite new, BERT has been tried in many different domains than the one proposed in Devlin et al. (2018) .",Background,s2,"This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",train,0,"The research problem is the detection of hyperpartisan news, specifically within the context of the SemEval-2019 Task 4 competition. The motivation behind this research is not explicitly stated in the abstract, but it can be inferred that the researchers were aiming to participate in and contribute to the SemEval-2019 Task 4 competition. The research is likely driven by an interest in developing and improving techniques for identifying hyperpartisan news.",train,"classification task, held-out test dataset, news domain, pretraining"
53,574,1146,ABC_b27150a3506730c61dc78b3034887e_6,ACL:S19-2175,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"Though it is quite new, BERT has been tried in many different domains than the one proposed in Devlin et al. (2018) . However, almost all of these studies have two things in common: they don't start training BERT from scratch and the target domain contains very limited data (Zhu et al., 2018; Yang et al., 2019; Alberti et al., 2019) . In this study, on the other hand, we address (1) the performance of BERT by comparing its domain specific pre-trained and fine-tuned performances, and (2) in the setting where the target domain has extensively more data.",Motivation,s2,"This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",train,0,"The research problem is the detection of hyperpartisan news, specifically within the context of the SemEval-2019 Task 4 competition. The motivation behind this research is not explicitly stated in the abstract, but it can be inferred that the researchers were aiming to participate in and contribute to the SemEval-2019 Task 4 competition. The research is likely driven by an interest in developing and improving techniques for identifying hyperpartisan news.",train,"classification task, held-out test dataset, news domain, pretraining"
54,575,1147,ABC_b27150a3506730c61dc78b3034887e_6,ACL:S19-2175,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"At the end of pretraining data generation process, we accumulated near 3.5 million samples, only running the process once on our train split, so without any duplication unlike Devlin et al. (2018) because of time restrictions.",Difference,s2,"This paper describes our system for SemEval-2019 Task 4: Hyperpartisan News Detection (Kiesel et al., 2019). We use pretrained BERT (Devlin et al., 2018) architecture and investigate the effect of different fine tuning regimes on the final classification task. We show that additional pretraining on news domain improves the performance on the Hyperpartisan News Detection task. Our system ranked 8th out of 42 teams with 78.3% accuracy on the held-out test dataset.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",train,0,"The research problem is the detection of hyperpartisan news, specifically within the context of the SemEval-2019 Task 4 competition. The motivation behind this research is not explicitly stated in the abstract, but it can be inferred that the researchers were aiming to participate in and contribute to the SemEval-2019 Task 4 competition. The research is likely driven by an interest in developing and improving techniques for identifying hyperpartisan news.",train,"classification task, held-out test dataset, news domain, pretraining"
55,1438,2762,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b; Ma et al., 2019) have demonstrated large gains in accuracy on the VLN task.",Background,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",train,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
56,1439,2764,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"We then use the same visual attention mechanism as in Fried et al. (2018b) and Ma et al. (2019) to obtain an attended object representation x obj,att over these {x obj,j } vectors.",Similar,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",train,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
57,1440,2765,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b; Ma et al., 2019) .",Motivation,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",train,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
58,1441,2766,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"In this paper, we show that the same trends hold for two recent state-of-the-art architectures (Ma et al., 2019; Fried et al., 2018b) for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues.",Similar,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",train,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
59,1442,2767,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,29e13746fa5aed13e51558a521a39aaeaa99c1b1,"In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic ""follower"" model from the Speaker-Follower (SF) system of Fried et al. (2018b) and the Self-Monitoring (SM) model of Ma et al. (2019) .",Similar,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at this https URL .",train,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
60,1443,2768,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,29e13746fa5aed13e51558a521a39aaeaa99c1b1,"The Speaker-Follower (SF) model (Fried et al., 2018b ) and the Self-Monitoring (SM) model (Ma et al., 2019) which we analyze both use sequenceto-sequence model (Cho et al., 2014) with attention (Bahdanau et al., 2015) as their base instruction-following agent.",Similar,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at this https URL .",train,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
61,1444,2769,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,29e13746fa5aed13e51558a521a39aaeaa99c1b1,"In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic ""follower"" model from the Speaker-Follower (SF) system of Fried et al. (2018b) and the Self-Monitoring (SM) model of Ma et al. (2019) .",Uses,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at this https URL .",train,1,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
62,1446,2771,ABC_0d798fcdee6ee5722d6dc5638210c2_15,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"We then use the same visual attention mechanism as in Fried et al. (2018b) and Ma et al. (2019) to obtain an attended object representation x obj,att over these {x obj,j } vectors.",Uses,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",train,1,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
63,1453,2812,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"Recent work using word embeddings-low-dimensional vector representations of words trained on large datasets to capture key semantic informationhas demonstrated that language encodes several gender, racial, and other common contemporary biases that correlate with both implicit biases (Caliskan et al., 2017) and macro-scale historical trends (Garg et al., 2018) . For example, the historical biases presented in (Garg et al., 2018) are computed using decade-specific word embeddings produced by training different Word2Vec (Mikolov et al., 2013 ) models on a large corpus of historical text from that decade.",Background,s2,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,0,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
64,1454,2813,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,305da31110b22cb878914bfcb325d537fe88abb9,"To validate our model, we compare our results to those produced via the decade-by-decade models trained in (Garg et al., 2018) using the Corpus of Historical American English (Davies, 2010) . In particular, we compute linguistic bias scores for two analyses presented in (Garg et al., 2018) : the extent to which female versus male words are semantically similar to occupation-related words, and the extent to which Asian vs. White last names are semantically similar to the same, from 1910 through 1990.",Similar,s2,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","The 400 million word Corpus of Historical American English (1810–2009) provides researchers with an extremely robust set of data for Late Modern English. The corpus is composed of fiction, magazines, newspapers, and nonfiction books, and its genre balance stays roughly the same from decade to decade. Because of its size and its advanced architecture and interface, it allows researchers to look at an extremely wide range of changes – many of which would not be possible with a small 2–4 million word corpus. These include the frequency of any word or phrase by decade and mass comparison of all words in different periods (to examine lexical changes), morphological shifts (via wildcards and pattern matching), syntactic shifts (due to very accurate lemmatization and part of speech tagging), and semantic change (by comparing collocates over time, as well as searches that use data from the integrated thesaurus and customized word lists).",train,0,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
65,1455,2814,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"Finally, we define bias towards refugees similar to how the authors of (Garg et al., 2018) define bias against Asians during the 20th century, measuring to what extent radio shows associate ""outsider"" adjectives like ""aggressive"", ""frightening"", ""illegal"", etc. To compute refugee bias scores with respect to the attribute set A, we use the relative norm distance metric from (Garg et al., 2018) :",Similar,s2,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,0,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
66,1456,2816,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,305da31110b22cb878914bfcb325d537fe88abb9,"To validate our model, we compare our results to those produced via the decade-by-decade models trained in (Garg et al., 2018) using the Corpus of Historical American English (Davies, 2010) . In particular, we compute linguistic bias scores for two analyses presented in (Garg et al., 2018) : the extent to which female versus male words are semantically similar to occupation-related words, and the extent to which Asian vs. White last names are semantically similar to the same, from 1910 through 1990.",Uses,s2,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","The 400 million word Corpus of Historical American English (1810–2009) provides researchers with an extremely robust set of data for Late Modern English. The corpus is composed of fiction, magazines, newspapers, and nonfiction books, and its genre balance stays roughly the same from decade to decade. Because of its size and its advanced architecture and interface, it allows researchers to look at an extremely wide range of changes – many of which would not be possible with a small 2–4 million word corpus. These include the frequency of any word or phrase by decade and mass comparison of all words in different periods (to examine lexical changes), morphological shifts (via wildcards and pattern matching), syntactic shifts (due to very accurate lemmatization and part of speech tagging), and semantic change (by comparing collocates over time, as well as searches that use data from the integrated thesaurus and customized word lists).",train,1,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
67,1457,2817,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"Finally, we define bias towards refugees similar to how the authors of (Garg et al., 2018) define bias against Asians during the 20th century, measuring to what extent radio shows associate ""outsider"" adjectives like ""aggressive"", ""frightening"", ""illegal"", etc. To compute refugee bias scores with respect to the attribute set A, we use the relative norm distance metric from (Garg et al., 2018) :",Uses,s2,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,1,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
68,1458,2818,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"From qualitative inspection, the day-byday scores produced by the non-dynamic model appear much less smooth, and hence, fail to show the relative shift in discourse that likely occurred in response to a major refugee-related news event. One possible reason for this is that the median number of words for each day in the talk radio corpus is 4 million-over 5x fewer than a median of 22 million words per decade used to train each decade-specific model in (Garg et al., 2018) .",Difference,s2,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,0,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
69,1461,2821,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"Qualitative inspection of Figure 2 suggests that our model also produces smoother decade-by-decade scores, suggesting that it not only identifies attribute- (Garg et al., 2018) and our model (blue dotted and green dashed lines, respectively) compared to actual workforce participation rates (solid lines) for gender (top) and Asian/White (bottom) linguistic biases.",Difference,s2,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,0,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
70,1498,2921,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,Bolukbasi et al. (2016b) show that using word embeddings for simple analogies surfaces many gender stereotypes.,Background,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
71,1499,2922,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,"Recently, some work has been done to reduce the gender bias in word embeddings, both as a post-processing step (Bolukbasi et al., 2016b) and as part of the training procedure (Zhao et al., 2018) .",Background,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
72,1500,2923,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,"2 In a seminal work, Bolukbasi et al. (2016b) use a post-processing debiasing method.",Background,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
73,1501,2924,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"These works implicitly define what is good gender debiasing: according to Bolukbasi et al. (2016b) , there is no gender bias if each nonexplicitly gendered word in the vocabulary is in equal distance to both elements of all explicitly gendered pairs.",Background,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
74,1502,2925,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Bolukbasi et al. (2016b) define the gender bias of a word w by its projection on the ""gender direction"": − → w · ( − → he − −→ she), assuming all vectors are normalized.",Background,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
75,1503,2926,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,"Both Bolukbasi et al. (2016b) and Zhao et al. (2018) propose methods for debiasing word embeddings, substantially reducing the bias according to the suggested definition.",Background,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
76,1504,2927,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,Professions We consider the list of professions used in Bolukbasi et al. (2016b) and Zhao et al. (2018) 10 in light of the neighbours-based bias definition.,Similar,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
77,1505,2928,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,"6 Unless otherwise specified, we follow Bolukbasi et al. (2016b) and use a reduced version of the vocabulary for both word embeddings: we take the most frequent 50,000 words and phrases and remove words with upper-case letters, digits, or punctuation, and words longer than 20 characters.",Extention,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
78,1506,2929,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,"Male-and female-biased words cluster together We take the most biased words in the vocabulary according to the original bias (500 malebiased and 500 female-biased 8 ), and cluster them 6 We use the embeddings provided by Bolukbasi et al. (2016b) in https://github.com/tolga-b/ debiaswe and by Zhao et al. (2018) in https:// github.com/uclanlp/gn_glove.",Uses,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
79,1507,2930,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,"Male-and female-biased words cluster together We take the most biased words in the vocabulary according to the original bias (500 malebiased and 500 female-biased 8 ), and cluster them 6 We use the embeddings provided by Bolukbasi et al. (2016b) in https://github.com/tolga-b/ debiaswe and by Zhao et al. (2018) in https:// github.com/uclanlp/gn_glove.",Similar,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
80,1508,2931,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,"6 Unless otherwise specified, we follow Bolukbasi et al. (2016b) and use a reduced version of the vocabulary for both word embeddings: we take the most frequent 50,000 words and phrases and remove words with upper-case letters, digits, or punctuation, and words longer than 20 characters.",Difference,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
81,1509,2933,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,babbf74939612ee2f0203c30a190b4b95881415b,Professions We consider the list of professions used in Bolukbasi et al. (2016b) and Zhao et al. (2018) 10 in light of the neighbours-based bias definition.,Uses,s2,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",train,1,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
82,1555,3015,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,668db48c6a79826456341680ee1175dfc4cced71,"Recently, a number of taskspecific attention variants have been proposed to deal with these issues: See et al. (2017) introduced a coverage mechanism (Tu et al., 2016 ) * Work performed while at Apple.",Background,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
83,1556,3016,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,c6850869aa5e78a107c378d2e8bfa39633158c0c,"Later on, many improvements were described in the Google neural machine translation system (Wu et al., 2016) , including utilizing coverage penalty (Tu et al., 2016) while decoding.",Background,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
84,1557,3017,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,02534853626c18c9a097c2712f1ddf3613257d35,"Copynet (Gu et al., 2016) and pointer-generator networks (Vinyals et al., 2015) , for example, aim to reduce input-output vocabulary mismatch and, thereby, improve specificity, while the coveragebased techniques of Tu et al. (2016) tackle repetition and under-generation.",Background,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
85,1558,3018,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,02534853626c18c9a097c2712f1ddf3613257d35,"We find, for each task, that Scratchpad attains improvements over several strong baselines: Sequence-to-Sequence with attention Bahdanau et al., 2014) , copy-enhanced approaches (Gu et al., 2016; Vinyals et al., 2015) , and coverageenhanced approaches (Tu et al., 2016; See et al., 2017) .",Background,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
86,1559,3019,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,82dbca10dbf6c7a5b2c13579d35c70ac3e8c509c,"Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by Tu et al. (2016) that aims to solve attention-related problems.",Similar,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
87,1560,3020,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,c6850869aa5e78a107c378d2e8bfa39633158c0c,"For IWSLT15, we primarily compare to GNMT (Wu et al., 2016) , which incorporates Coverage (Tu et al., 2016) .",Similar,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
88,1561,3021,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,d864750eb8877bc46d8f8e7ec5305f734ddc91aa,"In Song et al. (2017) , a seq2seq model with copynet and a coverage mechanism (Tu et al., 2016 ) is used to achieve state-of-the-art results.",Similar,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","We propose a query-based generative model for solving both tasks of question generation (QG) and question an- swering (QA). The model follows the classic encoder- decoder framework. The encoder takes a passage and a query as input then performs query understanding by matching the query with the passage from multiple per- spectives. The decoder is an attention-based Long Short Term Memory (LSTM) model with copy and coverage mechanisms. In the QG task, a question is generated from the system given the passage and the target answer, whereas in the QA task, the answer is generated given the question and the passage. During the training stage, we leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias, a major prob- lem resulted from sequence learning with cross-entropy loss. For the QG task, our experiments show higher per- formances than the state-of-the-art results. When used as additional training data, the automatically generated questions even improve the performance of a strong ex- tractive QA system. In addition, our model shows bet- ter performance than the state-of-the-art baselines of the generative QA task.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
89,1562,3022,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,82dbca10dbf6c7a5b2c13579d35c70ac3e8c509c,"Accordingly, we compare our Scratchpad Mechanism against three baselines: (1) Seq2Seq, (2) Copynet and (3) Coverage, a method introduced by Tu et al. (2016) that aims to solve attention-related problems.",Uses,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Generating answer with natural language sentence is very important in real-world question answering systems, which needs to obtain a right answer as well as a coherent natural response. In this paper, we propose an end-to-end question answering system called COREQA in sequence-to-sequence learning, which incorporates copying and retrieving mechanisms to generate natural answers within an encoder-decoder framework. Specifically, in COREQA, the semantic units (words, phrases and entities) in a natural answer are dynamically predicted from the vocabulary, copied from the given question and/or retrieved from the corresponding knowledge base jointly. Our empirical study on both synthetic and real-world datasets demonstrates the efficiency of COREQA, which is able to generate correct, coherent and natural answers for knowledge inquired questions.",train,1,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
90,1563,3023,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,668db48c6a79826456341680ee1175dfc4cced71,"Attention Closest to our work, in the general paradigm of seq2seq learning, is the coverage mechanism introduced in Tu et al. (2016) and later adapted for summarization in See et al. (2017) .",Uses,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",train,1,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
91,1564,3024,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,668db48c6a79826456341680ee1175dfc4cced71,"Attention Closest to our work, in the general paradigm of seq2seq learning, is the coverage mechanism introduced in Tu et al. (2016) and later adapted for summarization in See et al. (2017) .",Similar,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
92,1566,3026,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,d864750eb8877bc46d8f8e7ec5305f734ddc91aa,"In Song et al. (2017) , a seq2seq model with copynet and a coverage mechanism (Tu et al., 2016 ) is used to achieve state-of-the-art results.",Uses,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","We propose a query-based generative model for solving both tasks of question generation (QG) and question an- swering (QA). The model follows the classic encoder- decoder framework. The encoder takes a passage and a query as input then performs query understanding by matching the query with the passage from multiple per- spectives. The decoder is an attention-based Long Short Term Memory (LSTM) model with copy and coverage mechanisms. In the QG task, a question is generated from the system given the passage and the target answer, whereas in the QA task, the answer is generated given the question and the passage. During the training stage, we leverage a policy-gradient reinforcement learning algorithm to overcome exposure bias, a major prob- lem resulted from sequence learning with cross-entropy loss. For the QG task, our experiments show higher per- formances than the state-of-the-art results. When used as additional training data, the automatically generated questions even improve the performance of a strong ex- tractive QA system. In addition, our model shows bet- ter performance than the state-of-the-art baselines of the generative QA task.",train,1,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
93,1569,3029,ABC_be67496882917c2a44afb42e6f9f15_16,ACL:P19-1407,668db48c6a79826456341680ee1175dfc4cced71,"Previous work based on coverage based approaches (Tu et al., 2016; See et al., 2017) either imposed an extra term to the loss function or used an extra vector to keep track of which parts of the input sequences had been attended to, thereby focusing the attention weights in subsequent steps on tokens that received little attention before.",Difference,s2,"We introduce the Scratchpad Mechanism, a novel addition to the sequence-to-sequence (seq2seq) neural network architecture and demonstrate its effectiveness in improving the overall fluency of seq2seq models for natural language generation tasks. By enabling the decoder at each time step to write to all of the encoder output layers, Scratchpad can employ the encoder as a “scratchpad” memory to keep track of what has been generated so far and thereby guide future generation. We evaluate Scratchpad in the context of three well-studied natural language generation tasks — Machine Translation, Question Generation, and Text Summarization — and obtain state-of-the-art or comparable performance on standard datasets for each task. Qualitative assessments in the form of human judgements (question generation), attention visualization (MT), and sample output (summarization) provide further evidence of the ability of Scratchpad to generate fluent and expressive output.","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",train,0,"The research problem is to improve the overall fluency of sequence-to-sequence (seq2seq) models for natural language generation tasks. The motivation behind this research is to address the challenge of improving the fluency of seq2seq models, which are widely used in natural language generation tasks.",train,"decoder, Machine Translation, natural language generation tasks, natural language generation tasks, Scratchpad, Scratchpad, Scratchpad, Scratchpad Mechanism, seq2seq models, sequence-to-sequence ( seq2seq ) neural network architecture, Text Summarization"
94,1642,3191,ABC_9c5baf669470fe4dd18277591591f1_17,ACL:S19-1008,907659d4744e2796d81fe2ce65d7235d79826f66,"The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (Zhang et al., 2015) , named entity recognition (Kuru et al., 2016) , and time normalization (Laparra et al., 2018a) .",Background,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
95,1643,3192,ABC_9c5baf669470fe4dd18277591591f1_17,ACL:S19-1008,907659d4744e2796d81fe2ce65d7235d79826f66,"There are three types of outputs per Laparra et al. (2018a) 's encoding of the SCATE schema, so there is a separate stack of bi-GRUs and a softmax for each output type.",Background,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
96,1644,3193,ABC_9c5baf669470fe4dd18277591591f1_17,ACL:S19-1008,907659d4744e2796d81fe2ce65d7235d79826f66,"We focus on the task of parsing time normalizations (Laparra et al., 2018b) , where large gains of character-level models over word-level models have been observed (Laparra et al., 2018a) .",Background,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
97,1645,3194,ABC_9c5baf669470fe4dd18277591591f1_17,ACL:S19-1008,907659d4744e2796d81fe2ce65d7235d79826f66,"The analyses have focused on wordlevel models, yet character-level models have been shown to outperform word-level models in some NLP tasks, such as text classification (Zhang et al., 2015) , named entity recognition (Kuru et al., 2016) , and time normalization (Laparra et al., 2018a) . Thus, there is a need to study pre-trained contextualized character embeddings, to see if they also yield improvements, and if so, to analyze where those benefits are coming from.",Motivation,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
98,1646,3195,ABC_9c5baf669470fe4dd18277591591f1_17,ACL:S19-1008,907659d4744e2796d81fe2ce65d7235d79826f66,"We focus on the task of parsing time normalizations (Laparra et al., 2018b) , where large gains of character-level models over word-level models have been observed (Laparra et al., 2018a) .",Uses,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).",train,1,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
99,1647,3196,ABC_9c5baf669470fe4dd18277591591f1_17,ACL:S19-1008,907659d4744e2796d81fe2ce65d7235d79826f66,"In this paper, We focus on the character-level time entity identifier that is the foundation of Laparra et al. (2018a) 's model.",Uses,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).",train,1,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
100,1648,3198,ABC_9c5baf669470fe4dd18277591591f1_17,ACL:S19-1008,907659d4744e2796d81fe2ce65d7235d79826f66,"We first take a state-of-the-art neural network for parsing time normalizations (Laparra et al., 2018a) and replace its randomly initialized character embeddings with pre-trained contextual character embeddings.",Extention,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","This paper presents the outcomes of the Parsing Time Normalization shared task held within SemEval-2018. The aim of the task is to parse time expressions into the compositional semantic graphs of the Semantically Compositional Annotation of Time Expressions (SCATE) schema, which allows the representation of a wider variety of time expressions than previous approaches. Two tracks were included, one to evaluate the parsing of individual components of the produced graphs, in a classic information extraction way, and another one to evaluate the quality of the time intervals resulting from the interpretation of those graphs. Though 40 participants registered for the task, only one team submitted output, achieving 0.55 F1 in Track 1 (parsing) and 0.70 F1 in Track 2 (intervals).",train,1,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
101,1665,3245,ABC_975413dd6b3d3df9c5d111d94e8eb7_18,ACL:P19-1127,119e8be1496da4661dbdd4f2e1c174556aa1013e,"Textual relation (Bunescu and Mooney, 2005) , defined as the shortest path between two entities in the dependency parse tree of a sentence, has been widely shown to be the main bearer of relational information in text and proved effective in relation extraction tasks (Xu et al., 2015; Su et al., 2018) .",Background,s2,"Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus","We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.",train,0,"The research problem is to learn a general-purpose embedding for textual relations, defined as the shortest dependency path between entities. This embedding aims to bridge the gap between word/phrase level and sentence level understanding, facilitating downstream NLP tasks requiring relational understanding of text. The motivation for this research is the need for a general-purpose embedding for textual relations, which can be beneficial for various downstream NLP tasks requiring relational understanding of text.",train,"distant supervision dataset, downstream NLP tasks, downstream tasks, English ClueWeb09 corpus, general-purpose embedding of textual relations, relational understanding of the text, relational understanding tasks, sentence embeddings, Textual relation embedding, textual relation embedding, word embeddings"
102,1668,3248,ABC_975413dd6b3d3df9c5d111d94e8eb7_18,ACL:P19-1127,119e8be1496da4661dbdd4f2e1c174556aa1013e,"Recently Su et al. (2018) propose to leverage global co-occurrence statistics of textual and KB relations to learn embeddings of textual relations, and show that it can effectively combat the wrong labeling problem of distant supervision (see Figure 1 for example).",Background,s2,"Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus","We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.",train,0,"The research problem is to learn a general-purpose embedding for textual relations, defined as the shortest dependency path between entities. This embedding aims to bridge the gap between word/phrase level and sentence level understanding, facilitating downstream NLP tasks requiring relational understanding of text. The motivation for this research is the need for a general-purpose embedding for textual relations, which can be beneficial for various downstream NLP tasks requiring relational understanding of text.",train,"distant supervision dataset, downstream NLP tasks, downstream tasks, English ClueWeb09 corpus, general-purpose embedding of textual relations, relational understanding of the text, relational understanding tasks, sentence embeddings, Textual relation embedding, textual relation embedding, word embeddings"
103,1669,3249,ABC_975413dd6b3d3df9c5d111d94e8eb7_18,ACL:P19-1127,119e8be1496da4661dbdd4f2e1c174556aa1013e,"(Su et al., 2018) use global co-occurrence statistics of 1 https://github.com/czyssrs/GloREPlus textual and KB relations to effectively combat the wrong labeling problem. But the global statistics in their work is limited to NYT dataset, capturing domain-specific distributions.",Background,s2,"Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus","We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.",train,0,"The research problem is to learn a general-purpose embedding for textual relations, defined as the shortest dependency path between entities. This embedding aims to bridge the gap between word/phrase level and sentence level understanding, facilitating downstream NLP tasks requiring relational understanding of text. The motivation for this research is the need for a general-purpose embedding for textual relations, which can be beneficial for various downstream NLP tasks requiring relational understanding of text.",train,"distant supervision dataset, downstream NLP tasks, downstream tasks, English ClueWeb09 corpus, general-purpose embedding of textual relations, relational understanding of the text, relational understanding tasks, sentence embeddings, Textual relation embedding, textual relation embedding, word embeddings"
104,1670,3250,ABC_975413dd6b3d3df9c5d111d94e8eb7_18,ACL:P19-1127,119e8be1496da4661dbdd4f2e1c174556aa1013e,"We also compare with using vanilla RNN in GloRE (Su et al., 2018) .",Uses,s2,"Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus","We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.",train,1,"The research problem is to learn a general-purpose embedding for textual relations, defined as the shortest dependency path between entities. This embedding aims to bridge the gap between word/phrase level and sentence level understanding, facilitating downstream NLP tasks requiring relational understanding of text. The motivation for this research is the need for a general-purpose embedding for textual relations, which can be beneficial for various downstream NLP tasks requiring relational understanding of text.",train,"distant supervision dataset, downstream NLP tasks, downstream tasks, English ClueWeb09 corpus, general-purpose embedding of textual relations, relational understanding of the text, relational understanding tasks, sentence embeddings, Textual relation embedding, textual relation embedding, word embeddings"
105,1671,3251,ABC_975413dd6b3d3df9c5d111d94e8eb7_18,ACL:P19-1127,119e8be1496da4661dbdd4f2e1c174556aa1013e,"Same as (Su et al., 2018) , we use PCNN+ATT (Lin et al., 2016 ) as our base model.",Uses,s2,"Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus","We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.",train,1,"The research problem is to learn a general-purpose embedding for textual relations, defined as the shortest dependency path between entities. This embedding aims to bridge the gap between word/phrase level and sentence level understanding, facilitating downstream NLP tasks requiring relational understanding of text. The motivation for this research is the need for a general-purpose embedding for textual relations, which can be beneficial for various downstream NLP tasks requiring relational understanding of text.",train,"distant supervision dataset, downstream NLP tasks, downstream tasks, English ClueWeb09 corpus, general-purpose embedding of textual relations, relational understanding of the text, relational understanding tasks, sentence embeddings, Textual relation embedding, textual relation embedding, word embeddings"
106,1672,3252,ABC_975413dd6b3d3df9c5d111d94e8eb7_18,ACL:P19-1127,119e8be1496da4661dbdd4f2e1c174556aa1013e,"Therefore, we mainly employ manual evaluation. As shown in previous work (Su et al., 2018) , on NYT dataset, due to a significant amount of false negatives, the PR curve on the held-out set may not be an accurate measure of performance.",Motivation,s2,"Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus","We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.",train,0,"The research problem is to learn a general-purpose embedding for textual relations, defined as the shortest dependency path between entities. This embedding aims to bridge the gap between word/phrase level and sentence level understanding, facilitating downstream NLP tasks requiring relational understanding of text. The motivation for this research is the need for a general-purpose embedding for textual relations, which can be beneficial for various downstream NLP tasks requiring relational understanding of text.",train,"distant supervision dataset, downstream NLP tasks, downstream tasks, English ClueWeb09 corpus, general-purpose embedding of textual relations, relational understanding of the text, relational understanding tasks, sentence embeddings, Textual relation embedding, textual relation embedding, word embeddings"
107,1675,3255,ABC_975413dd6b3d3df9c5d111d94e8eb7_18,ACL:P19-1127,119e8be1496da4661dbdd4f2e1c174556aa1013e,"Same as (Su et al., 2018) , we use PCNN+ATT (Lin et al., 2016 ) as our base model.",Similar,s2,"Pre-trained embeddings such as word embeddings and sentence embeddings are fundamental tools facilitating a wide range of downstream NLP tasks. In this work, we investigate how to learn a general-purpose embedding of textual relations, defined as the shortest dependency path between entities. Textual relation embedding provides a level of knowledge between word/phrase level and sentence level, and we show that it can facilitate downstream tasks requiring relational understanding of the text. To learn such an embedding, we create the largest distant supervision dataset by linking the entire English ClueWeb09 corpus to Freebase. We use global co-occurrence statistics between textual and knowledge base relations as the supervision signal to train the embedding. Evaluation on two relational understanding tasks demonstrates the usefulness of the learned textual relation embedding. The data and code can be found at https://github.com/czyssrs/GloREPlus","We study the problem of textual relation embedding with distant supervision. To combat the wrong labeling problem of distant supervision, we propose to embed textual relations with global statistics of relations, i.e., the co-occurrence statistics of textual and knowledge base relations collected from the entire corpus. This approach turns out to be more robust to the training noise introduced by distant supervision. On a popular relation extraction dataset, we show that the learned textual relation embedding can be used to augment existing relation extraction models and significantly improve their performance. Most remarkably, for the top 1,000 relational facts discovered by the best existing model, the precision can be improved from 83.9% to 89.3%.",train,0,"The research problem is to learn a general-purpose embedding for textual relations, defined as the shortest dependency path between entities. This embedding aims to bridge the gap between word/phrase level and sentence level understanding, facilitating downstream NLP tasks requiring relational understanding of text. The motivation for this research is the need for a general-purpose embedding for textual relations, which can be beneficial for various downstream NLP tasks requiring relational understanding of text.",train,"distant supervision dataset, downstream NLP tasks, downstream tasks, English ClueWeb09 corpus, general-purpose embedding of textual relations, relational understanding of the text, relational understanding tasks, sentence embeddings, Textual relation embedding, textual relation embedding, word embeddings"
108,1870,3652,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,"Recent research in the area of unit segmentation (Eger et al., 2017; Ajjour et al., 2017) has lead to promising results with F1-scores of up to 0.90 for in-domain segmentation (Eger et al., 2017) .",Background,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,0,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
109,1871,3653,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,"This framework has been applied previously for the same task (Stab, 2017; Eger et al., 2017; Ajjour et al., 2017) .",Similar,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,0,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
110,1873,3655,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,"According to Ajjour et al. (2017) , the latter Bi-LSTM is used to correct the errors of the first one.",Similar,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,0,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
111,1874,3656,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,"For our re-implementation of the baseline, we are able to approximately reproduce the results reported by Ajjour et al. (2017) .",Similar,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,0,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
112,1875,3657,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,e23c34414e66118ecd9b08cf0cd4d016f59b0b85,"Further, Ajjour et al. (2017) proposed a setup with three bidirectional LSTMs (Bi-LSTMs) (Schuster and Paliwal, 1997) in total as their best solution.",Background,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","In the first part of this paper, a regular recurrent neural network (RNN) is extended to a bidirectional recurrent neural network (BRNN). The BRNN can be trained without the limitation of using input information just up to a preset future frame. This is accomplished by training it simultaneously in positive and negative time direction. Structure and training procedure of the proposed network are explained. In regression and classification experiments on artificial data, the proposed structure gives better results than other approaches. For real data, classification experiments for phonemes from the TIMIT database show the same tendency. In the second part of this paper, it is shown how the proposed bidirectional structure can be easily modified to allow efficient estimation of the conditional posterior probability of complete symbol sequences without making any explicit assumption about the shape of the distribution. For this part, experiments on real data are reported.",train,0,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
113,1876,3658,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,"The architectures proposed in this section build on Ajjour et al. (2017) , omitting the second Bi-LSTM, which was used to process features other than word embeddings (see section 3.1).",Difference,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,0,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
114,1877,3659,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,"The architectures proposed in this section build on Ajjour et al. (2017) , omitting the second Bi-LSTM, which was used to process features other than word embeddings (see section 3.1).",Extention,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,1,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
115,1878,3660,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,Baseline re-implementation The baseline model from Ajjour et al. (2017) uses a total of three Bi-LSTMs (two of them fully connected) to assign labels to tokens (see Figure 1a) .,Similar,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,0,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
116,1879,3662,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,Baseline re-implementation The baseline model from Ajjour et al. (2017) uses a total of three Bi-LSTMs (two of them fully connected) to assign labels to tokens (see Figure 1a) .,Uses,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,1,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
117,1880,3664,ABC_44916cd85311c78666839a3376ccc6_20,ARXIV:1906.10068,97d798fb89b46ccfdc6e7e9add827f95afa2b53b,"For our re-implementation of the baseline, we are able to approximately reproduce the results reported by Ajjour et al. (2017) .",Uses,s2,"Attention mechanisms have seen some success for natural language processing downstream tasks in recent years and generated new state-of-the-art results. A thorough evaluation of the attention mechanism for the task of Argumentation Mining is missing. With this paper, we report a comparative evaluation of attention layers in combination with a bidirectional long short-term memory network, which is the current state-of-the-art approach for the unit segmentation task. We also compare sentence-level contextualized word embeddings to pre-generated ones. Our findings suggest that for this task, the additional attention layer does not improve the performance. In most cases, contextualized embeddings do also not show an improvement on the score achieved by pre-defined embeddings.","The segmentation of an argumentative text into argument units and their non-argumentative counterparts is the first step in identifying the argumentative structure of the text. Despite its importance for argument mining, unit segmentation has been approached only sporadically so far. This paper studies the major parameters of unit segmentation systematically. We explore the effectiveness of various features, when capturing words separately, along with their neighbors, or even along with the entire text. Each such context is reflected by one machine learning model that we evaluate within and across three domains of texts. Among the models, our new deep learning approach capturing the entire text turns out best within all domains, with an F-score of up to 88.54. While structural features generalize best across domains, the domain transfer remains hard, which points to major challenges of unit segmentation.",train,1,"The research problem is the lack of thorough evaluation of the effectiveness of attention mechanisms in the specific task of Argumentation Mining. The motivation is to address the gap in research by thoroughly evaluating the effectiveness of attention mechanisms for Argumentation Mining, especially in the context of unit segmentation.",train,"Argumentation, attention layer, attention layers, attention mechanism, Attention mechanisms, bidirectional long short-term memory network, contextualized embeddings, natural language processing downstream tasks, pre-defined embeddings, sentence-level contextualized word embeddings, unit segmentation task"
118,1917,3737,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"In [8] , a parallel corpus was needed as the loss functions adopted try to minimise either the distance between captions in two languages or the distance between captions in two languages and the associated image as pivot.",Background,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
119,1918,3738,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"This preliminary result, in line with previous findings of [8] , confirms that neural speech-image models can capture a cross-lingual semantic signal, a first step in the perspective of learning speech-to-speech translation systems without text supervision.",Similar,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
120,1919,3739,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,We evaluated our approach on 1k captions of our test corpus to be comparable with [8] .,Similar,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
121,1920,3740,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"For comparison, we report [8] 's results on English to Hindi (HI) and Hindi to English speech-to-speech retrieval.",Similar,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
122,1921,3741,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"These corpora, as well as deep learning models, lead to contributions in multilingual language grounding and learning of shared and multimodal representations with neural networks [4, 7, 8, 9, 10, 11, 12, 13] .",Background,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
123,1922,3742,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"is paired with image I, we assess the ability of our approach to rank the matching spoken caption in language tgt paired with image I in the top 1, 5, and 10 results and give its median rank r. We report our results in Table 4 as well as results from [8] who performed speechto-speech retrieval using crowd-sourced spoken captions in English and Hindi.",Similar,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
124,1923,3743,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"Nevertheless, it is also important to mention that [8] experimented on real speech with multiple speakers while we used synthetic speech with only one voice.",Difference,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
125,1924,3744,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"We have seen in previous section that attention focuses on nouns and Table 2 suggests that these nouns correspond to the main concept of the paired image. To confirm this trend, we experiment on a crosslingual speech-to-speech retrieval task using images as pivots. This possibility was introduced in [8] , but required training jointly or alternatively two speech encoders within the same architecture and a parallel bilingual speech dataset while we experiment with separately trained models for both languages.",Extention,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,1,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
126,1925,3745,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"We have seen in previous section that attention focuses on nouns and Table 2 suggests that these nouns correspond to the main concept of the paired image. To confirm this trend, we experiment on a crosslingual speech-to-speech retrieval task using images as pivots. This possibility was introduced in [8] , but required training jointly or alternatively two speech encoders within the same architecture and a parallel bilingual speech dataset while we experiment with separately trained models for both languages.",Difference,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
127,1926,3746,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,ces so that there would be only one target caption for each query in order to compare our results with [8] .,Future Work,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
128,1927,3747,ABC_e9779b09826d709f8851550d958df7_20,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,We evaluated our approach on 1k captions of our test corpus to be comparable with [8] .,Uses,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,1,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
129,1975,3993,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"To model homophily, recent research in abusive language detection on Twitter (Mishra et al., 2018a) incorporates embeddings for authors (i.e., users who have composed tweets) that encode the structure of their surrounding communities.",Background,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,0,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
130,1976,3994,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"This is the state of the art method (Mishra et al., 2018a) for the dataset we are using.",Background,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,0,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
131,1977,3995,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"We first describe the approach of Mishra et al. (2018a) that learns author embeddings using node2vec (Grover and Leskovec, 2016) ; this serves as our baseline.",Background,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,0,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
132,1978,3996,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,We create two different graphs: the first one is identical to the community graph of Mishra et al. (2018a) (referred to as the community graph).,Uses,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,1,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
133,1979,3997,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"Qian et al. (2018) Following previous work (Mishra et al., 2018a) , we experiment with a subset of the Twitter dataset compiled by Waseem and Hovy (2016",Uses,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,1,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
134,1980,3998,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"Following Mishra et al. (2018a) , we initialize these parameters to their default value of 1 and set the embedding size and number of iterations to 200 and 25 respectively.",Uses,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,1,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
135,1981,3999,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,We create two different graphs: the first one is identical to the community graph of Mishra et al. (2018a) (referred to as the community graph).,Similar,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,0,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
136,1982,4000,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"We first describe the approach of Mishra et al. (2018a) that learns author embeddings using node2vec (Grover and Leskovec, 2016) ; this serves as our baseline.",Uses,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,1,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
137,1983,4002,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"However, on the racism class, its recall is hindered by the same factor that Mishra et al. (2018a) highlighted for their node2vec-only method, i.e., that racist tweets come from 5 unique authors only who have also contributed sexist or clean tweets.",Similar,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,0,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
138,1984,4004,ABC_45551e674210bb9bbb56c8778d2f8c_22,ACL:N19-1221,b9f4cb8e84ea0a44d09f38cd777a986d21d3d219,"In this paper, we built on the work of Mishra et al. (2018a) that introduces community-based profiling of authors for abusive language detection.",Extention,s2,"Abuse on the Internet represents a significant societal problem of our time. Previous research on automated abusive language detection in Twitter has shown that community-based profiling of users is a promising technique for this task. However, existing approaches only capture shallow properties of online communities by modeling follower–following relationships. In contrast, working with graph convolutional networks (GCNs), we present the first approach that captures not only the structure of online communities but also the linguistic behavior of the users within them. We show that such a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection.","The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this problem and resort to a generic OOV (out of vocabulary) embedding for unseen words. However, in using a single embedding for all unseen words we lose the ability to distinguish between obfuscated and non-obfuscated or rare words. In this paper, we address this problem by designing a model that can compose embeddings for unseen words. We experimentally demonstrate that our approach significantly advances the current state of the art in abuse detection on datasets from two different domains, namely Twitter and Wikipedia talk page.",train,1,"The research problem is the detection of abusive language on Twitter, specifically addressing the limitations of existing community-based profiling methods that only capture shallow properties of online communities. The motivation behind this research is to address the societal problem of online abuse by improving the accuracy of automated abusive language detection. The proposed approach aims to overcome the limitations of existing methods by providing a more comprehensive understanding of online communities.",train,"abusive language detection, automated abusive language detection, community-based profiling of users, graph convolutional networks ( GCNs ), heterogeneous graph-structured modeling of communities, Twitter"
139,2057,4289,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"We infer relations between entities using MIL-based bi-affine pairwise scoring function (Verga et al., 2018) on the entity node representations.",Similar,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,0,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
140,2058,4290,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"Since each target entity can have multiple mentions in a document, we employ a multi-instance learning (MIL)-based classification scheme to aggregate the predictions of all target mention pairs using bi-affine pairwise scoring (Verga et al., 2018) .",Similar,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,0,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
141,2059,4291,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"Next, it encodes the graph structure using a stacked GCNN layer and classifies the relation between the target entities by applying MIL (Verga et al., 2018) to aggregate all 1 The dataset is publicly available at http://nactem.",Similar,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,0,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
142,2060,4292,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"Recently, Verga et al. (2018) introduced multi-instance learning (MIL) (Riedel et al., 2010; Surdeanu et al., 2012) to treat multiple mentions of target entities in a document.",Background,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,0,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
143,2061,4293,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"Next, it encodes the graph structure using a stacked GCNN layer and classifies the relation between the target entities by applying MIL (Verga et al., 2018) to aggregate all 1 The dataset is publicly available at http://nactem.",Uses,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,1,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
144,2062,4294,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"For the CDR dataset, we performed hypernym filtering similar to Gu et al. (2017) and Verga et al. (2018) .",Similar,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,0,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
145,2064,4296,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"We infer relations between entities using MIL-based bi-affine pairwise scoring function (Verga et al., 2018) on the entity node representations.",Uses,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,1,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
146,2065,4297,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"Since each target entity can have multiple mentions in a document, we employ a multi-instance learning (MIL)-based classification scheme to aggregate the predictions of all target mention pairs using bi-affine pairwise scoring (Verga et al., 2018) .",Uses,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,1,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
147,2066,4298,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"For the CDR dataset, we performed hypernym filtering similar to Gu et al. (2017) and Verga et al. (2018) .",Uses,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,1,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
148,2067,4299,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,Our work is different from Verga et al. (2018) in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference.,Difference,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,0,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
149,2068,4300,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"Unlike Verga et al. (2018), we used the pre-trained word embeddings in place of sub-word embeddings to align with our word graphs.",Extention,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,1,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
150,2069,4301,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,Our work is different from Verga et al. (2018) in that we replace Transformer with a GCNN model for full-abstract encoding using non-local dependencies such as entity coreference.,Extention,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,1,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
151,2071,4303,ABC_939274ae40a68acc322b34d8f91f7e_24,ACL:P19-1423,48f786f66eb846012ceee822598a335d0388f034,"Unlike Verga et al. (2018), we used the pre-trained word embeddings in place of sub-word embeddings to align with our word graphs.",Difference,s2,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.","Most work in relation extraction forms a prediction by looking at a short span of text within a single sentence containing a single entity pair mention. This approach often does not consider interactions across mentions, requires redundant computation for each mention pair, and ignores relationships expressed across sentence boundaries. These problems are exacerbated by the document- (rather than sentence-) level annotation common in biological text. In response, we propose a model which simultaneously predicts relationships between all mention pairs in a document. We form pairwise predictions over entire paper abstracts using an efficient self-attention encoder. All-pairs mention scores allow us to perform multi-instance learning by aggregating over mentions to form entity pair representations. We further adapt to settings without mention-level annotation by jointly training to predict named entities and adding a corpus of weakly labeled data. In experiments on two Biocreative benchmark datasets, we achieve state of the art performance on the Biocreative V Chemical Disease Relation dataset for models without external KB resources. We also introduce a new dataset an order of magnitude larger than existing human-annotated biological information extraction datasets and more accurate than distantly supervised alternatives.",train,0,"Inter-sentence relation extraction faces the challenge of handling complex semantic relationships in documents, requiring the extraction of various dependencies (local, non-local, syntactic, and semantic). Current methods fail to fully exploit these dependencies. The research is motivated by the limitations of existing methods in fully exploiting dependencies for inter-sentence relation extraction. The goal is to develop a more comprehensive model that captures these dependencies effectively.",train,"biochemistry datasets, Existing methods, Inter-sentence relation extraction, inter-sentence relation extraction, inter-sentence relation extraction model, labelled edge graph convolutional neural network model, multi-instance learning, neural models"
152,2120,4415,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,6dbcb288498657b7d3bb3e59383d0a790123afa3,"Recent advances have shown that E2E models can outperform the state-of-the-art conventional system when trained on thousands of hours of data [5, 6] .",Background,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
153,2121,4416,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,6dbcb288498657b7d3bb3e59383d0a790123afa3,Shallow fusion has been used in E2E models for decoding [10] and contextual biasing [6] .,Background,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
154,2122,4417,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,d6c70a2ce72ccc11461860c3a738a1f7ca8d7309,"In [13] , wordpieces have been shown to outperform graphemes in biasing since they create a sparser match of biasing units. All these improvements lead to significantly better biasing which is comparable to the state-of-the-art conventional model [6] .",Background,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","Contextual biasing to a speciﬁc domain, including a user’s song names, app names and contact names, is an important component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in end-to-end models because these models keep a small list of candidates during beam search, and also do poorly on proper nouns, which is the main source of biasing phrases. In this paper, we present various algorithmic and training improvements to shallow-fusion-based biasing for end-to-end models. We will show that the proposed approach obtains better performance than a state-of-the-art conventional model across a variety of tasks, the ﬁrst time this has been demonstrated.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
155,2123,4418,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"Since phonemes show strength in recognizing rare words [16] , we want to present these words as phonemes more often.",Background,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
156,2124,4419,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,d6c70a2ce72ccc11461860c3a738a1f7ca8d7309,"Further improvements such as biasing before beam pruning, and wordpiece-based biasing have been proposed to achieve state-of-the-art biasing results [12, 6, 13] .",Background,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","Contextual biasing to a speciﬁc domain, including a user’s song names, app names and contact names, is an important component of any production-level automatic speech recognition (ASR) system. Contextual biasing is particularly challenging in end-to-end models because these models keep a small list of candidates during beam search, and also do poorly on proper nouns, which is the main source of biasing phrases. In this paper, we present various algorithmic and training improvements to shallow-fusion-based biasing for end-to-end models. We will show that the proposed approach obtains better performance than a state-of-the-art conventional model across a variety of tasks, the ﬁrst time this has been demonstrated.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
157,2125,4420,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,6dbcb288498657b7d3bb3e59383d0a790123afa3,"Similarly to [6] , an input utterance is divided to 25-ms frames, windowed and shifted at a rate of 10 ms.",Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
158,2126,4421,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"Based on [16] , we add two improvements to the decoding strategy.",Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
159,2127,4422,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,6dbcb288498657b7d3bb3e59383d0a790123afa3,"Similar to [6] , the encoder of the RNN-T consists of 8 Long Short-Term Memory (LSTM) [21] layers and the prediction network contains 2 layers.",Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
160,2128,4423,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,We use context-independent phonemes as in [16] .,Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
161,2129,4424,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in [16] .",Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
162,2130,4425,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"Based on [16] , we add two improvements to the decoding strategy.",Uses,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
163,2131,4426,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,We use context-independent phonemes as in [16] .,Uses,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
164,2132,4427,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,6dbcb288498657b7d3bb3e59383d0a790123afa3,"Similarly to [6] , an input utterance is divided to 25-ms frames, windowed and shifted at a rate of 10 ms.",Uses,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",train,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
165,2133,4428,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,6dbcb288498657b7d3bb3e59383d0a790123afa3,"Similar to [6] , the encoder of the RNN-T consists of 8 Long Short-Term Memory (LSTM) [21] layers and the prediction network contains 2 layers.",Uses,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",train,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
166,2134,4429,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"However, we note that the regression is significantly smaller than the all-phoneme model in [16] .",Difference,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
167,2135,4430,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"To generate words as outputs, we search through a decoding graph similar to [16] but accept both phonemes and wordpieces.",Extention,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
168,2136,4431,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,6dbcb288498657b7d3bb3e59383d0a790123afa3,"Lastly, since wordpieces perform better than graphemes [6] in E2E modeling, it would be interesting to explore longer phonemic units such as phoneme pieces for biasing.",Future Work,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","End-to-end (E2E) models, which directly predict output character sequences given input speech, are good candidates for on-device speech recognition. E2E models, however, present numerous challenges: In order to be truly useful, such models must decode speech utterances in a streaming fashion, in real time; they must be robust to the long tail of use cases; they must be able to leverage user-specific context (e.g., contact lists); and above all, they must be extremely accurate. In this work, we describe our efforts at building an E2E speech recog-nizer using a recurrent neural network transducer. In experimental evaluations, we find that the proposed approach can outperform a conventional CTC-based model in terms of both latency and accuracy in a number of evaluation categories.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
169,2137,4432,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in [16] .",Uses,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
170,2138,4433,ABC_8bd97eb118175c9fd2147b6456421c_24,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"To generate words as outputs, we search through a decoding graph similar to [16] but accept both phonemes and wordpieces.",Difference,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",train,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
171,2264,4655,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,Preliminary experiments using a similar collection used in Potthast et al. (2018) show that neural-based classification methods reach state-of-the art results.,Background,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,0,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
172,2265,4656,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"They achieve state-of-the-art results on a publicly available collection (Potthast et al., 2018) , showing that neural models can effectively address the task of hyperpartisan detection without including stylometric features.",Background,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,0,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
173,2266,4657,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"Two state-of-the-art models (SpaCy and Kim (2014)) outperform the approach presented in Potthast et al. (2018) , showing that stylometric features are probably not necessary for the task.",Background,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,0,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
174,2267,4658,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"Identifying partisan preferences in news, based only on text content, has been shown to be a challenging task (Potthast et al., 2018) .",Background,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,0,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
175,2268,4659,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"A recent paper (Potthast et al., 2018) claims that stylometric features are a key factor to tackle this task.",Background,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,0,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
176,2269,4660,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"Experiments were performed using two collections, the ACL2018 collection (Potthast et al., 2018) and the SemEval19 collection .",Uses,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,1,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
177,2270,4661,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"Identifying partisan preferences in news, based only on text content, has been shown to be a challenging task (Potthast et al., 2018) .",Motivation,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,0,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
178,2271,4662,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"As our results are not directly comparable with the values reported in Potthast et al. (2018) , we re-evaluated their approach on this single fold.",Difference,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,0,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
179,2272,4663,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,"As our results are not directly comparable with the values reported in Potthast et al. (2018) , we re-evaluated their approach on this single fold.",Uses,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,1,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
180,2274,4665,ABC_c437e447603ecdbe4053651169770a_26,ACL:S19-2169,ed31e1225f6a76b469dfe4d022b235dc70be4390,This deserves a set of extra experiments to better understand the real contribution of stylometric features when combined with strong representations/classifiers to validate the work of Potthast et al. (2018) .,Uses,s2,"This paper describes the Rouletabille participation to the Hyperpartisan News Detection task. We propose the use of different text classification methods for this task. Preliminary experiments using a similar collection used in (Potthast et al., 2018) show that neural-based classification methods reach state-of-the art results. Our final submission is composed of a unique run that ranks among all runs at 3/49 position for the by-publisher test dataset and 43/96 for the by-article test dataset in terms of Accuracy.","We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",train,1,The research problem is the detection of hyperpartisan news. The paper focuses on exploring different text classification methods for this task. The motivation for this research is not explicitly stated in the abstract. It can be inferred that the authors are aiming to contribute to the field of hyperpartisan news detection by exploring and comparing different text classification methods.,train,"by-article test dataset, by-publisher test dataset, neural-based classification methods, text classification methods"
181,2667,5488,ABC_b31acd3535cd740e609d45986fbf33_32,ACL:S19-1008,421fc2556836a6b441de806d7b393a35b6eaea58,"Pre-trained language models (LMs) such as ELMo (Peters et al., 2018) , ULMFiT (Howard and Ruder, 2018) , OpenAI GPT (Radford et al., 2018) , Flair (Akbik et al., 2018) and Bert (Devlin et al., 2018) have shown great improvements in NLP tasks ranging from sentiment analysis to named entity recognition to question answering.",Background,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
182,2668,5489,ABC_b31acd3535cd740e609d45986fbf33_32,ACL:S19-1008,421fc2556836a6b441de806d7b393a35b6eaea58,"All of the pre-trained word-level contextual embedding models include some character or subword components in their architecture. For example, Flair is a forward-backward LM trained over characters using recurrent neural networks (RNNs), that generates pre-trained contextual word embeddings by concatenating the forward LM's hidden state for the word's last character and the backward LM's hidden state for the word's first character. Flair achieves state-of-the-art or competitive results on part-of-speech tagging and named entity tagging (Akbik et al., 2018) .",Background,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
183,2669,5490,ABC_b31acd3535cd740e609d45986fbf33_32,ACL:S19-1008,421fc2556836a6b441de806d7b393a35b6eaea58,"However, both Akbik et al. (2018) and Bohnet et al. (2018) discard all other contextual character embeddings, and no analyses of the models are performed at the character-level.",Motivation,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
184,2670,5492,ABC_b31acd3535cd740e609d45986fbf33_32,ACL:S19-1008,3febb2bed8865945e7fddc99efd791887bb7e14f,"We perform a feature ablation to see if pre-trained contextual character embeddings capture basic syntax (e.g., part-of-speech) like pre-trained contextual word embeddings do (Peters et al., 2018; Akbik et al., 2018) .",Unsure,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",train,0,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
185,2671,5493,ABC_b31acd3535cd740e609d45986fbf33_32,ACL:S19-1008,5d833331b0e22ff359db05c62a8bca18c4f04b68,"In the current paper, we derive pre-trained contextual character embeddings from Flair's forwardbackward LM trained on a 1-billion word corpus of English (Chelba et al., 2014) , and observe if these embeddings yield the same large improvements for character-level tasks as yielded by pre-trained contextual word embeddings for word-level tasks.",Extention,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned KneserNey 5-gram model achieves perplexity 67.6. A combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.",train,1,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
186,2672,5495,ABC_b31acd3535cd740e609d45986fbf33_32,ACL:S19-1008,421fc2556836a6b441de806d7b393a35b6eaea58,"We derive pre-trained character-level contextual embeddings from Flair (Akbik et al., 2018) , a wordlevel embedding model, inject these into a state-ofthe-art time normalization system, and achieve major performance improvements: 51% error reduction in news and 33% in clinical notes.",Extention,s2,"Recent studies have shown that pre-trained contextual word embeddings, which assign the same word different vectors in different contexts, improve performance in many tasks. But while contextual embeddings can also be trained at the character level, the effectiveness of such embeddings has not been studied. We derive character-level contextual embeddings from Flair (Akbik et al., 2018), and apply them to a time normalization task, yielding major performance improvements over the previous state-of-the-art: 51% error reduction in news and 33% in clinical notes. We analyze the sources of these improvements, and find that pre-trained contextual character embeddings are more robust to term variations, infrequent terms, and cross-domain changes. We also quantify the size of context that pre-trained contextual character embeddings take advantage of, and show that such embeddings capture features like part-of-speech and capitalization.","Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",train,1,"The research addresses the lack of understanding about the effectiveness of character-level contextual embeddings, specifically in the context of time normalization tasks. The research is motivated by the increasing evidence of the effectiveness of contextual word embeddings in various tasks, and the need to investigate the potential of character-level embeddings in enhancing performance.",train,"character-level contextual embeddings, clinical notes, contextual embeddings, Flair, news, pre-trained contextual character embeddings, pre-trained contextual character embeddings, pre-trained contextual word embeddings, time normalization task"
187,2823,5734,ABC_ada92083e8c012c328d5b6172b76ad_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"Several works even directly transfer semantic roles into opinion roles for ORL (Kim and Hovy, 2006; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates.",Background,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",train,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
188,2825,5736,ABC_ada92083e8c012c328d5b6172b76ad_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"Several works even directly transfer semantic roles into opinion roles for ORL (Kim and Hovy, 2006; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR).",Motivation,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",train,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
189,2826,5737,ABC_ada92083e8c012c328d5b6172b76ad_34,ACL:N19-1066,2557adc02e1c07d0af0f6a93eb5547a1cce08eca,"The results show that SRL information is very helpful for ORL, which is consistent with previous studies (Kim and Hovy, 2006; Ruppenhofer et al., 2008; Marasović and Frank, 2018) .",Similar,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question “Who expressed what kind of sentiment towards what?”. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.",train,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
190,2827,5738,ABC_ada92083e8c012c328d5b6172b76ad_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles (Kim and Hovy, 2006; Ruppenhofer et al., 2008) .",Background,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",train,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
191,2828,5740,ABC_ada92083e8c012c328d5b6172b76ad_34,ACL:N19-1066,2557adc02e1c07d0af0f6a93eb5547a1cce08eca,"Results show that SRL is highly effective for ORL, which is consistent with previous findings (Kim and Hovy, 2006; Ruppenhofer et al., 2008; Marasović and Frank, 2018) .",Similar,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question “Who expressed what kind of sentiment towards what?”. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.",train,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
192,2829,5742,ABC_ada92083e8c012c328d5b6172b76ad_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"According to the above findings, we design a simple system by mapping SRL outputs into ORL directly (Kim and Hovy, 2006; Ruppenhofer et al., 2008) .",Uses,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",train,1,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
193,2949,6021,ABC_e29c7551ea78cb425054963489e1b9_37,ACL:N19-1285,aa65d6a2e6eedd80b12b43b5210e27c6532d69e6,Salesky et al. (2018) introduced a new set of fluent reference translations collected on Mechanical Turk.,Background,s2,"Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected ‘copy-edited’ references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, implicitly removing disfluencies in end-to-end translation of conversational speech.","When translating from speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned target data for the Fisher Spanish-English dataset for this task. We compare how different architectures handle disfluencies and provide a baseline for removing disfluencies in end-to-end translation.",train,0,"The primary research problem is the negative impact of disfluencies on the accuracy of spoken language translation applications. Specifically, the study addresses the challenge of incorporating disfluency removal into end-to-end speech translation models to improve translation quality. The research is motivated by the increasing popularity of end-to-end speech translation models and the need to address the issue of disfluencies in conversational speech. The study aims to establish a baseline for a new task focusing on implicit disfluency removal in end-to-end speech translation.",train,"end-to-end speech translation models, end-to-end translation of conversational speech, Fisher Spanish-English dataset, fluent text, fluent translations, machine translation, model architectures, noisy , disfluent speech, sequence-to-sequence model, speech, speech recognition, Spoken language translation applications"
194,2951,6023,ABC_e29c7551ea78cb425054963489e1b9_37,ACL:N19-1285,aa65d6a2e6eedd80b12b43b5210e27c6532d69e6,"For our experiments, we use Fisher Spanish speech (Graff et al.) and with two sets of English translations (Salesky et al., 2018; Post et al., 2013) .",Uses,s2,"Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected ‘copy-edited’ references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, implicitly removing disfluencies in end-to-end translation of conversational speech.","When translating from speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned target data for the Fisher Spanish-English dataset for this task. We compare how different architectures handle disfluencies and provide a baseline for removing disfluencies in end-to-end translation.",train,1,"The primary research problem is the negative impact of disfluencies on the accuracy of spoken language translation applications. Specifically, the study addresses the challenge of incorporating disfluency removal into end-to-end speech translation models to improve translation quality. The research is motivated by the increasing popularity of end-to-end speech translation models and the need to address the issue of disfluencies in conversational speech. The study aims to establish a baseline for a new task focusing on implicit disfluency removal in end-to-end speech translation.",train,"end-to-end speech translation models, end-to-end translation of conversational speech, Fisher Spanish-English dataset, fluent text, fluent translations, machine translation, model architectures, noisy , disfluent speech, sequence-to-sequence model, speech, speech recognition, Spoken language translation applications"
195,2952,6024,ABC_e29c7551ea78cb425054963489e1b9_37,ACL:N19-1285,aa65d6a2e6eedd80b12b43b5210e27c6532d69e6,"Further, corpora can have different translation and annotation schemes: for example for Fisher Spanish-English, translated using Mechanical Turk, Salesky et al. (2018) found 268 unique filler words due to spelling and casing.",Background,s2,"Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected ‘copy-edited’ references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, implicitly removing disfluencies in end-to-end translation of conversational speech.","When translating from speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned target data for the Fisher Spanish-English dataset for this task. We compare how different architectures handle disfluencies and provide a baseline for removing disfluencies in end-to-end translation.",train,0,"The primary research problem is the negative impact of disfluencies on the accuracy of spoken language translation applications. Specifically, the study addresses the challenge of incorporating disfluency removal into end-to-end speech translation models to improve translation quality. The research is motivated by the increasing popularity of end-to-end speech translation models and the need to address the issue of disfluencies in conversational speech. The study aims to establish a baseline for a new task focusing on implicit disfluency removal in end-to-end speech translation.",train,"end-to-end speech translation models, end-to-end translation of conversational speech, Fisher Spanish-English dataset, fluent text, fluent translations, machine translation, model architectures, noisy , disfluent speech, sequence-to-sequence model, speech, speech recognition, Spoken language translation applications"
196,2953,6025,ABC_e29c7551ea78cb425054963489e1b9_37,ACL:N19-1285,aa65d6a2e6eedd80b12b43b5210e27c6532d69e6,"Using clean references for disfluent data collected by Salesky et al. (2018) , we extend their text baseline to speech input and provide first results for direct generation of fluent text from noisy disfluent speech.",Extention,s2,"Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected ‘copy-edited’ references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, implicitly removing disfluencies in end-to-end translation of conversational speech.","When translating from speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned target data for the Fisher Spanish-English dataset for this task. We compare how different architectures handle disfluencies and provide a baseline for removing disfluencies in end-to-end translation.",train,1,"The primary research problem is the negative impact of disfluencies on the accuracy of spoken language translation applications. Specifically, the study addresses the challenge of incorporating disfluency removal into end-to-end speech translation models to improve translation quality. The research is motivated by the increasing popularity of end-to-end speech translation models and the need to address the issue of disfluencies in conversational speech. The study aims to establish a baseline for a new task focusing on implicit disfluency removal in end-to-end speech translation.",train,"end-to-end speech translation models, end-to-end translation of conversational speech, Fisher Spanish-English dataset, fluent text, fluent translations, machine translation, model architectures, noisy , disfluent speech, sequence-to-sequence model, speech, speech recognition, Spoken language translation applications"
197,3021,6161,ABC_78a7ca27c5ca032116db12205af939_38,ARXIV:1906.05190,58b6bd06ea58c367c64286126ba14128b45041b8,"Our contributions are in four-fold: (1) we describe an integrated image interpretation framework for disease annotation and medical report generation, (2) we transfer knowledge from large image data sets (ChestX-ray8 [16] and ImageNet) to enhance medical image interpretation using a small number of reports for training (IU X-ray [2] ), (3) we evaluate suitability of the NLP evaluation metrics for medical report generation, and (4) we demonstrate the functionality of localizing the key finding in an X-ray with a heatmap.",Background,s2,"Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.","The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.",train,0,"The research problem lies in the difficulty of automating medical image interpretation, specifically in the area of generating reports from X-ray images. This is due to the lack of high-quality annotated image-report pairs and the absence of generative models specifically designed to extract and leverage localized semantic features, particularly those related to abnormalities. The motivation behind this research stems from the desire to reduce the workload of radiologists by automating the tedious tasks of inspecting and interpreting medical images. The ultimate goal is to improve efficiency and accuracy in generating reports from X-ray images.",train,"annotated image-report pairs, attentive LSTM model, automatic medical image interpretation system, CIDEr, classifying and localizing common thoracic diseases, computer-aided medical image visual perception, disease classification, image, imaging, localization, natural image captioning, report generation, tailor-made generative models, Vispi, Vispi, X-ray dataset"
198,3022,6162,ABC_78a7ca27c5ca032116db12205af939_38,ARXIV:1906.05190,58b6bd06ea58c367c64286126ba14128b45041b8,"Our contributions are in four-fold: (1) we describe an integrated image interpretation framework for disease annotation and medical report generation, (2) we transfer knowledge from large image data sets (ChestX-ray8 [16] and ImageNet) to enhance medical image interpretation using a small number of reports for training (IU X-ray [2] ), (3) we evaluate suitability of the NLP evaluation metrics for medical report generation, and (4) we demonstrate the functionality of localizing the key finding in an X-ray with a heatmap.",Uses,s2,"Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.","The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.",train,1,"The research problem lies in the difficulty of automating medical image interpretation, specifically in the area of generating reports from X-ray images. This is due to the lack of high-quality annotated image-report pairs and the absence of generative models specifically designed to extract and leverage localized semantic features, particularly those related to abnormalities. The motivation behind this research stems from the desire to reduce the workload of radiologists by automating the tedious tasks of inspecting and interpreting medical images. The ultimate goal is to improve efficiency and accuracy in generating reports from X-ray images.",train,"annotated image-report pairs, attentive LSTM model, automatic medical image interpretation system, CIDEr, classifying and localizing common thoracic diseases, computer-aided medical image visual perception, disease classification, image, imaging, localization, natural image captioning, report generation, tailor-made generative models, Vispi, Vispi, X-ray dataset"
199,3023,6163,ABC_78a7ca27c5ca032116db12205af939_38,ARXIV:1906.05190,58b6bd06ea58c367c64286126ba14128b45041b8,"We filter out images and reports that are non-relevant to the eight common thoracic diseases included in both ChestX-ray8 [16] and IU X-ray datasets [2] , resulting in a dataset with 2225 pairs of X-ray image and report.",Uses,s2,"Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.","The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.",train,1,"The research problem lies in the difficulty of automating medical image interpretation, specifically in the area of generating reports from X-ray images. This is due to the lack of high-quality annotated image-report pairs and the absence of generative models specifically designed to extract and leverage localized semantic features, particularly those related to abnormalities. The motivation behind this research stems from the desire to reduce the workload of radiologists by automating the tedious tasks of inspecting and interpreting medical images. The ultimate goal is to improve efficiency and accuracy in generating reports from X-ray images.",train,"annotated image-report pairs, attentive LSTM model, automatic medical image interpretation system, CIDEr, classifying and localizing common thoracic diseases, computer-aided medical image visual perception, disease classification, image, imaging, localization, natural image captioning, report generation, tailor-made generative models, Vispi, Vispi, X-ray dataset"
200,3024,6164,ABC_78a7ca27c5ca032116db12205af939_38,ARXIV:1906.05190,58b6bd06ea58c367c64286126ba14128b45041b8,We do not fine-tune the DenseNet pretrained with ChestX-ray8 [16] and ResNet pretrained with ImageNet due to the small sample size of IU X-ray dataset [2] .,Unsure,s2,"Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.","The chest X-ray is one of the most commonly accessible radiological examinations for screening and diagnosis of many lung diseases. A tremendous number of X-ray imaging studies accompanied by radiological reports are accumulated and stored in many modern hospitals Picture Archiving and Communication Systems (PACS). On the other side, it is still an open question how this type of hospital-size knowledge database containing invaluable imaging informatics (i.e., loosely labeled) can be used to facilitate the data-hungry deep learning paradigms in building truly large-scale high precision computer-aided diagnosis (CAD) systems. In this paper, we present a new chest X-ray database, namely ChestX-ray8, which comprises 108,948 frontal-view X-ray images of 32,717 unique patients with the text-mined eight disease image labels (where each image can have multi-labels), from the associated radiological reports using natural language processing. Importantly, we demonstrate that these commonly occurring thoracic diseases can be detected and even spatially-located via a unified weakly-supervised multi-label image classification and disease localization framework, which is validated using our proposed dataset. Although the initial quantitative results are promising as reported, deep convolutional neural network based reading chest X-rays (i.e., recognizing and locating the common disease patterns trained with only image-level labels) remains a strenuous task for fully-automated high precision CAD systems.",train,0,"The research problem lies in the difficulty of automating medical image interpretation, specifically in the area of generating reports from X-ray images. This is due to the lack of high-quality annotated image-report pairs and the absence of generative models specifically designed to extract and leverage localized semantic features, particularly those related to abnormalities. The motivation behind this research stems from the desire to reduce the workload of radiologists by automating the tedious tasks of inspecting and interpreting medical images. The ultimate goal is to improve efficiency and accuracy in generating reports from X-ray images.",train,"annotated image-report pairs, attentive LSTM model, automatic medical image interpretation system, CIDEr, classifying and localizing common thoracic diseases, computer-aided medical image visual perception, disease classification, image, imaging, localization, natural image captioning, report generation, tailor-made generative models, Vispi, Vispi, X-ray dataset"
201,3042,6188,ABC_4d25b647a261a415d769532386265a_38,ARXIV:1904.11660,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Recently, transformer networks have been shown to perform well for neural machine translation [11] and many other NLP tasks [12] . A Transformer layer distinguishes itself from a regular recurrent network by entirely relying on a key-value ""self""-attention mechanism for learning relationships between distant concepts, rather than relying on recurrent connections and memory cells to preserve information, as in LSTMs, that can fade over time steps. Transformer layers can be seen as bagof-concept layers because they don't preserve location information in the weighted sum self-attention operation.",Background,s2,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",train,0,"The research aims to improve the application of transformer networks for speech recognition by addressing the challenges of combining positional embedding with speech features and ensuring stability in large-scale learning. The research is motivated by the recent success of transformer networks in other NLP tasks, leading to a growing interest in applying them to speech recognition. The researchers aim to tackle the specific challenges faced when adapting transformers to speech processing.",train,"contextual representations, convolutionally learned input representations, large scale learning of transformer networks, neural machine translation, NLP tasks, positional embedding, sinusoidal positional embedding, speech recognition, transformer networks, transformers"
202,3043,6189,ABC_4d25b647a261a415d769532386265a_38,ARXIV:1904.11660,204e3073870fae3d05bcbc2f6a8e263d9b72e776,Figure(1) -left show the details of one transformer layer as proposed by [11] .,Background,s2,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",train,0,"The research aims to improve the application of transformer networks for speech recognition by addressing the challenges of combining positional embedding with speech features and ensuring stability in large-scale learning. The research is motivated by the recent success of transformer networks in other NLP tasks, leading to a growing interest in applying them to speech recognition. The researchers aim to tackle the specific challenges faced when adapting transformers to speech processing.",train,"contextual representations, convolutionally learned input representations, large scale learning of transformer networks, neural machine translation, NLP tasks, positional embedding, sinusoidal positional embedding, speech recognition, transformer networks, transformers"
203,3045,6191,ABC_4d25b647a261a415d769532386265a_38,ARXIV:1904.11660,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Transformer layers [11] have the ability to learn long range relationships for many sequential classification tasks [12] . Multi- The dot product between keys and queries is scaled by the inverse square root of the key dimension. This self-attention operation is done h times in parallel, for the case of h attention heads, with different projection matrices from dinput to d k , d k , and dv. The final output is a concatenation of h vectors each with dimension dv which is in turn linearly projected to the desired output dimension of the self-attention layer. On top of the self-attention component, transformer layers have multiple operations applied on each time step; dropout, residual connection, layer norm, two fully connected layers with a ReLU layer in between, another residual and Layer norm operations.",Background,s2,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",train,0,"The research aims to improve the application of transformer networks for speech recognition by addressing the challenges of combining positional embedding with speech features and ensuring stability in large-scale learning. The research is motivated by the recent success of transformer networks in other NLP tasks, leading to a growing interest in applying them to speech recognition. The researchers aim to tackle the specific challenges faced when adapting transformers to speech processing.",train,"contextual representations, convolutionally learned input representations, large scale learning of transformer networks, neural machine translation, NLP tasks, positional embedding, sinusoidal positional embedding, speech recognition, transformer networks, transformers"
204,3046,6192,ABC_4d25b647a261a415d769532386265a_38,ARXIV:1904.11660,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"To model word order, sinusoidal positional embeddings are used [11] .",Uses,s2,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",train,1,"The research aims to improve the application of transformer networks for speech recognition by addressing the challenges of combining positional embedding with speech features and ensuring stability in large-scale learning. The research is motivated by the recent success of transformer networks in other NLP tasks, leading to a growing interest in applying them to speech recognition. The researchers aim to tackle the specific challenges faced when adapting transformers to speech processing.",train,"contextual representations, convolutionally learned input representations, large scale learning of transformer networks, neural machine translation, NLP tasks, positional embedding, sinusoidal positional embedding, speech recognition, transformer networks, transformers"
205,3047,6193,ABC_4d25b647a261a415d769532386265a_38,ARXIV:1904.11660,612a42e2fa4c33b609aade451528d3c11989f88a,"In table (1) we show the WER of the proposed transformer encoder-decoder model with convolutional context using the canonical configuration in the first row. Replacing the 1-D convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers [11] and adopted in [13, 15] , shows inferior WER performance. By combining sinusoidal and convolutional position embedding (rows 1+2), we don't observe any gains. This supports our intuition that the relative convolutional positional information provides sufficient signal for the transformer layers to recreate more global word order.",Uses,s2,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.",train,1,"The research aims to improve the application of transformer networks for speech recognition by addressing the challenges of combining positional embedding with speech features and ensuring stability in large-scale learning. The research is motivated by the recent success of transformer networks in other NLP tasks, leading to a growing interest in applying them to speech recognition. The researchers aim to tackle the specific challenges faced when adapting transformers to speech processing.",train,"contextual representations, convolutionally learned input representations, large scale learning of transformer networks, neural machine translation, NLP tasks, positional embedding, sinusoidal positional embedding, speech recognition, transformer networks, transformers"
206,3048,6194,ABC_4d25b647a261a415d769532386265a_38,ARXIV:1904.11660,612a42e2fa4c33b609aade451528d3c11989f88a,"In table (1) we show the WER of the proposed transformer encoder-decoder model with convolutional context using the canonical configuration in the first row. Replacing the 1-D convolutional context in the decoder with sinusoidal positional embedding, as proposed in the baseline machine translation transformers [11] and adopted in [13, 15] , shows inferior WER performance. By combining sinusoidal and convolutional position embedding (rows 1+2), we don't observe any gains. This supports our intuition that the relative convolutional positional information provides sufficient signal for the transformer layers to recreate more global word order.",Background,s2,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.",train,0,"The research aims to improve the application of transformer networks for speech recognition by addressing the challenges of combining positional embedding with speech features and ensuring stability in large-scale learning. The research is motivated by the recent success of transformer networks in other NLP tasks, leading to a growing interest in applying them to speech recognition. The researchers aim to tackle the specific challenges faced when adapting transformers to speech processing.",train,"contextual representations, convolutionally learned input representations, large scale learning of transformer networks, neural machine translation, NLP tasks, positional embedding, sinusoidal positional embedding, speech recognition, transformer networks, transformers"
207,3049,6195,ABC_4d25b647a261a415d769532386265a_38,ARXIV:1904.11660,204e3073870fae3d05bcbc2f6a8e263d9b72e776,Figure(1) -left show the details of one transformer layer as proposed by [11] .,Uses,s2,"The recent success of transformer networks for neural machine translation and other NLP tasks has led to a surge in research work trying to apply it for speech recognition. Recent efforts studied key research questions around ways of combining positional embedding with speech features, and stability of optimization for large scale learning of transformer networks. In this paper, we propose replacing the sinusoidal positional embedding for transformers with convolutionally learned input representations. These contextual representations provide subsequent transformer blocks with relative positional information needed for discovering long-range relationships between local concepts. The proposed system has favorable optimization characteristics where our reported results are produced with fixed learning rate of 1.0 and no warmup steps. The proposed model achieves a competitive 4.7% and 12.9% WER on the Librispeech ``test clean'' and ``test other'' subsets when no extra LM text is provided.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",train,1,"The research aims to improve the application of transformer networks for speech recognition by addressing the challenges of combining positional embedding with speech features and ensuring stability in large-scale learning. The research is motivated by the recent success of transformer networks in other NLP tasks, leading to a growing interest in applying them to speech recognition. The researchers aim to tackle the specific challenges faced when adapting transformers to speech processing.",train,"contextual representations, convolutionally learned input representations, large scale learning of transformer networks, neural machine translation, NLP tasks, positional embedding, sinusoidal positional embedding, speech recognition, transformer networks, transformers"
208,3293,6675,ABC_8fc0d25eb177ea876c2b69096f0145_45,ARXIV:1906.09912,0fe73c19513dfd17372d8ef58da0d0149725832c,"Consequently, it is unknown if Indonesian word embeddings introduced in, e.g., (Al-Rfou et al., 2013) and (Grave et al., 2018) , capture syntactic or semantic information as measured by analogy tasks.",Motivation,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.",train,0,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
209,3294,6676,ABC_8fc0d25eb177ea876c2b69096f0145_45,ARXIV:1906.09912,e2dba792360873aef125572812f3673b1a85d850,"We also used another two pretrained embeddings: polyglot embedding trained on Indonesian Wikipedia (Al-Rfou et al., 2013) and NLPL embedding trained on the Indonesian portion of CoNLL 2017 corpus (Fares et al., 2017) .",Uses,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",train,1,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
210,3295,6677,ABC_8fc0d25eb177ea876c2b69096f0145_45,ARXIV:1906.09912,8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731,"Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens.",Background,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.",train,0,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
211,3296,6678,ABC_8fc0d25eb177ea876c2b69096f0145_45,ARXIV:1906.09912,8e3f0f7a761f18cb91c11764d8d6cb3b1e9c5731,"Also, such embeddings are usually trained on Indonesian Wikipedia (Al-Rfou et al., 2013; Bojanowski et al., 2017) whose size is relatively small, approximately 60M tokens.",Motivation,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations (word embeddings) have recently contributed to competitive performance in language modeling and several NLP tasks. In this work, we train word embeddings for more than 100 languages using their corresponding Wikipedias. We quantitatively demonstrate the utility of our word embeddings by using them as the sole features for training a part of speech tagger for a subset of these languages. We find their performance to be competitive with near state-of-art methods in English, Danish and Swedish. Moreover, we investigate the semantic features captured by these embeddings through the proximity of word groupings. We will release these embeddings publicly to help researchers in the development and enhancement of multilingual applications.",train,0,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
212,3746,7706,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,12e9d005c77f76e344361f79c4b008034ae547eb,We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016) .,Uses,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.",train,1,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
213,3747,7707,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,e2dba792360873aef125572812f3673b1a85d850,"On the other hand, in the field of word embedding construction, some previous researchers found that character n-grams are more useful than single characters (Wieting et al. 2016; Bojanowski et al. 2017) . In particular, (Wieting et al. 2016) demonstrated that constructing word embeddings from character n-gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM). Based on their reports, in this paper, we propose a neural language model that utilizes character n-gram embeddings.",Uses,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",train,1,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
214,3748,7708,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,0be9ca65ad318ee3729928882ef2c403d4b6d24e,"Since AWD-LSTM-MoS (Yang et al. 2018 ) and AWD-LSTM-DOC (Takase, Suzuki, and Nagata 2018) achieved the stateof-the-art scores on PTB and WT2, we combined char3-MSvec with them.",Uses,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also middle layers. This method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). Our proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to application tasks: machine translation and headline generation.",train,1,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
215,3749,7709,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,e2dba792360873aef125572812f3673b1a85d850,"We incorporate charn-MS-vec, which is an embedding constructed from character n-gram embeddings, into RNN language models since, as discussed earlier, previous studies revealed that we can construct better word embeddings by using character n-gram embeddings (Wieting et al. 2016; Bojanowski et al. 2017 ).",Uses,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",train,1,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
216,3750,7710,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,38f35dd624cd1cf827416e31ac5e0e0454028eca,"Finally, (Merity, Keskar, and Socher 2018b) introduced DropConnect (Wan et al. 2013 ) and averaged SGD (Polyak and Juditsky 1992) into the LSTM language model and achieved state-of-the-art perplexities on PTB and WT2.",Uses,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We introduce DropConnect, a generalization of Dropout (Hinton et al., 2012), for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recognition benchmarks by aggregating multiple DropConnect-trained models.",train,1,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
217,3751,7711,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,4d4b46e545e1a3f6871b49cc69640ef2eb1a4654,"For example, neural encoderdecoder models, which are becoming the de facto standard for various natural language generation tasks including machine translation (Sutskever, Vinyals, and Le 2014) , summarization (Rush, Chopra, and Weston 2015) , dialogue (Wen et al. 2015) , and caption generation (Vinyals et al. 2015) can be interpreted as conditional neural language models.",Background,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","© 2015 Association for Computational Linguistics. Natural language generation (NLG) is a critical component of spoken dialogue and it has a significant impact both on usability and perceived quality. Most NLG systems in common use employ rules and heuristics and tend to generate rigid and stylised responses without the natural variation of human language. They are also not easily scaled to systems covering multiple domains and languages. This paper presents a statistical language generator based on a semantically controlled Long Short-term Memory (LSTM) structure. The LSTM generator can learn from unaligned data by jointly optimising sentence planning and surface realisation using a simple cross entropy training criterion, and language variation can be easily achieved by sampling from output candidates. With fewer heuristics, an objective evaluation in two differing test domains showed the proposed method improved performance compared to previous methods. Human judges scored the LSTM system higher on informativeness and naturalness and overall preferred it to the other systems..",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
218,3752,7712,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,12e9d005c77f76e344361f79c4b008034ae547eb,"Based on the research in the field of word embedding construction (Wieting et al. 2016) , we focused on character n-gram embeddings to construct word embeddings.",Uses,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.",train,1,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
219,3753,7713,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,8e091d4e247f74ff30f0ff7fe7426574b8817474,"In general, neural language models require word embeddings as an input (Zaremba, Sutskever, and Vinyals 2014). However, as described by (Verwimp et al. 2017) , this approach cannot make use of the internal structure of words although the internal structure is often an effective clue for considering the meaning of a word.",Background,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
220,3754,7714,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,8e091d4e247f74ff30f0ff7fe7426574b8817474,"To incorporate the internal structure, (Verwimp et al. 2017) concatenated character embeddings with an input word embedding.",Background,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
221,3755,7715,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,e2dba792360873aef125572812f3673b1a85d850,"On the other hand, in the field of word embedding construction, some previous researchers found that character n-grams are more useful than single characters (Wieting et al. 2016; Bojanowski et al. 2017) . In particular, (Wieting et al. 2016) demonstrated that constructing word embeddings from character n-gram embeddings outperformed the methods that construct word embeddings from character embeddings by using CNN or a Long Short-Term Memory (LSTM).",Background,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
222,3756,7716,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,8e091d4e247f74ff30f0ff7fe7426574b8817474,"In general, neural language models require word embeddings as an input (Zaremba, Sutskever, and Vinyals 2014). However, as described by (Verwimp et al. 2017) , this approach cannot make use of the internal structure of words although the internal structure is often an effective clue for considering the meaning of a word. For example, we can comprehend that the word 'causal' is related to 'cause' immediately because both words include the same character sequence 'caus'. Thus, if we incorporate a method that handles the internal structure such as character information, we can improve the quality of neural language models and probably make them robust to infrequent words.",Motivation,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present a Character-Word Long Short-Term Memory Language Model which both reduces the perplexity with respect to a baseline word-level language model and reduces the number of parameters of the model. Character information can reveal structural (dis)similarities between words and can even be used when a word is out-of-vocabulary, thus improving the modeling of infrequent and unknown words. By concatenating word and character embeddings, we achieve up to 2.77% relative improvement on English compared to a baseline model with a similar amount of parameters and 4.57% on Dutch. Moreover, we also outperform baseline word-level models with a larger number of parameters.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
223,3757,7717,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,e2dba792360873aef125572812f3673b1a85d850,"On the other hand, (Bojanowski et al. 2017 ) and (Wieting et al. 2016) focused on character n-gram.",Background,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
224,3758,7718,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,12e9d005c77f76e344361f79c4b008034ae547eb,"In addition, (Wieting et al. 2016) found that the sum of character n-gram embeddings also outperformed word embeddings constructed from character embeddings with CNN and LSTM.",Background,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
225,3759,7720,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,12e9d005c77f76e344361f79c4b008034ae547eb,"In addition, (Wieting et al. 2016) found that the sum of character n-gram embeddings also outperformed word embeddings constructed from character embeddings with CNN and LSTM. As an encoder, previous studies argued that additive composition, which computes the (weighted) sum of embeddings, is a suitable method theoretically (Tian, Okazaki, and Inui 2016) and empirically (Muraoka et al. 2014; . In this paper, we used multidimensional self-attention to construct word embeddings because it can be interpreted as an element-wise weighted sum.",Motivation,s2,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
226,3783,7778,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"Moreover, Finegan-Dollak et al. (2018) demonstrated that the sequence-tosequence model also lack the ability to generate SQL queries of unseen templates.",Background,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
227,3784,7779,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"However, Finegan-Dollak et al. (2018) showed that the sequence-to-tree approach was inefficient when generating complex SQL queries from a natural language question.",Background,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
228,3786,7781,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"Iyer et al. (2017); Finegan-Dollak et al. (2018) focused on the dataset that contains more complex queries such as ATIS (Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996) .",Background,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
229,3787,7782,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"Sequence-to-sequence models (Iyer et al., 2017; Finegan-Dollak et al., 2018) , as shown in the Table 3 , showed poor performance for the query-based split in the zero-shot setting. The model from Finegan-Dollak et al. (2018) showed accuracies of 0%, 32%, 20%, and 5% for each benchmark and accuracies of Iyer et al. (2017) showed 1%, 17%, 40%, and 3%, meaning that they also lack the capability to generate unseen templates of SQL.",Background,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
230,3788,7783,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"plied a sequence-to-sequence approach with attention mechanism, and Finegan-Dollak et al. (2018) proposed a template-based model and another sequence-to-sequence model with a copy mechanism. However, Finegan-Dollak et al. (2018) showed that both approaches lack the ability to generate SQL of the unseen template in the training stage. One-shot Learning/Matching Network Deep learning models usually require hundreds or thousands of examples in order to learn a class. To overcome this limitation, one-shot learning aims to learn a class from a single labeled example. We applied one-shot learning to the text-to-SQL task so that our model could learn a SQL template from just a few examples and adapt easily and promptly to the SQL of untrained templates.",Motivation,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
231,3789,7784,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"Moreover, Finegan-Dollak et al. (2018) demonstrated that the sequence-tosequence model also lack the ability to generate SQL queries of unseen templates. In this work, we propose an extension of a template-based model with one-shot learning, which can generate SQL queries of untrained templates based on a single example.",Extention,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,1,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
232,3790,7785,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"plied a sequence-to-sequence approach with attention mechanism, and Finegan-Dollak et al. (2018) proposed a template-based model and another sequence-to-sequence model with a copy mechanism.",Background,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
233,3791,7786,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"This architecture is based on an idea similar to the template-based model of Finegan-Dollak et al. (2018) . However, the previous model requires a number of examples for each template and needs retraining to support new templates of SQL. Conversely, we applied one-shot learning so that our model could learn a template with just a single example.",Difference,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
234,3792,7787,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"Our model shows 3-27% query generation accuracy gain, compared to the sequence-to-sequence model, 5-9% gain, compared to template-based model (Finegan-Dollak et al., 2018) , and 15-56% gain, compared to Iyer et al. (2017) .",Difference,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
235,3793,7788,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"The proposed model has three advantages. 2. It minimizes unnecessary search space, unlike sequence-to-sequence approaches (Iyer et al., 2017; Finegan-Dollak et al., 2018) ; thus, the model is guaranteed to be free of SQL syntax errors.",Difference,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
236,3794,7789,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"We conducted experiments with four different text-to-SQL datasets on both of the questionbased split and query-based split (Finegan-Dollak et al., 2018) .",Uses,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,1,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
237,3795,7790,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"plied a sequence-to-sequence approach with attention mechanism, and Finegan-Dollak et al. (2018) proposed a template-based model and another sequence-to-sequence model with a copy mechanism. However, Finegan-Dollak et al. (2018) showed that both approaches lack the ability to generate SQL of the unseen template in the training stage. One-shot Learning/Matching Network Deep learning models usually require hundreds or thousands of examples in order to learn a class. To overcome this limitation, one-shot learning aims to learn a class from a single labeled example. We applied one-shot learning to the text-to-SQL task so that our model could learn a SQL template from just a few examples and adapt easily and promptly to the SQL of untrained templates.",Difference,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
238,3796,7791,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,We used a SQL version of the dataset processed by Finegan-Dollak et al. (2018) .,Uses,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,1,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
239,3797,7792,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,We used a template and variables for each SQL from the preprocessed versions provided by Finegan-Dollak et al. (2018) .,Uses,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,1,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
240,3798,7793,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"Sequence-to-sequence models (Iyer et al., 2017; Finegan-Dollak et al., 2018) , as shown in the Table 3 , showed poor performance for the query-based split in the zero-shot setting. The model from Finegan-Dollak et al. (2018) showed accuracies of 0%, 32%, 20%, and 5% for each benchmark and accuracies of Iyer et al. (2017) showed 1%, 17%, 40%, and 3%, meaning that they also lack the capability to generate unseen templates of SQL. In a one-shot setting, where an example is added for each new template, our approach outperformed previous ones against every benchmark. Our model outperforms the sequence-to-sequence model (Finegan-Dollak et al., 2018) by 1-60%, the template-based model (Finegan-Dollak et al., 2018) by 17-52%, Iyer et al. (2017) by 14-62%.",Difference,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
241,3799,7794,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"We compare our results with three different previous approaches: a sequence-to-sequence model from Iyer et al. (2017) , template-based model, and another sequence-to-sequence model from Finegan-Dollak et al. (2018) .",Uses,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,1,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
242,3800,7795,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"We evaluated the query generation accuracy for both the question-based split and query-based split (Finegan-Dollak et al., 2018) .",Uses,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,1,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
243,3801,7796,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,This architecture is based on an idea similar to the template-based model of Finegan-Dollak et al. (2018) .,Similar,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
244,3802,7797,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"For the query-based split, we used the same split as in Finegan-Dollak et al. (2018) .",Uses,s2,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,1,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
245,3803,7861,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,210feb22ff541920caa4884e73eaff1c09644114,"After training, the aim is to execute, zero-shot, novel instructions such as ""walk around right after look twice."" Previous studies show that seq2seq recurrent neural networks (RNN) generalize well when the training and test sets are similar, but fail catastrophically when generalization requires systematic compositionality [15, 3, 20] .",Background,s2,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it’s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni’s main experiment required the models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as “around” and “right”) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of “X around right” to “jump around right”), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of “around right” from those of “right” and “around”).",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
246,3804,7864,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,210feb22ff541920caa4884e73eaff1c09644114,"Recent work has revisited these classic critiques through studies of modern neural architectures [10, 15, 3, 20, 22, 2, 6] , with a focus on the sequence-to-sequence (seq2seq) models used successfully in machine translation and other NLP tasks [32, 4, 36] .",Background,s2,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it’s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni’s main experiment required the models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as “around” and “right”) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of “X around right” to “jump around right”), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of “around right” from those of “right” and “around”).",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
247,3805,7865,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,210feb22ff541920caa4884e73eaff1c09644114,"After learning how to ""dax,"" people understand how to ""dax twice,"" ""dax slowly,"" or even ""dax like there is no tomorrow."" These abilities are central to language and thought yet they are conspicuously lacking in modern neural networks [15, 3, 20, 22, 2] .",Background,s2,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it’s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni’s main experiment required the models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as “around” and “right”) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of “X around right” to “jump around right”), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of “around right” from those of “right” and “around”).",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
248,3806,7866,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,856fe866bcce5e7a540655bea6ecc7406bdcfcba,"The goal is to learn a new primitive instruction and use it compositionally, operationalized in SCAN as the ""add jump"" split [15] . Models learn a new primitive ""jump"" and aim to use it compositionally in other instructions, resembling the ""to Facebook"" example introduced earlier in this paper. First, the original seq2seq problem from [15] is described.",Uses,s2,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb ""dax,"" he or she can immediately understand the meaning of ""dax twice"" or ""sing and dax."" In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply ""mix-and-match"" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the ""dax"" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks' notorious training data thirst.",train,1,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
249,3807,7867,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,210feb22ff541920caa4884e73eaff1c09644114,"After learning how to ""dax,"" people understand how to ""dax twice,"" ""dax slowly,"" or even ""dax like there is no tomorrow."" These abilities are central to language and thought yet they are conspicuously lacking in modern neural networks [15, 3, 20, 22, 2] . In this paper, I introduced a meta sequence-to-sequence (meta seq2seq) approach for learning to generalize compositionally, exploiting the algebraic structure of a domain to help understand novel utterances.",Motivation,s2,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it’s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni’s main experiment required the models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as “around” and “right”) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of “X around right” to “jump around right”), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of “around right” from those of “right” and “around”).",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
250,3808,7868,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,b59d91e0699d4e1896a15bae13fd180bdaf77ea5,"In future work, I intend to explore adding more symbolic machinery to the architecture [27] with the goal of handling genuinely new symbols. Hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks [15, 3, 28] including meta seq2seq learning.",Future Work,s2,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
251,3810,7870,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,210feb22ff541920caa4884e73eaff1c09644114,"After learning how to ""dax,"" people understand how to ""dax twice,"" ""dax slowly,"" or even ""dax like there is no tomorrow."" These abilities are central to language and thought yet they are conspicuously lacking in modern neural networks [15, 3, 20, 22, 2] . In this paper, I introduced a meta sequence-to-sequence (meta seq2seq) approach for learning to generalize compositionally, exploiting the algebraic structure of a domain to help understand novel utterances. In contrast to standard seq2seq, meta seq2seq learners can abstract away the surface patterns and operate closer to rule space.",Difference,s2,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it’s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni’s main experiment required the models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as “around” and “right”) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of “X around right” to “jump around right”), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of “around right” from those of “right” and “around”).",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
252,4164,8580,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,235780312bf7ead98d7a6332e01d846f12090d2a,"Multimodal corpora such as Flickr30k [1] or MSCOCO [2] containing images along with natural language captions were made available for research. They were soon extended with speech modality: speech recordings for the captions of Flickr8k were collected by [3] via crowdsourcing; spoken captions for MSCOCO were generated using Google Text-To-Speech (TTS) by [4] and using Voxygen TTS by [5] ; extensions of these corpora to other languages than English, such as Japanese, were also introduced by [6] .",Background,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","This paper presents an augmentation of MSCOCO dataset where speech is added to image and text. Speech captions are generated using text-to-speech (TTS) synthesis resulting in 616,767 spoken captions (more than 600h) paired with images. Disfluencies and speed perturbation are added to the signal in order to sound more natural. Each speech signal (WAV) is paired with a JSON file containing exact timecode for each word/syllable/phoneme in the spoken caption. Such a corpus could be used for Language and Vision (LaVi) tasks including speech input or output instead of text. Investigating multimodal learning schemes for unsupervised speech pattern discovery is also possible with this corpus, as demonstrated by a preliminary study conducted on a subset of the corpus (10h, 10k spoken captions).",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
253,4165,8584,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,f54d9dbad1f60de83485232707c945f209af867e,"These corpora, as well as deep learning models, lead to contributions in multilingual language grounding and learning of shared and multimodal representations with neural networks [4, 7, 8, 9, 10, 11, 12, 13] .",Background,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","In this paper, we explore the learning of neural network embeddings for natural images and speech waveforms describing the content of those images. These embeddings are learned directly from the waveforms without the use of linguistic transcriptions or conventional speech recognition technology. While prior work has investigated this setting in the monolingual case using English speech data, this work represents the first effort to apply these techniques to languages beyond English. Using spoken captions collected in English and Hindi, we show that the same model architecture can be successfully applied to both languages. Further, we demonstrate that training a multilingual model simultaneously on both languages offers improved performance over the monolingual models. Finally, we show that these models are capable of performing semantic cross-lingual speech-to-speech retrieval.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
254,4166,8585,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,0d719664b18578445931e9aa3e70e3672c9898ac,"While [11, 7, 4] focused on analyzing speech representations learnt by speech-image neural models from a phonological and semantic point of view, the present work focuses on lexical acquisition and the way speech utterances are segmented into lexical units and processed by a computational model of visually grounded speech. We analyze a key component of the neural model -the attention mechanism -and we observe its behaviour and draw parallels between artificial neural attention and human attention.",Difference,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
255,4167,8586,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,d4e8117aed23e806fbe7a623a4ff915ef60e6bef,"Contrary to the original model ( [4] ), we used GRU units instead of RHN units.",Difference,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
256,4168,8587,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,d4e8117aed23e806fbe7a623a4ff915ef60e6bef,"This paper focuses on computational models of visually grounded speech that were introduced by [14, 4] .",Background,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
257,4169,8588,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,0d719664b18578445931e9aa3e70e3672c9898ac,"Learned representations of such models were analyzed by [11, 7, 4] : [11] introduced novel methods for interpreting the activation patterns of recurrent neural networks (RNN) in a model of visually grounded meaning representation from textual and visual input and showed that RNN pay attention to word tokens belonging to specific lexical categories. [4] found that final layers tend to encode semantic information whereas lower layers tend to encode form-related information.",Background,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","We study the representation and encoding of phonemes in a recurrent neural network model of grounded speech. We use a model which processes images and their spoken descriptions, and projects the visual and auditory representations into the same semantic space. We perform a number of analyses on how information about individual phonemes is encoded in the MFCC features extracted from the speech signal, and the activations of the layers of the model. Via experiments with phoneme decoding and phoneme discrimination we show that phoneme representations are most salient in the lower layers of the model, where low-level signals are processed at a fine-grained level, although a large amount of phonological information is retain at the top recurrent layer. We further find out that the attention mechanism following the top recurrent layer significantly attenuates encoding of phonology and makes the utterance embeddings much more invariant to synonymy. Moreover, a hierarchical clustering of phoneme representations learned by the network shows an organizational structure of phonemes similar to those proposed in linguistics.",train,0,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
258,4170,8589,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,d4e8117aed23e806fbe7a623a4ff915ef60e6bef,"The model we use for our experiments is based on that of [4] . It is trained to solve an image retrieval task: given a spoken description it retrieves the closest image that matches the description. To do so, the model projects an image and its spoken description in a common representation space, so that matching image/utterance pairs lie near while mismatching image/utterance pairs lie apart.",Uses,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",train,1,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
259,4171,8590,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,d4e8117aed23e806fbe7a623a4ff915ef60e6bef,Spoken COCO dataset was introduced by [4] for English.,Uses,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",train,1,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
260,4172,8593,ABC_7ac01a84ab696e7fa9d0ce336a393e_5,CorpusID:59843099,d4e8117aed23e806fbe7a623a4ff915ef60e6bef,"In the original architecture ( [4] ), attention follows the last recurrent layer. To have more insight on the representation learnt by the network, we added an attention mechanism after the first recurrent layer.",Extention,s2,"We investigate the behaviour of attention in neural models of visually grounded speech trained on two languages: English and Japanese. Experimental results show that attention focuses on nouns and this behaviour holds true for two very typologically different languages. We also draw parallels between artificial neural attention and human attention and show that neural attention focuses on word endings as it has been theorised for human attention. Finally, we investigate how two visually grounded monolingual models can be used to perform cross-lingual speech-to-speech retrieval. For both languages, the enriched bilingual (speech-image) corpora with part-of-speech tags and forced alignments are distributed to the community for reproducible research.","We present a visually grounded model of speech perception which projects spoken utterances and images to a joint semantic space. We use a multi-layer recurrent highway network to model the temporal nature of spoken speech, and show that it learns to extract both form and meaning-based linguistic knowledge from the input signal. We carry out an in-depth analysis of the representations used by different components of the trained model and show that encoding of semantic aspects tends to become richer as we go up the hierarchy of layers, whereas encoding of form-related aspects of the language input tends to initially increase and then plateau or decrease.",train,1,"The primary research problems are understanding the behavior of attention in neural models trained on different languages (English and Japanese), specifically its focus on nouns and word endings, and exploring the use of monolingual models for cross-lingual speech-to-speech retrieval. The research is motivated by the desire to understand attention mechanisms in neural models, particularly in the context of multilingual training and cross-lingual tasks. This understanding could lead to improved models and a deeper understanding of language processing. The authors also aim to contribute to the research community by making their resources publicly available for reproducible research.",train,"artificial neural attention, cross-lingual speech-to-speech retrieval, English, enriched bilingual ( speech-image ) corpora, Japanese, neural models of visually grounded speech, visually grounded monolingual models, visually grounded speech"
261,4730,9807,ABC_f3282df3adadf78320e99c09d8384f_0,ACL:W19-0804,634b8216dcd15c19680d3790df117beb456e5daa,"Accordingly, <span style=""background: yellow; display: inline-block"">they</span> use Doc2Vec (Le and Mikolov (2014) ) as one of their baselines, and show that its performance is inferior to <span style=""background: yellow; display: inline-block"">their</span> method.",Background,llm,"We present a very simple, unsupervised method for the pairwise matching of documents from heterogeneous collections. We demonstrate our method with the Concept-Project matching task, which is a binary classification task involving pairs of documents from heterogeneous collections. Although our method only employs standard resources without any domain- or task-specific modifications, it clearly outperforms the more complex system of the original authors. In addition, our method is transparent, because it provides explicit information about how a similarity score was computed, and efficient, because it is based on the aggregation of (pre-computable) word-level similarities.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,0,"The research problem is to develop a method for effectively matching documents from heterogeneous collections, specifically tackling the Concept-Project matching task, which involves classifying pairs of documents from different sources. The motivation behind the research is to present a simpler and more efficient alternative to existing complex methods for document matching. The proposed method aims to outperform existing methods while being transparent in its calculations.",train,"binary classification task, Concept-Project matching task, heterogeneous collections, heterogeneous collections, unsupervised method"
262,4734,9829,ABC_418016e4df12f80205cadc41119244_1,ARXIV:1907.00720,6fc991dbc1714b425d11b4de3d9d247d21d77c0b,"Actually we find Inspired by <span style=""background: yellow; display: inline-block"">[2]</span> , we formulate BioCS as a sequence tagging problem.",Motivation,llm,"Conditions are essential in the statements of biological literature. Without the conditions (e.g., environment, equipment) that were precisely specified, the facts (e.g., observations) in the statements may no longer be valid. One biological statement has one or multiple fact(s) and/or condition(s). Their subject and object can be either a concept or a concept's attribute. Existing information extraction methods do not consider the role of condition in the biological statement nor the role of attribute in the subject/object. In this work, we design a new tag schema and propose a deep sequence tagging framework to structure conditional statement into fact and condition tuples from biological text. Experiments demonstrate that our method yields a information-lossless structure of the literature.","We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our supervised model outperforms the existing state-of-the-art Open IE systems on benchmark datasets.",train,0,"The research problem focuses on the critical role of conditions in biological statements, highlighting the need for accurate and precise specification of these conditions for valid interpretations. Existing information extraction methods fail to consider conditions and attributes adequately, leading to incomplete or inaccurate understanding of biological literature. The motivation stems from the importance of accurate interpretation of biological statements, which relies on understanding the specific conditions under which the facts were observed. The research aims to develop a method that accurately captures and represents these conditions, thus improving the understanding and analysis of biological literature.",train,"biological statement, biological text, deep sequence tagging framework, tag schema"
263,4736,9831,ABC_cf2cc67035107f5bdaab85a760e56e_1,ARXIV:1906.09912,e2dba792360873aef125572812f3673b1a85d850,"We found that (1) in general, accuracies on the analogy tasks were low, suggesting that improvements for Indonesian word embeddings are still possible and KaWAT is hard enough to be the benchmark dataset for that purpose, (2) on syntactic analogies, embedding by (<span style=""background: yellow; display: inline-block"">Bojanowski et al., 2017</span>) performed best and yielded 20% fewer training epochs when employed for POS tagging, and (3) on semantic analogies, GloVe embedding trained on Tempo corpus performed best and produced significant gains on ROUGE-1 and ROUGE-L scores when used for text summarization.",Uses,llm,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",train,1,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
264,4737,9832,ABC_cf2cc67035107f5bdaab85a760e56e_1,ARXIV:1906.09912,e2dba792360873aef125572812f3673b1a85d850,"To train the word embeddings, we experimented with three algorithms: word2vec (Mikolov et al., 2013b) , <span style=""background: yellow; display: inline-block"">fastText</span> (<span style=""background: yellow; display: inline-block"">Bojanowski et al., 2017</span>) , and GloVe (Pennington et al., 2014) . We used gensim 3 to run word2vec and <span style=""background: yellow; display: inline-block"">fastText</span> and the original C implementation for GloVe.",Uses,llm,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",train,1,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
265,4810,9979,ABC_95883b369c4b019fa98493a728c1a0_2,ARXIV:1906.08379,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Specifically, we find that the bias metric proposed by <span style=""background: yellow; display: inline-block"">(Bolukbasi et al. 2016 )</span> is highly sensitive to embedding hyperparameter selection, and that in many cases, the variance due to the selection of some hyper-parameters, notably the embedding space dimensionality, is greater than the variance in the metric due to corpus selection, while in fewer cases, even the relative rankings of the bias measured in the embedding spaces of various corpora varies with hyper-parameter selection.",Difference,llm,"Word embedding spaces are powerful tools for capturing latent semantic relationships between terms in corpora, and have become widely popular for building state-of-the-art natural language processing algorithms. However, studies have shown that societal biases present in text corpora may be incorporated into the word embedding spaces learned from them. Thus, there is an ethical concern that human-like biases contained in the corpora and their derived embedding spaces might be propagated, or even amplified with the usage of the biased embedding spaces in downstream applications. In an attempt to quantify these biases so that they may be better understood and studied, several bias metrics have been proposed. We explore the statistical properties of these proposed measures in the context of their cited applications as well as their supposed utilities. We find that there are caveats to the simple interpretation of these metrics as proposed. We find that the bias metric proposed by Bolukbasi et al. 2016 is highly sensitive to embedding hyper-parameter selection, and that in many cases, the variance due to the selection of some hyper-parameters is greater than the variance in the metric due to corpus selection, while in fewer cases the bias rankings of corpora vary with hyper-parameter selection. In light of these observations, it may be the case that bias estimates should not be thought to directly measure the properties of the underlying corpus, but rather the properties of the specific embedding spaces in question, particularly in the context of hyper-parameter selections used to generate them. Hence, bias metrics of spaces generated with differing hyper-parameters should be compared only with explicit consideration of the embedding-learning algorithms particular configurations.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The main research problem is the presence of societal biases in word embedding spaces learned from text corpora. This includes the ethical concerns of these biases being propagated or amplified in downstream applications, as well as the limitations of existing bias metrics in accurately capturing and interpreting these biases. The primary motivation behind this research is to understand and quantify biases present in word embedding spaces to mitigate their negative impact. The study aims to provide a more nuanced understanding of bias metrics and their limitations, ultimately contributing to the development of more reliable and ethical methods for working with word embeddings.",train,"bias estimates, corpus selection, embedding hyper-parameter selection, embedding spaces, embedding-learning algorithms, natural language processing algorithms, text corpora"
266,4811,10001,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,634b8216dcd15c19680d3790df117beb456e5daa,"Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the <span style=""background: yellow; display: inline-block"">WinoBias probing corpus</span>.",Uses,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
267,4812,10002,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,634b8216dcd15c19680d3790df117beb456e5daa,"Table 2 : F1 on OntoNotes and <span style=""background: yellow; display: inline-block"">WinoBias</span> development sets.",Uses,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
268,4813,10003,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"ELMo Bias Transfers to Coreference Row 3 in Table 2 summarizes performance of the ELMo based coreference system on <span style=""background: yellow; display: inline-block"">WinoBias</span>.",Uses,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
269,4814,10004,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,634b8216dcd15c19680d3790df117beb456e5daa,"We evaluate the performance of both aspects of <span style=""background: yellow; display: inline-block"">this approach</span>.",Uses,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
270,4815,10005,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Table 2 summarizes our results on <span style=""background: yellow; display: inline-block"">WinoBias</span>.",Uses,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
271,4816,10006,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,634b8216dcd15c19680d3790df117beb456e5daa,"It exhibits large differences between pro-and anti-stereotyped sets (|Diff|) on both semantic and syntactic examples in <span style=""background: yellow; display: inline-block"">WinoBias</span>.",Uses,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
272,4817,10007,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on <span style=""background: yellow; display: inline-block"">WinoBias</span> can be eliminated.",Extention,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
273,4818,10008,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,634b8216dcd15c19680d3790df117beb456e5daa,"<span style=""background: yellow; display: inline-block"">WinoBias dataset</span> is split Semantics Only and w/ Syntactic Cues subsets.",Background,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
274,4819,10009,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"<span style=""background: yellow; display: inline-block"">It</span> contains two different subsets, pro-stereotype, where pronouns are associated with occupations predominately associated with the gender of the pronoun, or anti-stereotype, when the opposite relation is true.",Background,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
275,4820,10010,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,5d833331b0e22ff359db05c62a8bca18c4f04b68,"The analysis shows that the <span style=""background: yellow; display: inline-block"">Billion Word corpus</span> contains a significant skew with respect to gender: (1) male pronouns occur three times more than female pronouns and (2) male pronouns co-occur more frequently with occupation words, irrespective of whether they are prototypically male or female.",Motivation,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","We propose a new benchmark corpus to be used for measuring progress in statistical language modeling. With almost one billion words of training data, we hope this benchmark will be useful to quickly evaluate novel language modeling techniques, and to compare their contribution when combined with other advanced techniques. We show performance of several well-known types of language models, with the best results achieved with a recurrent neural network based language model. The baseline unpruned KneserNey 5-gram model achieves perplexity 67.6. A combination of techniques leads to 35% reduction in perplexity, or 10% reduction in cross-entropy (bits), over that baseline. The benchmark is available as a code.google.com project; besides the scripts needed to rebuild the training/held-out data, it also makes available log-probability values for each word in each of ten held-out data sets, for each of the baseline n-gram models.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
276,4821,10011,ABC_3d6df70136820f74ce76f60a59cc42_3,ACL:N19-1064,634b8216dcd15c19680d3790df117beb456e5daa,"Neutralization is less effective than augmentation and cannot fully remove gender bias on the Semantics Only portion of <span style=""background: yellow; display: inline-block"">WinoBias</span>, indicating it is effective only for simpler cases.",Motivation,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
277,4983,10368,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"We validated our model by replicating gender and ethnic stereotypes produced in <span style=""background: yellow; display: inline-block"">(Garg et al., 2018)</span> by training multiple word embedding models and applied it to a novel corpus of talk radio data to analyze how perceptions of refugees as ""outsiders"" vary by geography and over time.",Similar,llm,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,0,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
278,4984,10369,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"The correlation between our scores and changes in workforce participation rates are similar to the correlation between the scores from <span style=""background: yellow; display: inline-block"">(Garg et al., 2018)</span> and the same (r = 0.8, p = 0.01 and r = 0.81, p < 0.01, respectively, for gender occupation bias; r = 0.84, p < 0.01 and r = 0.79, p = 0.01, respectively, for Asian/White occupation bias).",Similar,llm,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,0,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
279,4985,10370,ABC_34b73a56bd9b80dc415ca2c5608596_15,ACL:W19-2111,b5d7a19bd0bae10917a8e294960fdacf224d64fe,"We validated our model by replicating gender and ethnic stereotypes produced in <span style=""background: yellow; display: inline-block"">(Garg et al., 2018)</span> by training multiple word embedding models and applied it to a novel corpus of talk radio data to analyze how perceptions of refugees as ""outsiders"" vary by geography and over time.",Uses,llm,"Word embeddings trained on large-scale historical corpora can illuminate human biases and stereotypes that perpetuate social inequalities. These embeddings are often trained in separate vector space models defined according to different attributes of interest. In this paper, we introduce a single, unified dynamic embedding model that learns attribute-specific word embeddings and apply it to a novel dataset—talk radio shows from around the US—to analyze perceptions about refugees. We validate our model on a benchmark dataset and apply it to two corpora of talk radio shows averaging 117 million words produced over one month across 83 stations and 64 cities. Our findings suggest that dynamic word embeddings are capable of identifying nuanced differences in public discourse about contentious topics, suggesting their usefulness as a tool for better understanding how the public perceives and engages with different issues across time, geography, and other dimensions.","Significance Word embeddings are a popular machine-learning method that represents each English word by a vector, such that the geometry between these vectors captures semantic relations between the corresponding words. We demonstrate that word embeddings can be used as a powerful tool to quantify historical trends and social change. As specific applications, we develop metrics based on word embeddings to characterize how gender stereotypes and attitudes toward ethnic minorities in the United States evolved during the 20th and 21st centuries starting from 1910. Our framework opens up a fruitful intersection between machine learning and quantitative social science. Word embeddings are a powerful machine-learning framework that represents each English word by a vector. The geometric relationship between these vectors captures meaningful semantic relationships between the corresponding words. In this paper, we develop a framework to demonstrate how the temporal dynamics of the embedding helps to quantify changes in stereotypes and attitudes toward women and ethnic minorities in the 20th and 21st centuries in the United States. We integrate word embeddings trained on 100 y of text data with the US Census to show that changes in the embedding track closely with demographic and occupation shifts over time. The embedding captures societal shifts—e.g., the women’s movement in the 1960s and Asian immigration into the United States—and also illuminates how specific adjectives and occupations became more closely associated with certain populations over time. Our framework for temporal analysis of word embedding opens up a fruitful intersection between machine learning and quantitative social science.",train,1,"The research problem is analyzing perceptions about refugees, specifically by investigating how public discourse about this topic varies across time, geography, and other dimensions. The researchers aim to overcome the limitations of existing word embedding models, which are often trained in separate vector spaces for different attributes, by developing a unified dynamic embedding model. The motivation stems from the realization that word embeddings trained on large-scale historical corpora can reflect and perpetuate social biases and stereotypes. The researchers aim to develop a tool that can help identify and understand these biases in public discourse, particularly in relation to contentious topics like refugee perceptions, and contribute to a better understanding of how the public engages with different issues across diverse contexts.",train,"dynamic word embeddings, large-scale historical corpora, talk radio shows, unified dynamic embedding model, vector space models"
280,4989,10376,ABC_e7c947a02bb0e81d6b6b4b9da74024_16,ACL:N19-1061,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"We refer to the word embeddings of the previous works as HARD-DEBIASED<span style=""background: yellow; display: inline-block""> (Bolukbasi et al., 2016b)</span> and GN-GLOVE (gender-neutral GloVe) counterparts in a predefined set.",Similar,llm,"Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in word embeddings, which is pervasive and problematic. Existing debiasing methods, despite showing initial success, are argued to only superficially address the issue, hiding the bias rather than removing it. The research is motivated by the concern that existing bias removal techniques are insufficient and may lead to a false sense of gender neutrality in word embeddings. The authors aim to highlight the limitations of these methods and encourage the development of more robust solutions.",train,"bias removal techniques, debiased embeddings, debiasing methods, gender-neutral modeling, NLP, text corpora, word embedding models, word embeddings"
281,5130,10658,ABC_e29c7551ea78cb425054963489e1b9_37,ACL:N19-1285,aa65d6a2e6eedd80b12b43b5210e27c6532d69e6,"<span style=""background: yellow; display: inline-block"">Salesky et al. (2018)</span> introduced a set of fluent references 1 for Fisher Spanish-English, enabling a new task: end-to-end training and evaluation against fluent references.",Background,llm,"Spoken language translation applications for speech suffer due to conversational speech phenomena, particularly the presence of disfluencies. With the rise of end-to-end speech translation models, processing steps such as disfluency removal that were previously an intermediate step between speech recognition and machine translation need to be incorporated into model architectures. We use a sequence-to-sequence model to translate from noisy, disfluent speech to fluent text with disfluencies removed using the recently collected ‘copy-edited’ references for the Fisher Spanish-English dataset. We are able to directly generate fluent translations and introduce considerations about how to evaluate success on this task. This work provides a baseline for a new task, implicitly removing disfluencies in end-to-end translation of conversational speech.","When translating from speech, special consideration for conversational speech phenomena such as disfluencies is necessary. Most machine translation training data consists of well-formed written texts, causing issues when translating spontaneous speech. Previous work has introduced an intermediate step between speech recognition (ASR) and machine translation (MT) to remove disfluencies, making the data better-matched to typical translation text and significantly improving performance. However, with the rise of end-to-end speech translation systems, this intermediate step must be incorporated into the sequence-to-sequence architecture. Further, though translated speech datasets exist, they are typically news or rehearsed speech without many disfluencies (e.g. TED), or the disfluencies are translated into the references (e.g. Fisher). To generate clean translations from disfluent speech, cleaned references are necessary for evaluation. We introduce a corpus of cleaned target data for the Fisher Spanish-English dataset for this task. We compare how different architectures handle disfluencies and provide a baseline for removing disfluencies in end-to-end translation.",train,0,"The primary research problem is the negative impact of disfluencies on the accuracy of spoken language translation applications. Specifically, the study addresses the challenge of incorporating disfluency removal into end-to-end speech translation models to improve translation quality. The research is motivated by the increasing popularity of end-to-end speech translation models and the need to address the issue of disfluencies in conversational speech. The study aims to establish a baseline for a new task focusing on implicit disfluency removal in end-to-end speech translation.",train,"end-to-end speech translation models, end-to-end translation of conversational speech, Fisher Spanish-English dataset, fluent text, fluent translations, machine translation, model architectures, noisy , disfluent speech, sequence-to-sequence model, speech, speech recognition, Spoken language translation applications"
282,5136,10665,ABC_78a7ca27c5ca032116db12205af939_38,ARXIV:1906.05190,b1b2402dcd85b81381fde40d0b971b510471ef23,"They introduce a novel Text-Image Embedding network (TieNet), which integrates self-attention LSTM using tex- tual report data and visual attention CNN using image data. TieNet is capable of extracting an informative embedding to represent the paired medical image and report, which significantly improves the disease classification performance compared to <span style=""background: yellow; display: inline-block"">[16]</span> .",Background,llm,"Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.","Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reportscan be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).",train,0,"The research problem lies in the difficulty of automating medical image interpretation, specifically in the area of generating reports from X-ray images. This is due to the lack of high-quality annotated image-report pairs and the absence of generative models specifically designed to extract and leverage localized semantic features, particularly those related to abnormalities. The motivation behind this research stems from the desire to reduce the workload of radiologists by automating the tedious tasks of inspecting and interpreting medical images. The ultimate goal is to improve efficiency and accuracy in generating reports from X-ray images.",train,"annotated image-report pairs, attentive LSTM model, automatic medical image interpretation system, CIDEr, classifying and localizing common thoracic diseases, computer-aided medical image visual perception, disease classification, image, imaging, localization, natural image captioning, report generation, tailor-made generative models, Vispi, Vispi, X-ray dataset"
283,5137,10666,ABC_78a7ca27c5ca032116db12205af939_38,ARXIV:1906.05190,634b8216dcd15c19680d3790df117beb456e5daa,"Similar to <span style=""background: yellow; display: inline-block"">[16]</span> , we apply a thresholding based bounding box (B-Box) generation method. The B-Box bounds pixels whose heatmap intensity is above 90% of the maximum intensity. The resulting region of interest is then cropped for next level modeling. Fig. 2b illustrates the process of report generation. If there is no active thoracic disease found in an X-ray, a report will be directly generated by an attentive LSTM based on the original X-ray as shown in the green dashed box. Otherwise (as shown in the red dashed box), the cropped subimage with localized disease from the classification module (Fig. 2a) is used to generate description of abnormalities whereas the original X-ray is used to generate description of normalities in the report.",Uses,llm,"Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research problem lies in the difficulty of automating medical image interpretation, specifically in the area of generating reports from X-ray images. This is due to the lack of high-quality annotated image-report pairs and the absence of generative models specifically designed to extract and leverage localized semantic features, particularly those related to abnormalities. The motivation behind this research stems from the desire to reduce the workload of radiologists by automating the tedious tasks of inspecting and interpreting medical images. The ultimate goal is to improve efficiency and accuracy in generating reports from X-ray images.",train,"annotated image-report pairs, attentive LSTM model, automatic medical image interpretation system, CIDEr, classifying and localizing common thoracic diseases, computer-aided medical image visual perception, disease classification, image, imaging, localization, natural image captioning, report generation, tailor-made generative models, Vispi, Vispi, X-ray dataset"
284,5138,10667,ABC_78a7ca27c5ca032116db12205af939_38,ARXIV:1906.05190,b1b2402dcd85b81381fde40d0b971b510471ef23,"They introduce a novel Text-Image Embedding network (TieNet), which integrates self-attention LSTM using tex- tual report data and visual attention CNN using image data. TieNet is capable of extracting an informative embedding to represent the paired medical image and report, which significantly improves the disease classification performance compared to <span style=""background: yellow; display: inline-block"">[16]</span> .",Motivation,llm,"Medical imaging contains the essential information for rendering diagnostic and treatment decisions. Inspecting (visual perception) and interpreting image to generate a report are tedious clinical routines for a radiologist where automation is expected to greatly reduce the workload. Despite rapid development of natural image captioning, computer-aided medical image visual perception and interpretation remain a challenging task, largely due to the lack of high-quality annotated image-report pairs and tailor-made generative models for sufficient extraction and exploitation of localized semantic features, particularly those associated with abnormalities. To tackle these challenges, we present Vispi, an automatic medical image interpretation system, which first annotates an image via classifying and localizing common thoracic diseases with visual support and then followed by report generation from an attentive LSTM model. Analyzing an open IU X-ray dataset, we demonstrate a superior performance of Vispi in disease classification, localization and report generation using automatic performance evaluation metrics ROUGE and CIDEr.","Chest X-rays are one of the most common radiological examinations in daily clinical routines. Reporting thorax diseases using chest X-rays is often an entry-level task for radiologist trainees. Yet, reading a chest X-ray image remains a challenging job for learning-oriented machine intelligence, due to (1) shortage of large-scale machine-learnable medical image datasets, and (2) lack of techniques that can mimic the high-level reasoning of human radiologists that requires years of knowledge accumulation and professional training. In this paper, we show the clinical free-text radiological reportscan be utilized as a priori knowledge for tackling these two key problems. We propose a novel Text-Image Embedding network (TieNet) for extracting the distinctive image and text representations. Multi-level attention models are integrated into an end-to-end trainable CNN-RNN architecture for highlighting the meaningful text words and image regions. We first apply TieNet to classify the chest X-rays by using both image features and text embeddings extracted from associated reports. The proposed auto-annotation framework achieves high accuracy (over 0.9 on average in AUCs) in assigning disease labels for our hand-label evaluation dataset. Furthermore, we transform the TieNet into a chest X-ray reporting system. It simulates the reporting process and can output disease classification and a preliminary report together. The classification results are significantly improved (6% increase on average in AUCs) compared to the state-of-the-art baseline on an unseen and hand-labeled dataset (OpenI).",train,0,"The research problem lies in the difficulty of automating medical image interpretation, specifically in the area of generating reports from X-ray images. This is due to the lack of high-quality annotated image-report pairs and the absence of generative models specifically designed to extract and leverage localized semantic features, particularly those related to abnormalities. The motivation behind this research stems from the desire to reduce the workload of radiologists by automating the tedious tasks of inspecting and interpreting medical images. The ultimate goal is to improve efficiency and accuracy in generating reports from X-ray images.",train,"annotated image-report pairs, attentive LSTM model, automatic medical image interpretation system, CIDEr, classifying and localizing common thoracic diseases, computer-aided medical image visual perception, disease classification, image, imaging, localization, natural image captioning, report generation, tailor-made generative models, Vispi, Vispi, X-ray dataset"
285,5268,10932,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,f264e8b33c0d49a692a6ce2c4bcb28588aeb7d97,"Their proposed method achieved perplexity competitive with the basic LSTM language model<span style=""background: yellow; display: inline-block""> (Zaremba, Sutskever, and Vinyals 2014)</span> even though its parameter size is small.",Background,llm,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
286,5269,10933,ABC_6b1432f4aac35e6acd8ca8770fe484_2,ARXIV:1906.05506,12e9d005c77f76e344361f79c4b008034ae547eb,"Previous studies demonstrated that additive composition, which computes the (weighted) sum of embeddings, is a suitable method for embedding construction<span style=""background: yellow; display: inline-block""> Wieting et al. 2016</span> the number of character n-grams extracted from the word, and let S be the matrix whose i-th column corresponds to s i , that is, S = [s 1 , ..., s I ].",Background,llm,"This paper proposes a novel Recurrent Neural Network (RNN) language model that takes advantage of character information. We focus on character n-grams based on research in the field of word embedding construction (Wieting et al. 2016). Our proposed method constructs word embeddings from character ngram embeddings and combines them with ordinary word embeddings. We demonstrate that the proposed method achieves the best perplexities on the language modeling datasets: Penn Treebank, WikiText-2, and WikiText-103. Moreover, we conduct experiments on application tasks: machine translation and headline generation. The experimental results indicate that our proposed method also positively affects these tasks","We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks.",train,0,"The research problem is to improve language modeling by utilizing character information. The motivation is to leverage character information to enhance language modeling, drawing inspiration from existing research on word embedding construction.",train,"headline generation, language modeling datasets, machine translation, Penn Treebank, Recurrent Neural Network ( RNN ) language model, WikiText-103, WikiText-2, word embedding construction"
287,5271,10941,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"However,<span style=""background: yellow; display: inline-block""> Finegan-Dollak et al. (2018)</span> demonstrated that both the approaches lack the ability to generate SQL of unseen templates.",Background,llm,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
288,5272,10942,ABC_c4cc8d4013b0259eb626d06750e4ab_2,ARXIV:1905.11499,2f541b24f69798e255e04229e8dc78f4d1873fd7,"Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However,<span style=""background: yellow; display: inline-block""> Finegan-Dollak et al. (2018)</span> demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example.",Motivation,llm,"Most deep learning approaches for text-to-SQL generation are limited to the WikiSQL dataset, which only supports very simple queries. Recently, template-based and sequence-to-sequence approaches were proposed to support complex queries, which contain join queries, nested queries, and other types. However, Finegan-Dollak et al. (2018) demonstrated that both the approaches lack the ability to generate SQL of unseen templates. In this paper, we propose a template-based one-shot learning model for the text-to-SQL generation so that the model can generate SQL of an untrained template based on a single example. First, we classify the SQL template using the Matching Network that is augmented by our novel architecture Candidate Search Network. Then, we fill the variable slots in the predicted template using the Pointer Network. We show that our model outperforms state-of-the-art approaches for various text-to-SQL datasets in two aspects: 1) the SQL generation accuracy for the trained templates, and 2) the adaptability to the unseen SQL templates based on a single example without any additional training.","To be informative, an evaluation must measure how well systems generalize to realistic unseen data. We identify limitations of and propose improvements to current evaluations of text-to-SQL systems. First, we compare human-generated and automatically generated questions, characterizing properties of queries necessary for real-world applications. To facilitate evaluation on multiple datasets, we release standardized and improved versions of seven existing datasets and one new text-to-SQL dataset. Second, we show that the current division of data into training and test sets measures robustness to variations in the way questions are asked, but only partially tests how well systems generalize to new queries; therefore, we propose a complementary dataset split for evaluation of future work. Finally, we demonstrate how the common practice of anonymizing variables during evaluation removes an important challenge of the task. Our observations highlight key difficulties, and our methodology enables effective measurement of future development.",train,0,"The research problem is the limitation of current deep learning approaches for text-to-SQL generation, particularly their inability to handle complex queries and generate SQL for unseen templates. The research is motivated by the need to improve the performance of text-to-SQL generation, specifically addressing the shortcomings of existing approaches. The goal is to develop a model that can accurately generate SQL for both trained and unseen templates, enabling adaptability and enhancing the overall efficiency of text-to-SQL generation.",train,"deep learning approaches, Matching Network, Pointer Network, template-based and sequence-to-sequence approaches, template-based one-shot learning model, text-to-SQL datasets, text-to-SQL generation, text-to-SQL generation, WikiSQL dataset"
289,5273,10947,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"On the ""add jump"" test set <span style=""background: yellow; display: inline-block"">[15]</span> , standard seq2seq modeling completely fails to generalize compositionally, reaching an average performance of only 0.03% correct (SD = 0.02).",Background,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
290,5274,10948,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"A greedy decoder is used since it is effective on SCAN's deterministic outputs <span style=""background: yellow; display: inline-block"">[15]</span> .",Uses,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,1,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
291,5275,10949,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"Hybrid models could also address the challenge of generalizing to longer output sequences, a problem that continues to vex neural networks<span style=""background: yellow; display: inline-block""> [15,</span> 3, 28] including meta seq2seq learning.",Background,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
292,5276,10950,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"The meta seq2seq architecture builds upon the seq2seq architecture from <span style=""background: yellow; display: inline-block"">[15]</span> that performed best across a range of SCAN evaluations.",Extention,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,1,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
293,5277,10951,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"This experiment applies meta seq2seq learning to the SCAN task of adding a new primitive <span style=""background: yellow; display: inline-block"">[15]</span> .",Uses,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,1,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
294,5278,10952,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"This experiment uses the SCAN ""length"" split <span style=""background: yellow; display: inline-block"">[15]</span> .",Uses,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,1,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
295,5279,10953,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"It fails even while achieving near perfect performance on the training set (>99% on average). This replicates the results from <span style=""background: yellow; display: inline-block"">[15]</span> which trained many seq2seq models, finding the best network performed at only 1.2% accuracy.",Similar,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
296,5280,10954,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"Following <span style=""background: yellow; display: inline-block"">[15]</span> , the critical ""jump"" demonstration is overrepresented in training to ensure it is learned.",Uses,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,1,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
297,5281,10955,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"The standard seq2seq learner takes advantage of the augmented training to generalize better than in standard SCAN training (Experiment 4.3 and <span style=""background: yellow; display: inline-block"">[15]</span> ), achieving 12.26% accuracy (SD = 8.33) on the test instructions (with >99% accuracy during training).",Difference,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
298,5282,10956,ABC_4a28a289ffc730fea4114f6c71bd06_3,ARXIV:1906.05381,a2f09447e8f1ceda391101e5ae7f863a8f2b2836,"This approach is able to solve SCAN tasks for compositional learning that have eluded standard NLP approaches, with the exception of generalizing to longer sequences <span style=""background: yellow; display: inline-block"">[15]</span> .",Unsure,llm,"People can learn a new concept and use it compositionally, understanding how to ""blicket twice"" after learning how to ""blicket."" In contrast, powerful sequence-to-sequence (seq2seq) neural networks fail such tests of compositionality, especially when composing new concepts together with existing concepts. In this paper, I show how memory-augmented neural networks can be trained to generalize compositionally through meta seq2seq learning. In this approach, models train on a series of seq2seq problems to acquire the compositional skills needed to solve new seq2seq problems. Meta se2seq learning solves several of the SCAN tests for compositional learning and can learn to apply implicit rules to variables.","Hierarchical Bayesian networks and neural networks with stochastic hidden units are commonly perceived as two separate types of models. We show that either of these types of models can often be transformed into an instance of the other, by switching between centered and differentiable non-centered parameterizations of the latent variables. The choice of parameterization greatly influences the efficiency of gradient-based posterior inference; we show that they are often complementary to eachother, we clarify when each parameterization is preferred and show how inference can be made robust. In the noncentered form, a simple Monte Carlo estimator of the marginal likelihood can be used for learning the parameters. Theoretical results are supported by experiments.",train,0,"The research problem is the inability of powerful sequence-to-sequence (seq2seq) neural networks to exhibit compositional learning, specifically in the context of combining new and existing concepts, unlike humans who can learn and apply concepts compositionally. The research is motivated by the desire to develop neural networks that can generalize compositionally, similar to humans. The proposed meta seq2seq learning approach aims to address this challenge by enabling models to learn and apply implicit rules, demonstrating success in solving compositional learning tests.",train,"compositional learning, memory-augmented neural networks, meta seq2seq learning, se2seq learning, seq2seq problems, seq2seq problems, sequence-to-sequence ( seq2seq ) neural networks"
299,94,626,ABC_55160a7ab2df9a86e677bcc72d9842_4,ACL:D19-1545,d11777ca327c6d91de34d1d2ac50316b905578b2,"Neural encoder-decoder models have proved effective in mapping NL to logical forms (Dong and Lapata, 2016) and also for directly producing general purpose programs (Iyer et al., 2017 (Iyer et al., , 2018 .",Background,s2,"Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5\times larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.","We study the problem of generating source code in a strongly typed, Java-like programming language, given a label (for example a set of API calls or types) carrying a small amount of information about the code that is desired. The generated programs are expected to respect a ""realistic"" relationship between programs and labels, as exemplified by a corpus of labeled programs available during training. 
Two challenges in such conditional program generation are that the generated programs must satisfy a rich set of syntactic and semantic constraints, and that source code contains many low-level features that impede learning. We address these problems by training a neural generator not on code but on program sketches, or models of program syntax that abstract out names and operations that do not generalize across programs. During generation, we infer a posterior distribution over sketches, then concretize samples from this distribution into type-safe programs using combinatorial techniques. We implement our ideas in a system for generating API-heavy Java code, and show that it can often predict the entire body of a method given just a few API calls or data types that appear in the method.",dev,0,"The current limitation of semantic parsers in mapping natural language instructions to source code lies in their node-by-node approach to building the code syntax tree. This approach is inefficient and fails to capture the high-level coding patterns (idioms) that programmers typically utilize. The research aims to bridge the gap between the current approach of semantic parsers and the way programmers actually write code. By integrating idioms into the parsing process, the researchers seek to improve the efficiency, accuracy, and scalability of semantic parsing. This is further motivated by the need for more accurate and efficient semantic parsing, especially when training data is limited.",train,"context-dependent semantic parsing task, decoding, idiom-based decoding, iterative method, large source code corpora, Programmers, semantic parsers, semantic parsers, semantic parsing"
300,96,628,ABC_55160a7ab2df9a86e677bcc72d9842_4,ACL:D19-1545,558ac446dc26bee9789d660a251b75728cb6eeb2,We use a BPE vocabulary of 10K tokens for embedding matrix B and get the best validation set results using the original hyperparameters used by Iyer et al. (2018) .,Uses,s2,"Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5\times larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.","Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations.",dev,1,"The current limitation of semantic parsers in mapping natural language instructions to source code lies in their node-by-node approach to building the code syntax tree. This approach is inefficient and fails to capture the high-level coding patterns (idioms) that programmers typically utilize. The research aims to bridge the gap between the current approach of semantic parsers and the way programmers actually write code. By integrating idioms into the parsing process, the researchers seek to improve the efficiency, accuracy, and scalability of semantic parsing. This is further motivated by the need for more accurate and efficient semantic parsing, especially when training data is limited.",train,"context-dependent semantic parsing task, decoding, idiom-based decoding, iterative method, large source code corpora, Programmers, semantic parsers, semantic parsers, semantic parsing"
301,97,629,ABC_55160a7ab2df9a86e677bcc72d9842_4,ACL:D19-1545,d7da009f457917aa381619facfa5ffae9329a6e9,"We report exact match accuracy, corpus level BLEU score (which serves as a measure of partial credit) (Papineni et al., 2002) , and training time for all these configurations. Iyer et al. (2018) .",Similar,s2,"Programmers typically organize executable source code using high-level coding patterns or idiomatic structures such as nested loops, exception handlers and recursive blocks, rather than as individual code tokens. In contrast, state of the art (SOTA) semantic parsers still map natural language instructions to source code by building the code syntax tree one node at a time. In this paper, we introduce an iterative method to extract code idioms from large source code corpora by repeatedly collapsing most-frequent depth-2 subtrees of their syntax trees, and train semantic parsers to apply these idioms during decoding. Applying idiom-based decoding on a recent context-dependent semantic parsing task improves the SOTA by 2.2% BLEU score while reducing training time by more than 50%. This improved speed enables us to scale up the model by training on an extended training set that is 5\times larger, to further move up the SOTA by an additional 2.3% BLEU and 0.9% exact match. Finally, idioms also significantly improve accuracy of semantic parsing to SQL on the ATIS-SQL dataset, when training data is limited.","Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.",dev,0,"The current limitation of semantic parsers in mapping natural language instructions to source code lies in their node-by-node approach to building the code syntax tree. This approach is inefficient and fails to capture the high-level coding patterns (idioms) that programmers typically utilize. The research aims to bridge the gap between the current approach of semantic parsers and the way programmers actually write code. By integrating idioms into the parsing process, the researchers seek to improve the efficiency, accuracy, and scalability of semantic parsing. This is further motivated by the need for more accurate and efficient semantic parsing, especially when training data is limited.",train,"context-dependent semantic parsing task, decoding, idiom-based decoding, iterative method, large source code corpora, Programmers, semantic parsers, semantic parsers, semantic parsing"
302,438,2128,ABC_c705c0533600b9b93d2c89bcbc292b_12,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b; Ma et al., 2019) .",Difference,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",dev,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
303,439,2129,ABC_c705c0533600b9b93d2c89bcbc292b_12,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"Recent state-of-the-art models (Wang et al., 2018; Fried et al., 2018b; Ma et al., 2019) have demonstrated large gains in accuracy on the VLN task.",Background,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",dev,0,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
304,440,2130,ABC_c705c0533600b9b93d2c89bcbc292b_12,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"In this paper, we find that agents without any visual input can achieve competitive performance, matching or even outperforming their vision-based counterparts under two state-of-theart model models (Fried et al., 2018b; Ma et al., 2019) .",Uses,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",dev,1,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
305,441,2131,ABC_c705c0533600b9b93d2c89bcbc292b_12,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"In this paper, we show that the same trends hold for two recent state-of-the-art architectures (Ma et al., 2019; Fried et al., 2018b) for the VLN task; we also analyze to what extent object-based representations and mixture-ofexperts methods can address these issues.",Uses,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",dev,1,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
306,442,2132,ABC_c705c0533600b9b93d2c89bcbc292b_12,ACL:P19-1655,29e13746fa5aed13e51558a521a39aaeaa99c1b1,"The Speaker-Follower (SF) model (Fried et al., 2018b ) and the Self-Monitoring (SM) model (Ma et al., 2019) which we analyze both use sequenceto-sequence model (Cho et al., 2014) with attention (Bahdanau et al., 2015) as their base instruction-following agent.",Uses,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at this https URL .",dev,1,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
307,443,2133,ABC_c705c0533600b9b93d2c89bcbc292b_12,ACL:P19-1655,29e13746fa5aed13e51558a521a39aaeaa99c1b1,"In this work, we analyze two recent VLN models, which typify the visual grounding approaches of VLN work: the panoramic ""follower"" model from the Speaker-Follower (SF) system of Fried et al. (2018b) and the Self-Monitoring (SM) model of Ma et al. (2019) .",Uses,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","The Vision-and-Language Navigation (VLN) task entails an agent following navigational instruction in photo-realistic unknown environments. This challenging task demands that the agent be aware of which instruction was completed, which instruction is needed next, which way to go, and its navigation progress towards the goal. In this paper, we introduce a self-monitoring agent with two complementary components: (1) visual-textual co-grounding module to locate the instruction completed in the past, the instruction required for the next action, and the next moving direction from surrounding images and (2) progress monitor to ensure the grounded instruction correctly reflects the navigation progress. We test our self-monitoring agent on a standard benchmark and analyze our proposed approach through a series of ablation studies that elucidate the contributions of the primary components. Using our proposed method, we set the new state of the art by a significant margin (8% absolute increase in success rate on the unseen test set). Code is available at this https URL .",dev,1,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
308,444,2135,ABC_c705c0533600b9b93d2c89bcbc292b_12,ACL:P19-1655,893186c6bc08a17cb3f9f94fa3f14e9ad20b0525,"We then use the same visual attention mechanism as in Fried et al. (2018b) and Ma et al. (2019) to obtain an attended object representation x obj,att over these {x obj,j } vectors.",Uses,s2,"Vision-and-Language Navigation (VLN) requires grounding instructions, such as “turn right and stop at the door”, to routes in a visual environment. The actual grounding can connect language to the environment through multiple modalities, e.g. “stop at the door” might ground into visual objects, while “turn right” might rely only on the geometric structure of a route. We investigate where the natural language empirically grounds under two recent state-of-the-art VLN models. Surprisingly, we discover that visual features may actually hurt these models: models which only use route structure, ablating visual features, outperform their visual counterparts in unseen new environments on the benchmark Room-to-Room dataset. To better use all the available modalities, we propose to decompose the grounding procedure into a set of expert models with access to different modalities (including object detections) and ensemble them at prediction time, improving the performance of state-of-the-art models on the VLN task.","Navigation guided by natural language instructions presents a challenging reasoning problem for instruction followers. Natural language instructions typically identify only a few high-level decisions and landmarks rather than complete low-level motor behaviors; much of the missing information must be inferred based on perceptual context. In machine learning settings, this is doubly challenging: it is difficult to collect enough annotated data to enable learning of this reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models. Here we describe an approach to vision-and-language navigation that addresses both these issues with an embedded speaker model. We use this speaker model to (1) synthesize new instructions for data augmentation and to (2) implement pragmatic reasoning, which evaluates how well candidate action sequences explain an instruction. Both steps are supported by a panoramic action space that reflects the granularity of human-generated instructions. Experiments show that all three components of this approach---speaker-driven data augmentation, pragmatic reasoning and panoramic action space---dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing approach on a standard benchmark.",dev,1,"The research problem is to improve the grounding of natural language instructions in visual environments for Vision-and-Language Navigation (VLN), specifically addressing the challenge of visual features potentially hindering performance in state-of-the-art models. The motivation for this research stems from the surprising finding that visual features may negatively impact the performance of state-of-the-art VLN models. The goal is to leverage all available modalities effectively to improve performance in unseen environments.",train,"expert models, grounding instructions, Room-to-Room dataset, Vision-and-Language Navigation ( VLN ), VLN models"
309,579,2757,ABC_da8f30113f1126a78cefed06a15076_15,ACL:N19-2016,7a941148d8c5865749801b2f9f67f9ad1fba1d25,"A number of approaches for question answering have been proposed recently that use reinforcement learning to reason over a knowledge graph (Das et al., 2018; Lin et al., 2018; Chen et al., 2018; Zhang et al., 2018) .",Background,s2,"In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications. We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings. More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.","Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",dev,0,"The research focuses on addressing the challenges of applying reinforcement learning agents to question-answering over knowledge graphs in real-world settings. Specifically, the problem lies in the inadequacy of existing performance metrics, which fail to accurately evaluate systems in situations where no answer is available, leading to agents with poor confidence modeling. The research is motivated by the need for more robust and realistic evaluation of question-answering agents using reinforcement learning in real-world scenarios, where the absence of answers is a common occurrence. The goal is to develop agents that can model confidence and provide accurate answers when possible, while also knowing when to refrain from answering.",train,"question-answering agents, question-answering over knowledge graphs, real-world applications, reinforcement learning agents, reinforcement learning algorithm, supervised learning strategy"
310,580,2758,ABC_da8f30113f1126a78cefed06a15076_15,ACL:N19-2016,7a941148d8c5865749801b2f9f67f9ad1fba1d25,"The closest works to ours are the works by Lin et al. (2018) , Zhang et al. (2018) and Das et al. (2018) , which consider the question answering task in a reinforcement learning setting in which the agent always chooses to answer.",Similar,s2,"In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications. We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings. More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.","Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",dev,0,"The research focuses on addressing the challenges of applying reinforcement learning agents to question-answering over knowledge graphs in real-world settings. Specifically, the problem lies in the inadequacy of existing performance metrics, which fail to accurately evaluate systems in situations where no answer is available, leading to agents with poor confidence modeling. The research is motivated by the need for more robust and realistic evaluation of question-answering agents using reinforcement learning in real-world scenarios, where the absence of answers is a common occurrence. The goal is to develop agents that can model confidence and provide accurate answers when possible, while also knowing when to refrain from answering.",train,"question-answering agents, question-answering over knowledge graphs, real-world applications, reinforcement learning agents, reinforcement learning algorithm, supervised learning strategy"
311,581,2760,ABC_da8f30113f1126a78cefed06a15076_15,ACL:N19-2016,7a941148d8c5865749801b2f9f67f9ad1fba1d25,We base our work on the recent reinforcement learning approaches introduced in Das et al. (2018) and Lin et al. (2018) .,Uses,s2,"In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications. We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings. More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.","Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",dev,1,"The research focuses on addressing the challenges of applying reinforcement learning agents to question-answering over knowledge graphs in real-world settings. Specifically, the problem lies in the inadequacy of existing performance metrics, which fail to accurately evaluate systems in situations where no answer is available, leading to agents with poor confidence modeling. The research is motivated by the need for more robust and realistic evaluation of question-answering agents using reinforcement learning in real-world scenarios, where the absence of answers is a common occurrence. The goal is to develop agents that can model confidence and provide accurate answers when possible, while also knowing when to refrain from answering.",train,"question-answering agents, question-answering over knowledge graphs, real-world applications, reinforcement learning agents, reinforcement learning algorithm, supervised learning strategy"
312,582,2761,ABC_da8f30113f1126a78cefed06a15076_15,ACL:N19-2016,7a941148d8c5865749801b2f9f67f9ad1fba1d25,"Unlike Das et al. (2018) , we also train entity embeddings after initializing them with random values. This resulted in the final QA Score of 47.58%, around 8% higher than standard RL and 12% higher than Das et al. (2018) . The final QA Score also increased from 28.72% to 39.55%, and also significantly improved over Das et al. (2018) and Lin et al. (2018) .",Difference,s2,"In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications. We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings. More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.","Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",dev,0,"The research focuses on addressing the challenges of applying reinforcement learning agents to question-answering over knowledge graphs in real-world settings. Specifically, the problem lies in the inadequacy of existing performance metrics, which fail to accurately evaluate systems in situations where no answer is available, leading to agents with poor confidence modeling. The research is motivated by the need for more robust and realistic evaluation of question-answering agents using reinforcement learning in real-world scenarios, where the absence of answers is a common occurrence. The goal is to develop agents that can model confidence and provide accurate answers when possible, while also knowing when to refrain from answering.",train,"question-answering agents, question-answering over knowledge graphs, real-world applications, reinforcement learning agents, reinforcement learning algorithm, supervised learning strategy"
313,583,2763,ABC_da8f30113f1126a78cefed06a15076_15,ACL:N19-2016,7a941148d8c5865749801b2f9f67f9ad1fba1d25,"Unlike Das et al. (2018) , we also train entity embeddings after initializing them with random values. This resulted in the final QA Score of 47.58%, around 8% higher than standard RL and 12% higher than Das et al. (2018) . The final QA Score also increased from 28.72% to 39.55%, and also significantly improved over Das et al. (2018) and Lin et al. (2018) .",Extention,s2,"In this paper, we investigate the challenges of using reinforcement learning agents for question-answering over knowledge graphs for real-world applications. We examine the performance metrics used by state-of-the-art systems and determine that they are inadequate for such settings. More specifically, they do not evaluate the systems correctly for situations when there is no answer available and thus agents optimized for these metrics are poor at modeling confidence. We introduce a simple new performance metric for evaluating question-answering agents that is more representative of practical usage conditions, and optimize for this metric by extending the binary reward structure used in prior work to a ternary reward structure which also rewards an agent for not answering a question rather than giving an incorrect answer. We show that this can drastically improve the precision of answered questions while only not answering a limited number of previously correctly answered questions. Employing a supervised learning strategy using depth-first-search paths to bootstrap the reinforcement learning algorithm further improves performance.","Multi-hop reasoning is an effective approach for query answering (QA) over incomplete knowledge graphs (KGs). The problem can be formulated in a reinforcement learning (RL) setup, where a policy-based agent sequentially extends its inference path until it reaches a target. However, in an incomplete KG environment, the agent receives low-quality rewards corrupted by false negatives in the training data, which harms generalization at test time. Furthermore, since no golden action sequence is used for training, the agent can be misled by spurious search trajectories that incidentally lead to the correct answer. We propose two modeling advances to address both issues: (1) we reduce the impact of false negative supervision by adopting a pretrained one-hop embedding model to estimate the reward of unobserved facts; (2) we counter the sensitivity to spurious paths of on-policy RL by forcing the agent to explore a diverse set of paths using randomly generated edge masks. Our approach significantly improves over existing path-based KGQA models on several benchmark datasets and is comparable or better than embedding-based models.",dev,1,"The research focuses on addressing the challenges of applying reinforcement learning agents to question-answering over knowledge graphs in real-world settings. Specifically, the problem lies in the inadequacy of existing performance metrics, which fail to accurately evaluate systems in situations where no answer is available, leading to agents with poor confidence modeling. The research is motivated by the need for more robust and realistic evaluation of question-answering agents using reinforcement learning in real-world scenarios, where the absence of answers is a common occurrence. The goal is to develop agents that can model confidence and provide accurate answers when possible, while also knowing when to refrain from answering.",train,"question-answering agents, question-answering over knowledge graphs, real-world applications, reinforcement learning agents, reinforcement learning algorithm, supervised learning strategy"
314,706,3315,ABC_43622e43d6ef5291b64320d2d68b95_18,ACL:N19-1407,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"In addition, the performance of SANs can be improved by multi-head attention (Vaswani et al., 2017) , which projects the input sequence into multiple subspaces and applies attention to the representation in each subspace.",Background,s2,"Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to enhance the capabilities of self-attention networks (SANs), specifically addressing the need to strengthen dependencies between neighboring elements and model the interaction between features extracted by multiple attention heads. The research is motivated by the growing interest in self-attention networks due to their computational efficiency and flexibility in modeling dependencies. The goal is to further improve SANs by incorporating convolutional mechanisms to address limitations in locality and feature interaction modeling.",train,"convolutional self-attention networks, machine translation, multi-head attention, SANs, Self-attention networks ( SANs ), Transformer baseline"
315,707,3316,ABC_43622e43d6ef5291b64320d2d68b95_18,ACL:N19-1407,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"where ATT(·) is an attention model (Bahdanau et al., 2015; Vaswani et al., 2017) that retrieves the keys K h with the query q h i .",Background,s2,"Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to enhance the capabilities of self-attention networks (SANs), specifically addressing the need to strengthen dependencies between neighboring elements and model the interaction between features extracted by multiple attention heads. The research is motivated by the growing interest in self-attention networks due to their computational efficiency and flexibility in modeling dependencies. The goal is to further improve SANs by incorporating convolutional mechanisms to address limitations in locality and feature interaction modeling.",train,"convolutional self-attention networks, machine translation, multi-head attention, SANs, Self-attention networks ( SANs ), Transformer baseline"
316,708,3317,ABC_43622e43d6ef5291b64320d2d68b95_18,ACL:N19-1407,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Multi-Head Attention Multi-head attention mechanism (Vaswani et al., 2017) employs different attention heads to capture distinct features (Raganato and Tiedemann, 2018) .",Background,s2,"Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to enhance the capabilities of self-attention networks (SANs), specifically addressing the need to strengthen dependencies between neighboring elements and model the interaction between features extracted by multiple attention heads. The research is motivated by the growing interest in self-attention networks due to their computational efficiency and flexibility in modeling dependencies. The goal is to further improve SANs by incorporating convolutional mechanisms to address limitations in locality and feature interaction modeling.",train,"convolutional self-attention networks, machine translation, multi-head attention, SANs, Self-attention networks ( SANs ), Transformer baseline"
317,709,3318,ABC_43622e43d6ef5291b64320d2d68b95_18,ACL:N19-1407,0ef460c47377c3b9482d8177cbcafad1730a91a5,"Self-attention networks (SANs) (Parikh et al., 2016; Lin et al., 2017) have shown promising empirical results in various natural language processing (NLP) tasks, such as machine translation (Vaswani et al., 2017) , natural language inference (Shen et al., 2018a) , and acoustic modeling (Sperber et al., 2018) .",Background,s2,"Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.","Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but does not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called ""bi-directional block self-attention network (Bi-BloSAN)"", for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN.",dev,0,"The research problem is to enhance the capabilities of self-attention networks (SANs), specifically addressing the need to strengthen dependencies between neighboring elements and model the interaction between features extracted by multiple attention heads. The research is motivated by the growing interest in self-attention networks due to their computational efficiency and flexibility in modeling dependencies. The goal is to further improve SANs by incorporating convolutional mechanisms to address limitations in locality and feature interaction modeling.",train,"convolutional self-attention networks, machine translation, multi-head attention, SANs, Self-attention networks ( SANs ), Transformer baseline"
318,710,3319,ABC_43622e43d6ef5291b64320d2d68b95_18,ACL:N19-1407,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"We conducted experiments with the Transformer model (Vaswani et al., 2017) on English⇒German (En⇒De), Chinese⇒English (Zh⇒En) and Japanese⇒English (Ja⇒En) translation tasks.",Uses,s2,"Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,1,"The research problem is to enhance the capabilities of self-attention networks (SANs), specifically addressing the need to strengthen dependencies between neighboring elements and model the interaction between features extracted by multiple attention heads. The research is motivated by the growing interest in self-attention networks due to their computational efficiency and flexibility in modeling dependencies. The goal is to further improve SANs by incorporating convolutional mechanisms to address limitations in locality and feature interaction modeling.",train,"convolutional self-attention networks, machine translation, multi-head attention, SANs, Self-attention networks ( SANs ), Transformer baseline"
319,711,3320,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,de17bdef43b2a7ac75447c494b9f791d951a6b27,"Several studies have been conducted in order to measure compositionality for compounds in different languages (von der Heide and Borgwaldt, 2009; Reddy et al., 2011; Schulte im Walde et al., 2016b) .",Background,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","A multiword is compositional if its meaning can be expressed in terms of the meaning of its constituents. In this paper, we collect and analyse the compositionality judgments for a range of compound nouns using Mechanical Turk. Unlike existing compositionality datasets, our dataset has judgments on the contribution of constituent words as well as judgments for the phrase as a whole. We use this dataset to study the relation between the judgments at constituent level to that for the whole phrase. We then evaluate two different types of distributional models for compositionality detection – constituent based models and composition function based models. Both the models show competitive performance though the composition function based models perform slightly better. In both types, additive models perform better than their multiplicative counterparts.",dev,0,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
320,712,3321,ABC_43622e43d6ef5291b64320d2d68b95_18,ACL:N19-1407,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Experimental results demonstrate that our approach consistently improves performance over the strong TRANSFORMER model (Vaswani et al., 2017) across language pairs.",Difference,s2,"Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to enhance the capabilities of self-attention networks (SANs), specifically addressing the need to strengthen dependencies between neighboring elements and model the interaction between features extracted by multiple attention heads. The research is motivated by the growing interest in self-attention networks due to their computational efficiency and flexibility in modeling dependencies. The goal is to further improve SANs by incorporating convolutional mechanisms to address limitations in locality and feature interaction modeling.",train,"convolutional self-attention networks, machine translation, multi-head attention, SANs, Self-attention networks ( SANs ), Transformer baseline"
321,713,3322,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,cdabfa4e7e6b30a5bf1451f882571a079e9c6a4b,"From a synchronic perspective, Reddy et al. (2011 ), Schulte im Walde et al. (2013 and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations.",Similar,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","This paper explores two hypotheses regarding vector space models that predict the compositionality of German noun-noun compounds: (1) Against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent the most salient part-of-speech. Our overall best result is state-of-the-art, reaching Spearman’s = 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus. (2) While there are no significant differences in predicting compound‐modifier vs. compound‐head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound.",dev,0,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
322,714,3323,ABC_43622e43d6ef5291b64320d2d68b95_18,ACL:N19-1407,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"About configurations of NMT models, we used the Base and Big settings same as Vaswani et al. (2017) , and all models were trained on 8 NVIDIA P40 GPUs with a batch of 4096 tokens.",Similar,s2,"Self-attention networks (SANs) have drawn increasing interest due to their high parallelization in computation and flexibility in modeling dependencies. SANs can be further enhanced with multi-head attention by allowing the model to attend to information from different representation subspaces. In this work, we propose novel convolutional self-attention networks, which offer SANs the abilities to 1) strengthen dependencies among neighboring elements, and 2) model the interaction between features extracted by multiple attention heads. Experimental results of machine translation on different language pairs and model settings show that our approach outperforms both the strong Transformer baseline and other existing models on enhancing the locality of SANs. Comparing with prior studies, the proposed model is parameter free in terms of introducing no more parameters.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to enhance the capabilities of self-attention networks (SANs), specifically addressing the need to strengthen dependencies between neighboring elements and model the interaction between features extracted by multiple attention heads. The research is motivated by the growing interest in self-attention networks due to their computational efficiency and flexibility in modeling dependencies. The goal is to further improve SANs by incorporating convolutional mechanisms to address limitations in locality and feature interaction modeling.",train,"convolutional self-attention networks, machine translation, multi-head attention, SANs, Self-attention networks ( SANs ), Transformer baseline"
323,715,3324,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,cdabfa4e7e6b30a5bf1451f882571a079e9c6a4b,"From a synchronic perspective, Reddy et al. (2011 ), Schulte im Walde et al. (2013 and Schulte im Walde et al. (2016a) are closest to our approach, since they predict the compositionality of compounds using vector space representations.",Background,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","This paper explores two hypotheses regarding vector space models that predict the compositionality of German noun-noun compounds: (1) Against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent the most salient part-of-speech. Our overall best result is state-of-the-art, reaching Spearman’s = 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus. (2) While there are no significant differences in predicting compound‐modifier vs. compound‐head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound.",dev,0,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
324,716,3325,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,cdabfa4e7e6b30a5bf1451f882571a079e9c6a4b,"Like Reddy et al. (2011) and Schulte im Walde et al. (2013), we opt for Spearman's ρ.",Similar,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","This paper explores two hypotheses regarding vector space models that predict the compositionality of German noun-noun compounds: (1) Against our intuition, we demonstrate that window-based rather than syntax-based distributional features perform better predictions, and that not adjectives or verbs but nouns represent the most salient part-of-speech. Our overall best result is state-of-the-art, reaching Spearman’s = 0.65 with a wordspace model of nominal features from a 20word window of a 1.5 billion word web corpus. (2) While there are no significant differences in predicting compound‐modifier vs. compound‐head ratings on compositionality, we show that the modifier (rather than the head) properties predominantly influence the degree of compositionality of the compound.",dev,0,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
325,717,3326,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,de17bdef43b2a7ac75447c494b9f791d951a6b27,"However, as it is not possible to survey compositionality rating for diachronic data, we instead use the synchronic data provided by Reddy et al. (2011) (henceforth referred to as REDDY) for evaluating the quality of the Google Books Ngram data as a source for investigating the compositionality of compounds in general.",Uses,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","A multiword is compositional if its meaning can be expressed in terms of the meaning of its constituents. In this paper, we collect and analyse the compositionality judgments for a range of compound nouns using Mechanical Turk. Unlike existing compositionality datasets, our dataset has judgments on the contribution of constituent words as well as judgments for the phrase as a whole. We use this dataset to study the relation between the judgments at constituent level to that for the whole phrase. We then evaluate two different types of distributional models for compositionality detection – constituent based models and composition function based models. Both the models show competitive performance though the composition function based models perform slightly better. In both types, additive models perform better than their multiplicative counterparts.",dev,1,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
326,718,3327,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,de17bdef43b2a7ac75447c494b9f791d951a6b27,"As can also be seen from Table 2 , our correlation values are considerably lower than that of Reddy et al. (2011) , but on par with a replication study by Schulte im Walde et al. (2016a) for compound-mean.",Difference,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","A multiword is compositional if its meaning can be expressed in terms of the meaning of its constituents. In this paper, we collect and analyse the compositionality judgments for a range of compound nouns using Mechanical Turk. Unlike existing compositionality datasets, our dataset has judgments on the contribution of constituent words as well as judgments for the phrase as a whole. We use this dataset to study the relation between the judgments at constituent level to that for the whole phrase. We then evaluate two different types of distributional models for compositionality detection – constituent based models and composition function based models. Both the models show competitive performance though the composition function based models perform slightly better. In both types, additive models perform better than their multiplicative counterparts.",dev,0,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
327,719,3328,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,de17bdef43b2a7ac75447c494b9f791d951a6b27,"We speculate that these differences are potentially due to the use of different data sets, the fact that we use a considerably smaller context window for constructing the word vectors (5 due to the restrictions of Google Ngram corpus vs. 100 in Reddy et al. (2011) and 40 in Schulte im Walde et al. (2016b) ) and the use of a compound-centric setting (as described in 4.1).",Difference,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","A multiword is compositional if its meaning can be expressed in terms of the meaning of its constituents. In this paper, we collect and analyse the compositionality judgments for a range of compound nouns using Mechanical Turk. Unlike existing compositionality datasets, our dataset has judgments on the contribution of constituent words as well as judgments for the phrase as a whole. We use this dataset to study the relation between the judgments at constituent level to that for the whole phrase. We then evaluate two different types of distributional models for compositionality detection – constituent based models and composition function based models. Both the models show competitive performance though the composition function based models perform slightly better. In both types, additive models perform better than their multiplicative counterparts.",dev,0,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
328,720,3331,ABC_b49e6f8181d51a998c6c27a830b98e_18,ACL:W19-4729,de17bdef43b2a7ac75447c494b9f791d951a6b27,Our current work was limited to English compounds from Reddy et al. (2011) .,Uses,s2,"We present work in progress on the temporal progression of compositionality in noun-noun compounds. Previous work has proposed computational methods for determining the compositionality of compounds. These methods try to automatically determine how transparent the meaning of the compound as a whole is with respect to the meaning of its parts. We hypothesize that such a property might change over time. We use the time-stamped Google Books corpus for our diachronic investigations, and first examine whether the vector-based semantic spaces extracted from this corpus are able to predict compositionality ratings, despite their inherent limitations. We find that using temporal information helps predicting the ratings, although correlation with the ratings is lower than reported for other corpora. Finally, we show changes in compositionality over time for a selection of compounds.","A multiword is compositional if its meaning can be expressed in terms of the meaning of its constituents. In this paper, we collect and analyse the compositionality judgments for a range of compound nouns using Mechanical Turk. Unlike existing compositionality datasets, our dataset has judgments on the contribution of constituent words as well as judgments for the phrase as a whole. We use this dataset to study the relation between the judgments at constituent level to that for the whole phrase. We then evaluate two different types of distributional models for compositionality detection – constituent based models and composition function based models. Both the models show competitive performance though the composition function based models perform slightly better. In both types, additive models perform better than their multiplicative counterparts.",dev,1,"The research problem is to investigate the temporal progression of compositionality in noun-noun compounds. Specifically, the study aims to understand how the transparency of meaning in these compounds changes over time. The motivation for this research stems from the hypothesis that compositionality in noun-noun compounds might change over time. This study seeks to explore this hypothesis and provide evidence for the temporal evolution of compositionality.",train,"computational methods, diachronic investigations, noun-noun compounds, temporal progression of compositionality, time-stamped Google Books corpus"
329,872,3827,ABC_3bb6243de9f77fc6ebf2dc24de7faa_21,ACL:N19-1033,3741122650837776e4ea217b46705a05c39fab52,"While most of these approaches are based on deep learning (Cheng et al., 2018; Conneau et al., 2017; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Zhao et al., 2015; Zhou et al., 2018) , there have been some approaches that are inspired by computer vision research, namely by the bag-of-visual-words and by Fisher Vectors (Clinchant and Perronnin, 2013) .",Background,s2,"In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.","Recursive neural network (RvNN) has been proved to be an effective and promising tool to learn sentence representations by explicitly exploiting the sentence structure. However, most existing work can only exploit simple tree structure, e.g., binary trees, or ignore the order of nodes, which yields suboptimal performance. In this paper, we proposed a novel neural network, namely TreeNet, to capture sentences structurally over the raw unconstrained constituency trees, where the number of child nodes can be arbitrary. In TreeNet, each node is learning from its left sibling and right child in a bottom-up left-to-right order, thus enabling the net to learn over any tree. Furthermore, multiple soft gates and a memory cell are employed in implementing the TreeNet to determine to what extent it should learn, remember and output, which proves to be a simple and efficient mechanism for semantic synthesis. Moreover, TreeNet significantly suppresses convolutional neural networks (CNN) and Long Short-Term Memory (LSTM) with fewer parameters. It improves the classification accuracy by 2%-5% with 42% of the best CNN’s parameters or 94% of standard LSTM’s. Extensive experiments demonstrate TreeNet achieves the state-of-the-art performance on all four typical text classification tasks.",dev,0,"The main research problem is to develop an effective representation for text documents that can be used for various text classification tasks. The research is motivated by the need for an effective text document representation, drawing inspiration from successful image representation techniques like the Vector of Locally-Aggregated Descriptors. The goal is to create a representation that is useful for a variety of text classification tasks.",train,"classifier, image representation, k-means, Movie Review data set, text classification tasks, text documents, unsupervised manner, Vector of Locally-Aggregated Descriptors, VLAWE representation"
330,873,3828,ABC_3bb6243de9f77fc6ebf2dc24de7faa_21,ACL:N19-1033,26e743d5bd465f49b9538deaf116c15e61b7951f,"We follow the same evaluation procedure as Kiros et al. (2015) and Hill et al. (2016) , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set.",Similar,s2,"In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.","Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",dev,0,"The main research problem is to develop an effective representation for text documents that can be used for various text classification tasks. The research is motivated by the need for an effective text document representation, drawing inspiration from successful image representation techniques like the Vector of Locally-Aggregated Descriptors. The goal is to create a representation that is useful for a variety of text classification tasks.",train,"classifier, image representation, k-means, Movie Review data set, text classification tasks, text documents, unsupervised manner, Vector of Locally-Aggregated Descriptors, VLAWE representation"
331,874,3829,ABC_3bb6243de9f77fc6ebf2dc24de7faa_21,ACL:N19-1033,786f95cada23d4639aa1a8b922cdb9fb9a9c03fa,"In the learning stage, we employ the Support Vector Machines (SVM) implementation provided by LibSVM (Chang and Lin, 2011 Table 1 : Performance results (in %) of our approach (VLAWE) versus several state-of-the-art methods Cheng et al., 2018; Fu et al., 2018; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 on the Reuters-21578, RT-2k, MR, TREC and Subj data sets.",Background,s2,"In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.","Recurrent Neural Network (RNN) is one of the most popular architectures used in Natural Language Processsing (NLP) tasks because its recurrent structure is very suitable to process variable-length text. RNN can utilize distributed representations of words by first converting the tokens comprising each text into vectors, which form a matrix. And this matrix includes two dimensions: the time-step dimension and the feature vector dimension. Then most existing models usually utilize one-dimensional (1D) max pooling operation or attention-based operation only on the time-step dimension to obtain a fixed-length vector. However, the features on the feature vector dimension are not mutually independent, and simply applying 1D pooling operation over the time-step dimension independently may destroy the structure of the feature representation. On the other hand, applying two-dimensional (2D) pooling operation over the two dimensions may sample more meaningful features for sequence modeling tasks. To integrate the features on both dimensions of the matrix, this paper explores applying 2D max pooling operation to obtain a fixed-length representation of the text. This paper also utilizes 2D convolution to sample more meaningful information of the matrix. Experiments are conducted on six text classification tasks, including sentiment analysis, question classification, subjectivity classification and newsgroup classification. Compared with the state-of-the-art models, the proposed models achieve excellent performance on 4 out of 6 tasks. Specifically, one of the proposed models achieves highest accuracy on Stanford Sentiment Treebank binary classification and fine-grained classification tasks.",dev,0,"The main research problem is to develop an effective representation for text documents that can be used for various text classification tasks. The research is motivated by the need for an effective text document representation, drawing inspiration from successful image representation techniques like the Vector of Locally-Aggregated Descriptors. The goal is to create a representation that is useful for a variety of text classification tasks.",train,"classifier, image representation, k-means, Movie Review data set, text classification tasks, text documents, unsupervised manner, Vector of Locally-Aggregated Descriptors, VLAWE representation"
332,875,3830,ABC_3bb6243de9f77fc6ebf2dc24de7faa_21,ACL:N19-1033,26e743d5bd465f49b9538deaf116c15e61b7951f,"First, we notice that our approach outperforms both baselines on all data sets, unlike other related methods (Le and Mikolov, 2014; Hill et al., 2016) .",Difference,s2,"In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.","Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",dev,0,"The main research problem is to develop an effective representation for text documents that can be used for various text classification tasks. The research is motivated by the need for an effective text document representation, drawing inspiration from successful image representation techniques like the Vector of Locally-Aggregated Descriptors. The goal is to create a representation that is useful for a variety of text classification tasks.",train,"classifier, image representation, k-means, Movie Review data set, text classification tasks, text documents, unsupervised manner, Vector of Locally-Aggregated Descriptors, VLAWE representation"
333,876,3831,ABC_3bb6243de9f77fc6ebf2dc24de7faa_21,ACL:N19-1033,cab4f5b7ee77281c720c917fcf80490fe28826be,"We compare VLAWE with several state-of-theart methods Cheng et al., 2018; Fu et al., 2018; Hill et al., 2016; Iyyer et al., 2015; Kim, 2014; Kiros et al., 2015; Le and Mikolov, 2014; Liu et al., 2017; Shen et al., 2018; Torki, 2018; Xue and Zhou, 2009; Zhao et al., 2015; Zhou et al., 2016 Zhou et al., , 2018 as well as two baseline methods, namely the average of word embeddings and the standard bag-of-words (BOW).",Background,s2,"In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.","Attention-based models have shown to be effective in learning representations for sentence classification. They are typically equipped with multi-hop attention mechanism. However, existing multi-hop models still suffer from the problem of paying much attention to the most frequently noticed words, which might not be important to classify the current sentence. And there is a lack of explicitly effective way that helps the attention to be shifted out of a wrong part in the sentence. In this paper, we alleviate this problem by proposing a differentiated attentive learning model. It is composed of two branches of attention subnets and an example discriminator. An explicit signal with the loss information of the first attention subnet is passed on to the second one to drive them to learn different attentive preference. The example discriminator then selects the suitable attention subnet for sentence classification. Experimental results on real and synthetic datasets demonstrate the effectiveness of our model.",dev,0,"The main research problem is to develop an effective representation for text documents that can be used for various text classification tasks. The research is motivated by the need for an effective text document representation, drawing inspiration from successful image representation techniques like the Vector of Locally-Aggregated Descriptors. The goal is to create a representation that is useful for a variety of text classification tasks.",train,"classifier, image representation, k-means, Movie Review data set, text classification tasks, text documents, unsupervised manner, Vector of Locally-Aggregated Descriptors, VLAWE representation"
334,877,3832,ABC_3bb6243de9f77fc6ebf2dc24de7faa_21,ACL:N19-1033,26e743d5bd465f49b9538deaf116c15e61b7951f,"We follow the same evaluation procedure as Kiros et al. (2015) and Hill et al. (2016) , using 10-fold cross-validation when a train and test split is not pre-defined for a given data set.",Uses,s2,"In this paper, we propose a novel representation for text documents based on aggregating word embedding vectors into document embeddings. Our approach is inspired by the Vector of Locally-Aggregated Descriptors used for image representation, and it works as follows. First, the word embeddings gathered from a collection of documents are clustered by k-means in order to learn a codebook of semnatically-related word embeddings. Each word embedding is then associated to its nearest cluster centroid (codeword). The Vector of Locally-Aggregated Word Embeddings (VLAWE) representation of a document is then computed by accumulating the differences between each codeword vector and each word vector (from the document) associated to the respective codeword. We plug the VLAWE representation, which is learned in an unsupervised manner, into a classifier and show that it is useful for a diverse set of text classification tasks. We compare our approach with a broad range of recent state-of-the-art methods, demonstrating the effectiveness of our approach. Furthermore, we obtain a considerable improvement on the Movie Review data set, reporting an accuracy of 93.3%, which represents an absolute gain of 10% over the state-of-the-art approach.","Unsupervised methods for learning distributed representations of words are ubiquitous in today's NLP research, but far less is known about the best ways to learn distributed phrase or sentence representations from unlabelled data. This paper is a systematic comparison of models that learn such representations. We find that the optimal approach depends critically on the intended application. Deeper, more complex models are preferable for representations to be used in supervised systems, but shallow log-linear models work best for building representation spaces that can be decoded with simple spatial distance metrics. We also propose two new unsupervised representation-learning objectives designed to optimise the trade-off between training time, domain portability and performance.",dev,1,"The main research problem is to develop an effective representation for text documents that can be used for various text classification tasks. The research is motivated by the need for an effective text document representation, drawing inspiration from successful image representation techniques like the Vector of Locally-Aggregated Descriptors. The goal is to create a representation that is useful for a variety of text classification tasks.",train,"classifier, image representation, k-means, Movie Review data set, text classification tasks, text documents, unsupervised manner, Vector of Locally-Aggregated Descriptors, VLAWE representation"
335,1135,4911,ABC_4ad830d8377d2584798a30bed65254_28,ACL:W19-3821,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"This is shown when training word embeddings, a vector representation of words, in news sets with crowd-sourcing evaluation to quantify the presence of biases, such as gender bias, in those representation (Bolukbasi et al., 2016) .",Background,s2,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",dev,0,"The research problem is addressing the issue of fairness, specifically gender bias, in neural machine translation. This bias arises from the models being trained on large text corpora that contain biases and stereotypes, leading to the models inheriting these biases. The motivation stems from the observation that existing methods for reducing gender bias in other natural language processing tools, like word embeddings, suggest potential solutions for NMT. The researchers aim to adapt these methods to address gender bias in neural machine translation, building upon the successful application of debiasing techniques in other areas of NLP.",train,"debiasing techniques, gender bias evaluation, large text corpora, natural language processing tools, neural machine translation, neural machine translation, Neural machine translation, Neural models, Recent methods, Transformer translation architecture, WMT English-Spanish benchmark task"
336,1136,4912,ABC_4ad830d8377d2584798a30bed65254_28,ACL:W19-3821,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Debiaswe is a postprocess method for debiasing previously generated embeddings (Bolukbasi et al., 2016) .",Background,s2,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",dev,0,"The research problem is addressing the issue of fairness, specifically gender bias, in neural machine translation. This bias arises from the models being trained on large text corpora that contain biases and stereotypes, leading to the models inheriting these biases. The motivation stems from the observation that existing methods for reducing gender bias in other natural language processing tools, like word embeddings, suggest potential solutions for NMT. The researchers aim to adapt these methods to address gender bias in neural machine translation, building upon the successful application of debiasing techniques in other areas of NLP.",train,"debiasing techniques, gender bias evaluation, large text corpora, natural language processing tools, neural machine translation, neural machine translation, Neural machine translation, Neural models, Recent methods, Transformer translation architecture, WMT English-Spanish benchmark task"
337,1137,4913,ABC_4ad830d8377d2584798a30bed65254_28,ACL:W19-3821,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"More specifically, gender stereotypes are learned from human generated corpora as shown by (Bolukbasi et al., 2016) .",Background,s2,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",dev,0,"The research problem is addressing the issue of fairness, specifically gender bias, in neural machine translation. This bias arises from the models being trained on large text corpora that contain biases and stereotypes, leading to the models inheriting these biases. The motivation stems from the observation that existing methods for reducing gender bias in other natural language processing tools, like word embeddings, suggest potential solutions for NMT. The researchers aim to adapt these methods to address gender bias in neural machine translation, building upon the successful application of debiasing techniques in other areas of NLP.",train,"debiasing techniques, gender bias evaluation, large text corpora, natural language processing tools, neural machine translation, neural machine translation, Neural machine translation, Neural models, Recent methods, Transformer translation architecture, WMT English-Spanish benchmark task"
338,1138,4914,ABC_4ad830d8377d2584798a30bed65254_28,ACL:W19-3821,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Debiaswe (Bolukbasi et al., 2016 ) is a postprocess method for debiasing word embeddings.",Background,s2,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",dev,0,"The research problem is addressing the issue of fairness, specifically gender bias, in neural machine translation. This bias arises from the models being trained on large text corpora that contain biases and stereotypes, leading to the models inheriting these biases. The motivation stems from the observation that existing methods for reducing gender bias in other natural language processing tools, like word embeddings, suggest potential solutions for NMT. The researchers aim to adapt these methods to address gender bias in neural machine translation, building upon the successful application of debiasing techniques in other areas of NLP.",train,"debiasing techniques, gender bias evaluation, large text corpora, natural language processing tools, neural machine translation, neural machine translation, Neural machine translation, Neural models, Recent methods, Transformer translation architecture, WMT English-Spanish benchmark task"
339,1139,4915,ABC_4ad830d8377d2584798a30bed65254_28,ACL:W19-3821,babbf74939612ee2f0203c30a190b4b95881415b,"Then, we debiased the embeddings using Debiaswe (Bolukbasi et al., 2016) and also trained its gender neutral version with GN-GloVe (Zhao et al., 2018b) .",Uses,s2,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","Word embedding models have become a fundamental component in a wide range of Natural Language Processing (NLP) applications. However, embeddings trained on human-generated corpora have been demonstrated to inherit strong gender stereotypes that reflect social constructs. To address this concern, in this paper, we propose a novel training procedure for learning gender-neutral word embeddings. Our approach aims to preserve gender information in certain dimensions of word vectors while compelling other dimensions to be free of gender influence. Based on the proposed method, we generate a Gender-Neutral variant of GloVe (GN-GloVe). Quantitative and qualitative experiments demonstrate that GN-GloVe successfully isolates gender information without sacrificing the functionality of the embedding model.",dev,1,"The research problem is addressing the issue of fairness, specifically gender bias, in neural machine translation. This bias arises from the models being trained on large text corpora that contain biases and stereotypes, leading to the models inheriting these biases. The motivation stems from the observation that existing methods for reducing gender bias in other natural language processing tools, like word embeddings, suggest potential solutions for NMT. The researchers aim to adapt these methods to address gender bias in neural machine translation, building upon the successful application of debiasing techniques in other areas of NLP.",train,"debiasing techniques, gender bias evaluation, large text corpora, natural language processing tools, neural machine translation, neural machine translation, Neural machine translation, Neural models, Recent methods, Transformer translation architecture, WMT English-Spanish benchmark task"
340,1140,4916,ABC_4ad830d8377d2584798a30bed65254_28,ACL:W19-3821,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Debiaswe (Bolukbasi et al., 2016 ) is a debiasing post-process performed on trained embeddings.",Background,s2,"Neural machine translation has significantly pushed forward the quality of the field. However, there are remaining big issues with the output translations and one of them is fairness. Neural models are trained on large text corpora which contain biases and stereotypes. As a consequence, models inherit these social biases. Recent methods have shown results in reducing gender bias in other natural language processing tools such as word embeddings. We take advantage of the fact that word embeddings are used in neural machine translation to propose a method to equalize gender biases in neural machine translation using these representations. Specifically, we propose, experiment and analyze the integration of two debiasing techniques over GloVe embeddings in the Transformer translation architecture. We evaluate our proposed system on the WMT English-Spanish benchmark task, showing gains up to one BLEU point. As for the gender bias evaluation, we generate a test set of occupations and we show that our proposed system learns to equalize existing biases from the baseline system.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",dev,0,"The research problem is addressing the issue of fairness, specifically gender bias, in neural machine translation. This bias arises from the models being trained on large text corpora that contain biases and stereotypes, leading to the models inheriting these biases. The motivation stems from the observation that existing methods for reducing gender bias in other natural language processing tools, like word embeddings, suggest potential solutions for NMT. The researchers aim to adapt these methods to address gender bias in neural machine translation, building upon the successful application of debiasing techniques in other areas of NLP.",train,"debiasing techniques, gender bias evaluation, large text corpora, natural language processing tools, neural machine translation, neural machine translation, Neural machine translation, Neural models, Recent methods, Transformer translation architecture, WMT English-Spanish benchmark task"
341,1364,6140,ABC_a5d7f5c5fed218149818463427d6a1_38,ACL:N19-1064,3febb2bed8865945e7fddc99efd791887bb7e14f,"In this work, we extend these analyses to the ELMo contextualized word embeddings. Our work provides a new intrinsic analysis of how ELMo represents gender in biased ways. Distributed representations of words in the form of word embeddings Pennington et al., 2014) and contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; McCann et al., 2017; Radford et al., 2019) have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data (Bolukbasi et al., 2016; Caliskan et al., 2017) .",Background,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",dev,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
342,1365,6143,ABC_a5d7f5c5fed218149818463427d6a1_38,ACL:N19-1064,5966d7c7f60898d610812e24c64d4d57855ad86a,"For word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias.",Background,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
343,1366,6144,ABC_a5d7f5c5fed218149818463427d6a1_38,ACL:N19-1064,5966d7c7f60898d610812e24c64d4d57855ad86a,"For word representations, Bolukbasi et al. (2016) and Caliskan et al. (2017) show that word embeddings encode societal biases about gender roles and occupations, e.g. engineers are stereotypically men, and nurses are stereotypically women. As a consequence, downstream applications that use these pretrained word embeddings also reflect this bias.",Motivation,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
344,1367,6145,ABC_a5d7f5c5fed218149818463427d6a1_38,ACL:N19-1064,8ae1af4a424f5e464d46903bc3d18fe1cf1434ff,"Figure 1 shows there are two principal components for gender in ELMo, in contrast to GloVe which only has one (Bolukbasi et al., 2016) . The two principal components in ELMo seem to represent the gender from the contextual information (Contextual Gender) as well as the gender embedded in the word itself (Occupational Gender).",Difference,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.",dev,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
345,1369,6147,ABC_a5d7f5c5fed218149818463427d6a1_38,ACL:N19-1064,3febb2bed8865945e7fddc99efd791887bb7e14f,"In this work, we extend these analyses to the ELMo contextualized word embeddings. Our work provides a new intrinsic analysis of how ELMo represents gender in biased ways. Distributed representations of words in the form of word embeddings Pennington et al., 2014) and contextualized word embeddings (Peters et al., 2018; Devlin et al., 2018; Radford et al., 2018; McCann et al., 2017; Radford et al., 2019) have led to huge performance improvement on many NLP tasks. However, several recent studies show that training word embeddings in large corpora could lead to encoding societal biases present in these human-produced data (Bolukbasi et al., 2016; Caliskan et al., 2017) .",Motivation,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",dev,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
346,1371,6149,ABC_a5d7f5c5fed218149818463427d6a1_38,ACL:N19-1064,8ae1af4a424f5e464d46903bc3d18fe1cf1434ff,"Data augmentation is performed by replacing gender revealing entities in the OntoNotes dataset with words indicating the opposite gender and then training on the union of the original data and this swapped data. In addition, they find it useful to also mitigate bias in supporting resources and therefore replace standard GloVe embeddings with bias mitigated word embeddings from Bolukbasi et al. (2016) .",Uses,s2,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.",dev,1,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
347,1550,7125,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,2e61ddfa317a197e27ed90d4eab3a19882fe3e8e,"Such paths are not only useful for link prediction (Lao et al., 2011; Gardner et al., 2014) , but also for finding explanations for direct links and help with targeted information extraction to fill in incomplete knowledge repositories (Yin et al., 2018; Zhou and Nastase, 2018) .",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Large scale knowledge graphs (KGs) such as Freebase are generally incomplete. Reasoning over multi-hop (mh) KG paths is thus an important capability that is needed for question answering or other NLP tasks that require knowledge about the world. mh-KG reasoning includes diverse scenarios, e.g., given a head entity and a relation path, predict the tail entity; or given two entities connected by some relation paths, predict the unknown relation between them. We present ROPs, recurrent one-hop predictors, that predict entities at each step of mh-KB paths by using recurrent neural networks and vector representations of entities and relations, with two benefits: (i) modeling mh-paths of arbitrary lengths while updating the entity and relation representations by the training signal at each step; (ii) handling different types of mh-KG reasoning in a unified framework. Our models show state-of-the-art for two important multi-hop KG reasoning tasks: Knowledge Base Completion and Path Query Answering.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
348,1551,7126,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"The idea of using paths in the graph has then been applied to the task of link prediction (Lao et al., 2011) , and extended to incorporate textual information (Gardner et al., 2014) .",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
349,1552,7127,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"They can be used in different ways, e.g. (i) as features in a link prediction system (e.g. (Gardner et al., 2014) ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances.",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
350,1553,7128,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,2e61ddfa317a197e27ed90d4eab3a19882fe3e8e,"The paths themselves can be incorporated in different ways in a model -as features (Lao et al., 2011; Gardner et al., 2014) , as Horn clauses to provide rules for inference in KGs whether directly or through scores that represent the strength of the path as a direct relation (Neelakantan et al., 2015; Guu et al., 2015) , also taking into account information about intermediary nodes (Das et al., 2017; Yin et al., 2018) .",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Large scale knowledge graphs (KGs) such as Freebase are generally incomplete. Reasoning over multi-hop (mh) KG paths is thus an important capability that is needed for question answering or other NLP tasks that require knowledge about the world. mh-KG reasoning includes diverse scenarios, e.g., given a head entity and a relation path, predict the tail entity; or given two entities connected by some relation paths, predict the unknown relation between them. We present ROPs, recurrent one-hop predictors, that predict entities at each step of mh-KB paths by using recurrent neural networks and vector representations of entities and relations, with two benefits: (i) modeling mh-paths of arbitrary lengths while updating the entity and relation representations by the training signal at each step; (ii) handling different types of mh-KG reasoning in a unified framework. Our models show state-of-the-art for two important multi-hop KG reasoning tasks: Knowledge Base Completion and Path Query Answering.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
351,1554,7129,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"Negative sampling The number of negative instances used in (Gardner et al., 2014) is not clearly stated.",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
352,1555,7130,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,322aa32b2a409d2e135dbb14736d9aeb497f1c52,"Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals (Gardner et al., 2014; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) .",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
353,1556,7131,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"The graphs built by Gardner et al. (2014) cover several variations, where the KGs were enhanced with < subject, verb, object > triples extracted from dependency parses of ClueWeb documents.",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
354,1557,7132,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"Figure 1: Knowledge graphs statistics on a logarithmic scale: relation and nodes frequencies for Freebase and NELL (the version used by (Gardner et al., 2014) and in this paper).",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
355,1558,7133,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"For all Freebase KG configurations, Gardner et al. (2014) have 1000 paths for most relations (approx. 6 of the relations have between 230 and 973).",Background,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
356,1559,7134,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"They can be used in different ways, e.g. (i) as features in a link prediction system (e.g. (Gardner et al., 2014) ), (ii) to fill in larger portions of the graph by producing, rather than finding, groundings of the path for specific instances. In the work presented here we test the abstract paths through the link prediction task, so we will try to ground abstract paths for relation instances in the training and test data.",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
357,1560,7135,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"We test the extracted paths through the link prediction task on Freebase (Bollacker et al., 2008) and NELL (Carlson et al., 2010a) , using Gardner et al. (2014) 's experimental set-up: pairs of nodes are represented using their connected paths as fea-tures, and a model for predicting the direct relations is learned and tested on training and test sets for 24 relations in Freebase and 10 relations in NELL.",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
358,1561,7136,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"The Path Ranking Algorithm formalism originally proposed by (Lao and Cohen, 2010) performs two main steps to represent of a pair of nodes in a graph: (i) feature selection -adding paths that connect the node pair; (ii) feature computation - Table 1 : Graph statistics on the datasets used by (Gardner et al., 2014) , and their abstract versions associating a value for each added path.",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
359,1562,7137,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"We build abstract graphs and paths from the Freebase and NELL data described in (Gardner et al., 2014) .",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
360,1563,7138,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"The overall results of the experiments are presented in Table 3, and the relation-level results are  in Tables 4 for NELL, and 5 Table 3 : Results on the three graph variations of Freebase and NELL as reported by (Gardner et al., 2014) (G) and using abstract graphs (KG A ).",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
361,1564,7139,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,The results presented by Gardner et al. (2014) show that this configuration very rarely (and never overall) leads to better results than the other graph variations.,Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
362,1565,7140,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"Because we want to compare the abstract paths found using the abstract graph with paths found using PRA, we use the experimental set-up of (Gardner et al., 2014) , where we replace the feature selection and feature computation steps with the approach presented here.",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
363,1566,7141,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"The data thus obtained is used for training a linear regression model (similarly to (Gardner et al., 2014) ), and tested on the provided test sets and evaluated using mean average precision (MAP).",Similar,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
364,1567,7142,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"We include the best result on PRA (on any variation of the graph), as reported by (Gardner et al., 2014) , although since we used different negative instances the results are not directly comparable.",Uses,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,1,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
365,1568,7143,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"To test the quality of these paths we ground them using the original KG and use these grounded paths in a learning framework similar to (Gardner et al., 2014) .",Similar,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
366,1569,7144,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,322aa32b2a409d2e135dbb14736d9aeb497f1c52,"Algorithms that harness path information often mine paths either by performing costly random walks (Guu et al., 2015) , traversals (Gardner et al., 2014; Neelakantan et al., 2015; Das et al., 2016) or by constructing paths through generative models (Das et al., 2017; Ding et al., 2018) . Here, we adopt a different approach, by abstracting the graph first, then finding paths in this graph through traversal algorithms.",Difference,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Early works performed this task via simple models developed over KG triples. Recent attempts focused on either designing more complicated triple scoring models, or incorporating extra information beyond triples. This paper, by contrast, investigates the potential of using very simple constraints to improve KG embedding. We examine non-negativity constraints on entity representations and approximate entailment constraints on relation representations. The former help to learn compact and interpretable representations for entities. The latter further encode regularities of logical entailment between relations into their distributed representations. These constraints impose prior beliefs upon the structure of the embedding space, without negative impacts on efficiency or scalability. Evaluation on WordNet, Freebase, and DBpedia shows that our approach is simple yet surprisingly effective, significantly and consistently outperforming competitive baselines. The constraints imposed indeed improve model interpretability, leading to a substantially increased structuring of the embedding space. Code and data are available at https://github.com/iieir-km/ComplEx-NNE_AER.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
367,1570,7147,ABC_b7a7b7d9a6594dadb5853c49cddddf_0,ACL:S19-1016,5346525f5022b3d60e7a954b50772ea75d967e7d,"We include the best result on PRA (on any variation of the graph), as reported by (Gardner et al., 2014) , although since we used different negative instances the results are not directly comparable.",Difference,s2,"Knowledge graphs, which provide numerous facts in a machine-friendly format, are incomplete. Information that we induce from such graphs – e.g. entity embeddings, relation representations or patterns – will be affected by the imbalance in the information captured in the graph – by biasing representations, or causing us to miss potential patterns. To partially compensate for this situation we describe a method for representing knowledge graphs that capture an intensional representation of the original extensional information. This representation is very compact, and it abstracts away from individual links, allowing us to find better path candidates, as shown by the results of link prediction using this information.","Much work in recent years has gone into the construction of large knowledge bases (KBs), such as Freebase, DBPedia, NELL, and YAGO. While these KBs are very large, they are still very incomplete, necessitating the use of inference to fill in gaps. Prior work has shown how to make use of a large text corpus to augment random walk inference over KBs. We present two improvements to the use of such large corpora to augment KB inference. First, we present a new technique for combining KB relations and surface text into a single graph representation that is much more compact than graphs used in prior work. Second, we describe how to incorporate vector space similarity into random walk inference over KBs, reducing the feature sparsity inherent in using surface text. This allows us to combine distributional similarity with symbolic logical inference in novel and effective ways. With experiments on many relations from two separate KBs, we show that our methods significantly outperform prior work on KB inference, both in the size of problem our methods can handle and in the quality of predictions made.",dev,0,"The research problem is the incompleteness of knowledge graphs. This incompleteness leads to biased representations and missed patterns when inducing information from these graphs. The motivation behind this research is to address the issue of knowledge graph incompleteness by developing a method that can capture more comprehensive information, leading to improved link prediction and pattern discovery.",train,"graphs, link prediction, relation representations"
368,1805,7985,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,26b16f5815407ab59d2cc4589459bd71c92ae32e,"Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet (Noraset et al., 2017) for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities.",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words’ ambiguity and polysemy leads to performance improvement.",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
369,1806,7986,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,26b16f5815407ab59d2cc4589459bd71c92ae32e,"Considering various contexts where we need definitions of phrases, we evaluated our method with four datasets including WordNet (Noraset et al., 2017) for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slangs, and a newlycreated Wikipedia dataset for entities. Experimental results demonstrate the effectiveness of our method against the three baselines stated above (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) .",Difference,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words’ ambiguity and polysemy leads to performance improvement.",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
370,1807,7987,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Datasets To evaluate our model on the word description task on WordNet, we followed Noraset et al. (2017) and extracted data from WordNet 7 using the dict-definition 8 toolkit.",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
371,1830,8166,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,26b16f5815407ab59d2cc4589459bd71c92ae32e,"Considering various applications where we need definitions of expressions, we evaluated our method with four datasets including WordNet (Noraset et al., 2017) for general words, the Oxford dictionary (Gadetsky et al., 2018) for polysemous words, Urban Dictionary (Ni and Wang, 2017) for rare idioms or slang, and a newlycreated Wikipedia dataset for entities.",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words’ ambiguity and polysemy leads to performance improvement.",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
372,1831,8167,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character "" ,"" such as ""sonic boom."" Following Noraset et al. (2017), we set the CNN kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
373,1832,8168,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"In order to capture the surface information of X trg , we construct character-level CNNs (Eq. (6)) following (Noraset et al., 2017) .",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
374,1833,8169,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Datasets To evaluate our model on the word description task on WordNet, we followed Noraset et al. (2017) and extracted data from WordNet using the dict-definition 9 toolkit.",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
375,1834,8170,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Since not all entries in WordNet have usage examples, our dataset is a small subset of Noraset et al. (2017) .",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
376,1835,8171,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,26b16f5815407ab59d2cc4589459bd71c92ae32e,"Global (Noraset et al., 2017) , (2) Local (Ni and Wang, 2017) with CNN, (3) I-Attention (Gadetsky et al., 2018) , and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the best model (S + G + CH) in Noraset et al. (2017) .",Uses,s2,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","We explore recently introduced definition modeling technique that provided the tool for evaluation of different distributed vector representations of words through modeling dictionary definitions of words. In this work, we study the problem of word ambiguities in definition modeling and propose a possible solution by employing latent variable modeling and soft attention mechanisms. Our quantitative and qualitative evaluation and analysis of the model shows that taking into account words’ ambiguity and polysemy leads to performance improvement.",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
377,2000,8960,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"In addition, following the success of Coarse-grain Fine-grain Coattention (CFC) network (Zhong et al., 2019) , we apply both co-attention and self-attention to learn queryaware node representations of candidates, documents and entities;",Uses,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,1,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
378,2001,8961,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"Co-attention: Co-attention has achieved great success for single document reading comprehension tasks (Seo et al., 2016; Xiong et al., 2016) , and recently was applied to multiple-hop reading comprehension (Zhong et al., 2019) .",Background,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,0,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
379,2002,8962,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"Recently, an attention based system (Zhong et al., 2019) utilizing both documentlevel and entity-level information achieved stateof-the-art results on WIKIHOP data set, proving that techniques like co-attention and self-attention widely employed in single-document RC tasks are also useful in multi-document RC tasks.",Background,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,0,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
380,2003,8963,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"The co-attention and self-attention based encoding of multi-level information presented in each input is also inspired by the CFC model (Zhong et al., 2019) because they show the effectiveness of attention mechanisms.",Uses,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,1,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
381,2004,8964,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"We follow the implementation of coattention in (Zhong et al., 2019) .",Uses,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,1,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
382,2005,8965,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"The study presented in this paper is directly related to existing research on multi-hop reading comprehension across multiple documents (Dhingra et al., 2018; Song et al., 2018; De Cao et al., 2018; Zhong et al., 2019; Kundu et al., 2018) . The method presented in this paper is similar to previous studies using GNN for multi-hop reasoning (Song et al., 2018; De Cao et al., 2018) .",Uses,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,1,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
383,2006,8966,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"Self-attentive pooling: while co-attention yields a query-aware contextual representation of documents, self-attentive pooling is designed to convert the sequential contextual representation to a fixed dimensional non-sequential feature vector by selecting important query-aware information (Zhong et al., 2019) .",Uses,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,1,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
384,2007,8967,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,We expect S ca carries query-aware contextual information of supporting documents as shown by Zhong et al. (2019) .,Uses,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,1,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
385,2008,8968,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,Our context encoding module is different from the one used in Zhong et al. (2019) in following aspects: 1) we compute the co-attention between query and candidates which is not presented in the CFC model.,Difference,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,0,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
386,2009,8969,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,We expect S ca carries query-aware contextual information of supporting documents as shown by Zhong et al. (2019) .,Difference,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,0,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
387,2010,8970,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"2) For entity word sequences, we first calculate co-attention with query and then use selfattention to summarize each entity word sequence while Zhong et al. (2019) first do self-attention on entity word sequences to get a sequence of entity vectors in each documents.",Difference,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,0,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
388,2011,8971,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"We show that our proposed HDE graph based model improves the state-of-the-art accuracy on development set from 67.1% (Kundu et al., 2018) to 68.1%, on the blind test set from 70.6% (Zhong et al., 2019) to 70.9%.",Difference,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,0,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
389,2012,8977,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"We use simple exact match strategy (De Cao et al., 2018; Zhong et al., 2019) to find the locations of mentions of query subject and candidates, i.e. we need the start and end positions of each mention.",Uses,s2,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,1,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
390,2414,10664,ABC_a5d7f5c5fed218149818463427d6a1_38,ACL:N19-1064,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"To mitigate bias from word embeddings,<span style=""background: yellow; display: inline-block""> Bolukbasi et al. (2016)</span> propose a post-processing method to project out the bias subspace from the pre-trained embeddings. Their method is shown to reduce the gender information from the embeddings of gender-neutral words, and, remarkably, maintains the same level of performance on different downstream NLP tasks.",Background,llm,"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",dev,0,"The research problem is the presence of gender bias in ELMo's contextualized word vectors, which can lead to biased downstream applications. Specifically, the research investigates how the bias manifests in the training data and the embeddings themselves, and its impact on a coreference system. The motivation behind this research is to address the issue of gender bias in word embeddings, specifically in ELMo, which is a widely used language model. The research aims to understand the source of the bias, its impact on downstream applications, and ultimately to provide solutions to mitigate the bias.",train,"coreference system, ELMo embeddings, WinoBias, WinoBias probing corpus"
391,2440,10963,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"On the other hand, <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings.",Background,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
392,2441,10964,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Recently, <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> introduced a task of generating a definition sentence of a word from its pre-trained embedding.",Background,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
393,2442,10965,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> .",Background,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
394,2443,10966,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"On the other hand, <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> attempted to generate a definition of a word from its word embedding induced from massive text, followed by Gadetsky et al. (2018) that refers to a local context to define a polysemous word with a local context by choosing relevant dimensions of their embeddings. Although these research efforts revealed that both local and global contexts of words are useful in generating their definitions, none of these studies exploited both local and global contexts directly. In this study, we tackle a task of describing (defining) a phrase when given its local context as (Ni and Wang, 2017) , while allowing access to other usage examples via word embeddings trained from massive text (global contexts) (Noraset et al., 2017; Gadetsky et al., 2018) .",Motivation,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
395,2444,10967,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,""" Previous work on the definition generation task<span style=""background: yellow; display: inline-block""> (Noraset et al., 2017)</span> has shown that global contexts can be useful clues when generating definitions of unknown words.",Background,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
396,2445,10968,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"To incorporate the different types of contexts, we propose to use a GATE function<span style=""background: yellow; display: inline-block""> (Noraset et al., 2017)</span> to dynamically control how the global and local contexts influence the generation of the description.",Uses,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
397,2446,10969,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"We achieve this by two different strategies proposed by <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> .",Uses,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
398,2447,10970,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"We implemented four methods including three baselines: (1) Global, (2) Local, (3) I-Attention, and our proposed model, (4) LOGCaD. The Global model is our reimplementation of the strongest model (S + G + CH) in<span style=""background: yellow; display: inline-block""> (Noraset et al., 2017)</span> .",Uses,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
399,2448,10971,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"In order to capture prefixes and suffixes in X trg , we construct character-level CNNs (Eq. (5)) following<span style=""background: yellow; display: inline-block""> (Noraset et al., 2017)</span> . Note that the input to the CNNs is a sequence of words in X trg , which are concatenated with special character "" ,"" such as ""sonic boom."" Following <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> , we set the kernels of length 2-6 and size 10, 30, 40, 40, 40 respectively with a stride of 1 to obtain a 160-dimensional vector c trg .",Uses,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
400,2449,10972,ABC_a5f33403d23cdc0532547266f1841a_3,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Their model does not take advantage of global contexts (word embeddings induced from massive text) as was used in <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> . Our task of describing phrases with its given context is a generalization of these three tasks (Noraset et al., 2017; Ni and Wang, 2017; Gadetsky et al., 2018) , and the proposed method naturally utilizes both local and global contexts of a word in question.",Uses,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,1,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
401,2450,11007,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"On the other hand, <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> attempted to generate a definition of a word from an embedding induced from massive text (which can be seen as global context).",Background,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
402,2451,11008,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"Recently, <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> introduced a task of generating a definition sentence of a word from its pre-trained embedding.",Background,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
403,2452,11009,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"In order to capture the interaction between the local and global contexts, we adopt a GATE(·) function (Eq. (7)) which is similar to <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> .",Similar,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
404,2453,11010,ABC_5203c1037fe57bd1b813c0bf1ff5c4_4,ACL:N19-1350,be18e1f566baeffb71fc4eed24f79dc8724879d4,"To incorporate the different types of contexts, we propose to use a gate function similar to <span style=""background: yellow; display: inline-block"">Noraset et al. (2017)</span> to dynamically control how the global and local contexts influence the description.",Similar,llm,"When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.","
 
 Distributed representations of words have been shown to capture lexical semantics, based on their effectiveness in word similarity and analogical relation tasks. But, these tasks only evaluate lexical semantics indirectly. In this paper, we study whether it is possible to utilize distributed representations to generate dictionary definitions of words, as a more direct and transparent representation of the embeddings' semantics. We introduce definition modeling, the task of generating a definition for a given word and its embedding. We present different definition model architectures based on recurrent neural networks, and experiment with the models over multiple data sets. Our results show that a model that controls dependencies between the word being defined and the definition words performs significantly better, and that a character-level convolution layer that leverages morphology can complement word-level embeddings. Our analysis reveals which components of our models contribute to accuracy. Finally, the errors made by a definition model may provide insight into the shortcomings of word embeddings.
 
",dev,0,"The research problem is the difficulty in understanding unfamiliar words and phrases in text. This ambiguity is often resolved by humans through local and global context, but the study investigates whether machines can achieve the same through similar mechanisms. Specifically, the research examines the relative importance of local and global context for machines in resolving ambiguity. The motivation behind this research is to explore the potential of machines in resolving ambiguity in text, specifically by investigating the effectiveness of different types of context for machine understanding. The research aims to develop a model that can accurately describe unfamiliar phrases based on both local and global contexts.",train,"context encoders, definition generation, description decoder, dictionaries, natural language, neural description model, non-standard English explanation, WordNet"
405,2500,11123,ABC_5095f2af3f0c51283c8fbee08a17ac_7,ACL:P19-1260,e17c13217e1fad11bae46820c4acae1745f69b43,"Evaluated on the blind test set of WIKIHOP, our proposed end-to-end trained single neural model beats the current stateof-the-art results in<span style=""background: yellow; display: inline-block""> (Zhong et al., 2019)</span> 1 , without using pretrained contextual ELMo embedding (Peters et al., 2018) .",Difference,llm,"Multi-hop reading comprehension (RC) across documents poses new challenge over single-document RC because it requires reasoning over multiple documents to reach the final answer. In this paper, we propose a new model to tackle the multi-hop RC problem. We introduce a heterogeneous graph with different types of nodes and edges, which is named as Heterogeneous Document-Entity (HDE) graph. The advantage of HDE graph is that it contains different granularity levels of information including candidates, documents and entities in specific document contexts. Our proposed model can do reasoning over the HDE graph with nodes representation initialized with co-attention and self-attention based context encoders. We employ Graph Neural Networks (GNN) based message passing algorithms to accumulate evidences on the proposed HDE graph. Evaluated on the blind test set of the Qangaroo WikiHop data set, our HDE graph based single model delivers competitive result, and the ensemble model achieves the state-of-the-art performance.","End-to-end neural models have made significant progress in question answering, however recent studies show that these models implicitly assume that the answer and evidence appear close together in a single document. In this work, we propose the Coarse-grain Fine-grain Coattention Network (CFC), a new question answering model that combines information from evidence across multiple documents. The CFC consists of a coarse-grain module that interprets documents with respect to the query then finds a relevant answer, and a fine-grain module which scores each candidate answer by comparing its occurrences across all of the documents with the query. We design these modules using hierarchies of coattention and self-attention, which learn to emphasize different parts of the input. On the Qangaroo WikiHop multi-evidence question answering task, the CFC obtains a new state-of-the-art result of 70.6% on the blind test set, outperforming the previous best by 3% accuracy despite not using pretrained contextual encoders.",dev,0,"The research problem is addressing the challenge of multi-hop reading comprehension (RC) across multiple documents, which requires reasoning over multiple documents to reach the final answer. This problem is distinct from single-document RC, where reasoning is limited to a single document. The motivation behind this research is to develop an effective approach for multi-hop reading comprehension across multiple documents, addressing the challenge of reasoning over multiple sources of information to arrive at the final answer.",train,"blind test set, ensemble model, HDE graph based single model, Heterogeneous Document-Entity ( HDE ) graph, heterogeneous graph, multi-hop RC problem, Multi-hop reading comprehension ( RC ), Qangaroo WikiHop data set, self-attention based context encoders, single-document RC"
406,771,2867,ABC_195f41862b929318787aad9d8e5a1c_16,CorpusID:67856208,bc8fa64625d9189f5801837e7b133e7fe3c581f7,Our FastFusionNet reaches F1 75% in 4 epochs and achieves at F1 82.5% at the end which matches the reported F1 82.5% of FusionNet without CoVe on SQuAD development set [12] .,Difference,s2,"In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet [12]. FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements. For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19]. The resulting architecture obtains state-of-the-art results on DAWNBench [5] while achieving the lowest training and inference time on SQuAD [25] to-date. The code is available at this https URL.","Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",test,0,"The research problem is the high computational cost of FusionNet, a high-performing reading comprehension architecture. The goal is to create a variant that maintains performance while reducing computational requirements. The motivation is to improve the efficiency of existing reading comprehension architectures like FusionNet by reducing computational requirements without sacrificing performance.",train,"BiLSTMs, CoVe layers, DAWNBench, FastFusionNet, FusionNet, FusionNet, maximum retrieval accuracy, reading comprehension architecture, SRU layers"
407,772,2868,ABC_195f41862b929318787aad9d8e5a1c_16,CorpusID:67856208,bc8fa64625d9189f5801837e7b133e7fe3c581f7,"FusionNet [12] is reading comprehension model built on top of DrQA by introducing Fully-aware attention layers (context-question attention and context self-attention), contextual embeddings [21] , and more RNN layers.",Background,s2,"In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet [12]. FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements. For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19]. The resulting architecture obtains state-of-the-art results on DAWNBench [5] while achieving the lowest training and inference time on SQuAD [25] to-date. The code is available at this https URL.","Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",test,0,"The research problem is the high computational cost of FusionNet, a high-performing reading comprehension architecture. The goal is to create a variant that maintains performance while reducing computational requirements. The motivation is to improve the efficiency of existing reading comprehension architectures like FusionNet by reducing computational requirements without sacrificing performance.",train,"BiLSTMs, CoVe layers, DAWNBench, FastFusionNet, FusionNet, FusionNet, maximum retrieval accuracy, reading comprehension architecture, SRU layers"
408,773,2869,ABC_195f41862b929318787aad9d8e5a1c_16,CorpusID:67856208,bc8fa64625d9189f5801837e7b133e7fe3c581f7,"Figure 2 provides an analysis of the individual components of FusionNet that the contextual embedding layer, i.e. CoVe [21] , with several layers of wide LSTMs, takes up to 35.5% of the inference time while only contributing a 1.1% improvement of F1 Score (from 82.5% to 83.6%) Huang et al. [12] .",Background,s2,"In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet [12]. FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements. For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19]. The resulting architecture obtains state-of-the-art results on DAWNBench [5] while achieving the lowest training and inference time on SQuAD [25] to-date. The code is available at this https URL.","Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.",test,0,"The research problem is the high computational cost of FusionNet, a high-performing reading comprehension architecture. The goal is to create a variant that maintains performance while reducing computational requirements. The motivation is to improve the efficiency of existing reading comprehension architectures like FusionNet by reducing computational requirements without sacrificing performance.",train,"BiLSTMs, CoVe layers, DAWNBench, FastFusionNet, FusionNet, FusionNet, maximum retrieval accuracy, reading comprehension architecture, SRU layers"
409,774,2870,ABC_195f41862b929318787aad9d8e5a1c_16,CorpusID:67856208,f37e1b62a767a307c046404ca96bc140b3e68cb5,"The Question-Context Attention Layer is a fully-aware attention module [12] which takes the history (concatenation of GloVe, low-level, and high-level features) of each context word and question words as query and key for three attention modules, and represents each context word as three different vectors:",Similar,s2,"In this technical report, we introduce FastFusionNet, an efficient variant of FusionNet [12]. FusionNet is a high performing reading comprehension architecture, which was designed primarily for maximum retrieval accuracy with less regard towards computational requirements. For FastFusionNets we remove the expensive CoVe layers [21] and substitute the BiLSTMs with far more efficient SRU layers [19]. The resulting architecture obtains state-of-the-art results on DAWNBench [5] while achieving the lowest training and inference time on SQuAD [25] to-date. The code is available at this https URL.","Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",test,0,"The research problem is the high computational cost of FusionNet, a high-performing reading comprehension architecture. The goal is to create a variant that maintains performance while reducing computational requirements. The motivation is to improve the efficiency of existing reading comprehension architectures like FusionNet by reducing computational requirements without sacrificing performance.",train,"BiLSTMs, CoVe layers, DAWNBench, FastFusionNet, FusionNet, FusionNet, maximum retrieval accuracy, reading comprehension architecture, SRU layers"
410,976,3681,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,[21] further showed that use of synthetic training data can work better than multitask training.,Background,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
411,978,3683,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"Following recent speech translation [21] and recognition [28] models, the encoder is composed of a stack of 8 bidirectional LSTM layers.",Similar,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
412,979,3684,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,This proprietary dataset described in [21] was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset.,Similar,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
413,980,3685,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,Input feature frames are created by stacking 3 adjacent frames of an 80-channel log-mel spectrogram as in [21] .,Similar,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
414,981,3686,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"We study two Spanish-to-English translation datasets: the large scale ""conversational"" corpus of parallel text and read speech pairs from [21] , and the Spanish Fisher corpus of telephone conversations and corresponding English translations [38] , which is smaller and more challenging due to the spontaneous and informal speaking style.",Similar,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
415,982,3687,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"Finally, as in [21] , we find that pretraining the bottom 6 encoder layers on an ST task improves BLEU scores by over 5 points.",Similar,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
416,983,3688,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"We study two Spanish-to-English translation datasets: the large scale ""conversational"" corpus of parallel text and read speech pairs from [21] , and the Spanish Fisher corpus of telephone conversations and corresponding English translations [38] , which is smaller and more challenging due to the spontaneous and informal speaking style.",Uses,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,1,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
417,984,3689,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST → TTS cascade model using a speech-to-text translation model [21] trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets.",Similar,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
418,985,3690,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"Following recent speech translation [21] and recognition [28] models, the encoder is composed of a stack of 8 bidirectional LSTM layers.",Uses,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,1,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
419,986,3691,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"Table 2 shows performance of the model trained using different combinations of auxiliary losses, compared to a baseline ST → TTS cascade model using a speech-to-text translation model [21] trained on the same data, and the same Tacotron 2 TTS model used to synthesize training targets.",Uses,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,1,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
420,987,3692,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as [21] .",Difference,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
421,988,3693,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,This proprietary dataset described in [21] was obtained by crowdsourcing humans to read the both sides of a conversational Spanish-English MT dataset.,Uses,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,1,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
422,989,3694,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,e185a6b370c68aae67ea7d734f57bfe13937ef36,"Other future work includes utilizing weakly supervision to scale up training with synthetic data [21] or multitask learning [19, 20] , and transferring prosody and other acoustic factors from the source speech to the translated speech following [45] [46] [47] .",Future Work,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","We propose prosody embeddings for emotional and expressive speech synthesis networks. The proposed methods introduce temporal structures in the embedding networks, thus enabling fine-grained control of the speaking style of the synthesized speech. The temporal structures can be designed either on the speech side or the text side, leading to different control resolutions in time. The prosody embedding networks are plugged into end-to-end speech synthesis networks and trained without any other supervision except for the target speech for synthesizing. It is demonstrated that the prosody embedding networks learned to extract prosodic features. By adjusting the learned prosody features, we could change the pitch and amplitude of the synthesized speech both at the frame level and the phoneme level. We also introduce the temporal normalization of prosody embeddings, which shows better robustness against speaker perturbations during prosody transfer tasks.",test,0,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
423,990,3695,ABC_831342435ca0a4695e2a7f149891e4_20,ARXIV:1904.06037,b6222ad8acdf327368b45fb7fa5f4cf374d6da80,"In this section, instead of using the human target speech, we use a TTS model to synthesize target In addition, we augment the input source speech by adding background noise and reverberation in the same manner as [21] .",Extention,s2,"We present an attention-based sequence-to-sequence neural network which can directly translate speech from one language into speech in another language, without relying on an intermediate text representation. The network is trained end-to-end, learning to map speech spectrograms into target spectrograms in another language, corresponding to the translated content (in a different canonical voice). We further demonstrate the ability to synthesize translated speech using the voice of the source speaker. We conduct experiments on two Spanish-to-English speech translation datasets, and find that the proposed model slightly underperforms a baseline cascade of a direct speech-to-text translation model and a text-to-speech synthesis model, demonstrating the feasibility of the approach on this very challenging task.","End-to-end Speech Translation (ST) models have many potential advantages when compared to the cascade of Automatic Speech Recognition (ASR) and text Machine Translation (MT) models, including lowered inference latency and the avoidance of error compounding. However, the quality of end-to-end ST is often limited by a paucity of training data, since it is difficult to collect large parallel corpora of speech and translated transcript pairs. Previous studies have proposed the use of pre-trained components and multi-task learning in order to benefit from weakly supervised training data, such as speech-to-transcript or text-to-foreign-text pairs. In this paper, we demonstrate that using pre-trained MT or text-to-speech (TTS) synthesis models to convert weakly supervised data into speech-to-translation pairs for ST training can be more effective than multi-task learning. Furthermore, we demonstrate that a high quality end-to-end ST model can be trained using only weakly supervised datasets, and that synthetic data sourced from unlabeled monolingual text or speech can be used to improve performance. Finally, we discuss methods for avoiding overfitting to synthetic speech with a quantitative ablation study.",test,1,"The research problem is developing a direct speech-to-speech translation system that can translate speech from one language to another without relying on an intermediate text representation. The motivation for this research is to explore the feasibility of a direct speech-to-speech translation approach, which is a challenging task, as indicated by the comparison to baseline models.",train,"attention-based sequence-to-sequence neural network, direct speech-to-text translation model, intermediate text representation, Spanish-to-English speech translation datasets, text-to-speech synthesis model, translated speech"
424,1068,3990,ABC_be39cfec0479ace0a7e08508239cb0_22,ACL:P19-1638,cea967b59209c6be22829699f05b8b1ac4dc092d,"We specifically investigate the paragraph embedding method of Zhang et al. (2017) , which consists of a CNN-based encoder-decoder model paired with a reconstruction objective to learn powerful paragraph embeddings that are capable of accurately reconstructing long paragraphs.",Uses,s2,"While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstruction-based objective of Zhang et al. (2017) with our sentence content probe objective in a semi-supervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.","Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",test,1,"The research problem focuses on the lack of transparency in how paragraph embedding models learn and encode information. Specifically, the inability of a state-of-the-art method to reliably determine if a given sentence is present within the input paragraph is identified as a key issue. The motivation stems from the need to improve the understanding and performance of paragraph embedding models. The researchers aim to replace the reconstruction-based objective of Zhang et al. (2017) with a new objective function that explicitly addresses the identified shortcomings, ultimately leading to better performance in downstream tasks.",train,"bag-of-words method, downstream classification tasks, paragraph embedding method, paragraph embedding models, paragraph reconstruction, semi-supervised setting, sentence content task"
425,1069,3991,ABC_be39cfec0479ace0a7e08508239cb0_22,ACL:P19-1638,1da8eabfd02436dda5f3131fe404dc840a04e6cc,"Methods that embed a paragraph into a single vector have been successfully integrated into many NLP applications, including text classification (Zhang et al., 2017) , document retrieval (Le and Mikolov, 2014) , and semantic similarity and relatedness (Dai et al., 2015; Chen, 2017) .",Background,s2,"While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstruction-based objective of Zhang et al. (2017) with our sentence content probe objective in a semi-supervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.","We present an efficient document representation learning framework, Document Vector through Corruption (Doc2VecC). Doc2VecC represents each document as a simple average of word embeddings. It ensures a representation generated as such captures the semantic meanings of the document during learning. A corruption model is included, which introduces a data-dependent regularization that favors informative or rare words while forcing the embeddings of common and non-discriminative ones to be close to zero. Doc2VecC produces significantly better word embeddings than Word2Vec. We compare Doc2VecC with several state-of-the-art document representation learning algorithms. The simple model architecture introduced by Doc2VecC matches or out-performs the state-of-the-art in generating high-quality document representations for sentiment analysis, document classification as well as semantic relatedness tasks. The simplicity of the model enables training on billions of words per hour on a single machine. At the same time, the model is very efficient in generating representations of unseen documents at test time.",test,0,"The research problem focuses on the lack of transparency in how paragraph embedding models learn and encode information. Specifically, the inability of a state-of-the-art method to reliably determine if a given sentence is present within the input paragraph is identified as a key issue. The motivation stems from the need to improve the understanding and performance of paragraph embedding models. The researchers aim to replace the reconstruction-based objective of Zhang et al. (2017) with a new objective function that explicitly addresses the identified shortcomings, ultimately leading to better performance in downstream tasks.",train,"bag-of-words method, downstream classification tasks, paragraph embedding method, paragraph embedding models, paragraph reconstruction, semi-supervised setting, sentence content task"
426,1070,3992,ABC_be39cfec0479ace0a7e08508239cb0_22,ACL:P19-1638,e2587eddd57bc4ba286d91b27c185083f16f40ee,"Text embeddings and probe tasks A variety of methods exist for obtaining fixed-length dense vector representations of words (e.g., Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) , sentences (e.g., Kiros et al., 2015; Conneau et al., 2017; Subramanian et al., 2018; Cer et al., 2018) , and larger bodies of text (e.g., Le and Mikolov, 2014; Dai et al., 2015; Iyyer et al., 2015; Li et al., 2015; Chen, 2017; Zhang et al., 2017) To analyze word and sentence embeddings, recent work has studied classification tasks that probe them for various linguistic properties (Shi et al., 2016; Adi et al., 2017; Belinkov et al., 2017a,b; Conneau et al., 2018; Tenney et al., 2019) .",Background,s2,"While paragraph embedding models are remarkably effective for downstream classification tasks, what they learn and encode into a single vector remains opaque. In this paper, we investigate a state-of-the-art paragraph embedding method proposed by Zhang et al. (2017) and discover that it cannot reliably tell whether a given sentence occurs in the input paragraph or not. We formulate a sentence content task to probe for this basic linguistic property and find that even a much simpler bag-of-words method has no trouble solving it. This result motivates us to replace the reconstruction-based objective of Zhang et al. (2017) with our sentence content probe objective in a semi-supervised setting. Despite its simplicity, our objective improves over paragraph reconstruction in terms of (1) downstream classification accuracies on benchmark datasets, (2) faster training, and (3) better generalization ability.","Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",test,0,"The research problem focuses on the lack of transparency in how paragraph embedding models learn and encode information. Specifically, the inability of a state-of-the-art method to reliably determine if a given sentence is present within the input paragraph is identified as a key issue. The motivation stems from the need to improve the understanding and performance of paragraph embedding models. The researchers aim to replace the reconstruction-based objective of Zhang et al. (2017) with a new objective function that explicitly addresses the identified shortcomings, ultimately leading to better performance in downstream tasks.",train,"bag-of-words method, downstream classification tasks, paragraph embedding method, paragraph embedding models, paragraph reconstruction, semi-supervised setting, sentence content task"
427,1119,4077,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,9be8c1b661e4ffa7e3d061c70c7a771313b39a8f,"We propose a word-frequency based sampling strategy to randomly tokenize rare words into phonemes in the target sequence using a lexicon. This approach also mitigates accuracy regressions that have been observed when using phoneme-only E2E models [16, 17] .",Motivation,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","In conventional speech recognition, phoneme-based models outperform grapheme-based models for non-phonetic languages such as English. The performance gap between the two typically reduces as the amount of training data is increased. In this work, we examine the impact of the choice of modeling unit for attention-based encoder-decoder models. We conduct experiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various target units (phoneme, grapheme, and word-piece); across all tasks, we find that grapheme or word-piece models consistently outperform phoneme-based models, even though they are evaluated without a lexicon or an external language model. We also investigate model complementarity: we find that we can improve WERs by up to 9% relative by rescoring N-best lists generated from a strong word-piece based baseline with either the phoneme or the grapheme model. Rescoring an N-best list generated by the phonemic system, however, provides limited improvements. Further analysis shows that the word-piece-based models produce more diverse N-best hypotheses, and thus lower oracle WERs, than phonemic models.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
428,1120,4078,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"Since phonemes show strength in recognizing rare words [16] , we want to present these words as phonemes more often.",Motivation,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
429,1123,4081,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,9be8c1b661e4ffa7e3d061c70c7a771313b39a8f,"Phoneme-only E2E systems have been shown to have inferior performance compared to grapheme or wordpiece models (WPM) in general [16, 17] , but shows better recognition of rare words and proper nouns.",Background,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","In conventional speech recognition, phoneme-based models outperform grapheme-based models for non-phonetic languages such as English. The performance gap between the two typically reduces as the amount of training data is increased. In this work, we examine the impact of the choice of modeling unit for attention-based encoder-decoder models. We conduct experiments on the LibriSpeech 100hr, 460hr, and 960hr tasks, using various target units (phoneme, grapheme, and word-piece); across all tasks, we find that grapheme or word-piece models consistently outperform phoneme-based models, even though they are evaluated without a lexicon or an external language model. We also investigate model complementarity: we find that we can improve WERs by up to 9% relative by rescoring N-best lists generated from a strong word-piece based baseline with either the phoneme or the grapheme model. Rescoring an N-best list generated by the phonemic system, however, provides limited improvements. Further analysis shows that the word-piece-based models produce more diverse N-best hypotheses, and thus lower oracle WERs, than phonemic models.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
430,1124,4082,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,We use context-independent phonemes as in [16] .,Uses,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
431,1125,4083,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,We use context-independent phonemes as in [16] .,Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
432,1126,4084,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"We attribute the superior per- formance of the wordpiece-phoneme model to the robustness of phonemes to OOV words, as observed in [16] .",Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
433,1127,4085,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"To generate words as outputs, we search through a decoding graph similar to [16] but accept both phonemes and wordpieces.",Difference,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
434,1128,4086,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"However, we note that the regression is significantly smaller than the all-phoneme model in [16] .",Difference,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
435,1129,4087,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"Based on [16] , we add two improvements to the decoding strategy.",Extention,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,1,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
436,1130,4088,ABC_cd10d509dacd8f55993396258eb92a_22,ARXIV:1906.09292,8eff044945bf2543d210adb1e327b8957a219e4d,"To generate words as outputs, we search through a decoding graph similar to [16] but accept both phonemes and wordpieces.",Similar,s2,"Contextual automatic speech recognition, i.e., biasing recognition towards a given context (e.g. user's playlists, or contacts), is challenging in end-to-end (E2E) models. Such models maintain a limited number of candidates during beam-search decoding, and have been found to recognize rare named entities poorly. The problem is exacerbated when biasing towards proper nouns in foreign languages, e.g., geographic location names, which are virtually unseen in training and are thus out-of-vocabulary (OOV). While grapheme or wordpiece E2E models might have a difficult time spelling OOV words, phonemes are more acoustically salient and past work has shown that E2E phoneme models can better predict such words. In this work, we propose an E2E model containing both English wordpieces and phonemes in the modeling space, and perform contextual biasing of foreign words at the phoneme level by mapping pronunciations of foreign words into similar English phonemes. In experimental evaluations, we find that the proposed approach performs 16% better than a grapheme-only biasing model, and 8% better than a wordpiece-only biasing model on a foreign place name recognition task, with only slight degradation on regular English tasks.","For decades, context-dependent phonemes have been the dominant sub-word unit for conventional acoustic modeling systems. This status quo has begun to be challenged recently by end-to-end models which seek to combine acoustic, pronunciation, and language model components into a single neural network. Such systems, which typically predict graphemes or words, simplify the recognition process since they remove the need for a separate expert-curated pronunciation lexicon to map from phoneme-based units to words. However, there has been little previous work comparing phoneme-based versus grapheme-based sub-word units in the end-to-end modeling framework, to determine whether the gains from such approaches are primarily due to the new probabilistic model, or from the joint learning of the various components with grapheme-based units. In this work, we conduct detailed experiments which are aimed at quantifying the value of phoneme-based pronunciation lexica in the context of end-to-end models. We examine phoneme-based end-to-end models, which are contrasted against grapheme-based ones on a large vocabulary English Voice-search task, where we find that graphemes do indeed outperform phonemes. We also compare grapheme and phoneme-based approaches on a multi-dialect English task, which once again confirm the superiority of graphemes, greatly simplifying the system for recognizing multiple dialects.",test,0,"The research focuses on the challenge of contextual automatic speech recognition, particularly the difficulty of biasing recognition towards context in end-to-end (E2E) models. This problem is exacerbated when dealing with proper nouns in foreign languages, which are often out-of-vocabulary (OOV) words and poorly recognized by E2E models due to their limited candidate generation during decoding. The research is motivated by the need to improve the accuracy of contextual automatic speech recognition, particularly in scenarios involving foreign language proper nouns, which are often OOV and poorly recognized by traditional E2E models.",train,"beam-search decoding, Contextual automatic speech recognition, contextual biasing of foreign words, E2E model, E2E phoneme models, end-to-end ( E2E ) models, foreign place name recognition task, grapheme or wordpiece E2E models, grapheme-only biasing model, recognition, regular English tasks, wordpiece-only biasing model"
437,1199,4210,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,694b3c58712deefb59502847ba1b52b192c413e5,"This is particularly pronounced in systems trained on clean, formalized parallel data such as Europarl (Koehn, 2005) , are tasked with translation of unedited, human generated text such as is common in domains such as social media, where accurate translation is becoming of widespread relevance (Michel and Neubig, 2018) .",Background,s2,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",test,0,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
438,1200,4211,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,753e30826f1908a62a8d251fc6b1b598f86d2bb2,"Human generated text on the internet and social media are a particularly rich source of natural noise (Eisenstein, 2013; Baldwin et al., 2015) which causes pronounced problems for MT (Michel and Neubig, 2018) .",Background,s2,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","This paper presents the results of the two shared tasks associated with W-NUT 2015: (1) a text normalization task with 10 participants; and (2) a named entity tagging task with 8 participants. We outline the task, annotation process and dataset statistics, and provide a high-level overview of the participating systems for each shared task.",test,0,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
439,1201,4212,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,c928e453122fa7c0658e02a7aa07a623d4e5b679,"In this work we present two primary methods of synthesizing natural noise in accordance with the types of noise identified in prior work (Eisenstein, 2013; Michel and Neubig, 2018) as naturally occurring in internet and social media based text.",Background,s2,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","The rise of social media has brought computational linguistics in ever-closer contact with bad language: text that defies our expectations about vocabulary, spelling, and syntax. This paper surveys the landscape of bad language, and offers a critical review of the NLP community’s response, which has largely followed two paths: normalization and domain adaptation. Each approach is evaluated in the context of theoretical and empirical work on computer-mediated communication. In addition, the paper presents a quantitative analysis of the lexical diversity of social media text, and its relationship to other corpora.",test,0,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
440,1305,4627,ABC_70d41cad40091bcc30a1fd544c277d_26,CorpusID:118697759,3febb2bed8865945e7fddc99efd791887bb7e14f,"Bidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018) currently represents state of the art, vastly outperforming previous models, such as the Generative Pretrained Transformer (GPT; Radford et al.) and Embeddings from Language Models (ELMo; Peters et al., 2018) .",Background,s2,"We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.","We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",test,0,"The research problem is to effectively apply BERT, a powerful language model, to the task of document classification. This task presents challenges like the importance of semantic content over syntactic structure, potentially longer documents than typical BERT inputs, and the possibility of multiple labels per document. The motivation for this research is to establish improved baselines for document classification using BERT, which can contribute to further advancements in this area and pave the way for future research.",train,"BERT, BERT, BERT, BERT inference, classification model, document classification"
441,1306,4628,ABC_70d41cad40091bcc30a1fd544c277d_26,CorpusID:118697759,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"Bidirectional Encoder Representations from Transformers (BERT; Devlin et al., 2018) represents one of the latest developments in this line of work.",Background,s2,"We present, to our knowledge, the first application of BERT to document classification. A few characteristics of the task might lead one to think that BERT is not the most appropriate model: syntactic structures matter less for content categories, documents can often be longer than typical BERT input, and documents often have multiple labels. Nevertheless, we show that a straightforward classification model using BERT is able to achieve the state of the art across four popular datasets. To address the computational expense associated with BERT inference, we distill knowledge from BERT-large to small bidirectional LSTMs, reaching BERT-base parity on multiple datasets using 30x fewer parameters. The primary contribution of our paper is improved baselines that can provide the foundation for future work.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",test,0,"The research problem is to effectively apply BERT, a powerful language model, to the task of document classification. This task presents challenges like the importance of semantic content over syntactic structure, potentially longer documents than typical BERT inputs, and the possibility of multiple labels per document. The motivation for this research is to establish improved baselines for document classification using BERT, which can contribute to further advancements in this area and pave the way for future research.",train,"BERT, BERT, BERT, BERT inference, classification model, document classification"
442,1307,4629,ABC_c067711a58722737ef8b7ea987bcf3_26,ACL:W19-0504,7b28f39323169abe60a00f4f6830a9b63d9426b9,"We now compare our best models to previous parsers 4 (Bos, 2015; Van Noord, Abzianidze, Toral, and Bos, 2018) and two baseline systems, SPAR and SIM-SPAR.",Uses,s2,"Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.","Boxer is a semantic parser for English texts with many input and output possibilities, and various ways to perform meaning analysis based on Discourse Representation Theory. This involves the various ways that meaning representations can be computed, as well as their possible semantic ingredients.",test,1,"While sequence-to-sequence models perform well in semantic parsing, they often underutilize available linguistic resources. This study aims to improve performance by incorporating these resources and exploring their impact on Discourse Representation Structure Parsing. The study is motivated by the potential of linguistic resources in improving neural semantic parsing, drawing inspiration from successful applications in neural machine translation. The researchers aim to demonstrate the benefits of using these resources, particularly within a multi-encoder framework.",train,"Discourse Representation, encoders, linguistic resources, multi-encoder setup, neural machine translation, neural semantic parsing, semantic parsing tasks, sequence-to-sequence models"
443,1308,4630,ABC_c067711a58722737ef8b7ea987bcf3_26,ACL:W19-0504,aab5002a22b9b4244a8329b140bd0a86021aa2d1,"As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features.",Uses,s2,"Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.","We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.",test,1,"While sequence-to-sequence models perform well in semantic parsing, they often underutilize available linguistic resources. This study aims to improve performance by incorporating these resources and exploring their impact on Discourse Representation Structure Parsing. The study is motivated by the potential of linguistic resources in improving neural semantic parsing, drawing inspiration from successful applications in neural machine translation. The researchers aim to demonstrate the benefits of using these resources, particularly within a multi-encoder framework.",train,"Discourse Representation, encoders, linguistic resources, multi-encoder setup, neural machine translation, neural semantic parsing, semantic parsing tasks, sequence-to-sequence models"
444,1309,4631,ABC_c067711a58722737ef8b7ea987bcf3_26,ACL:W19-0504,aab5002a22b9b4244a8329b140bd0a86021aa2d1,"As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features.",Difference,s2,"Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.","We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.",test,0,"While sequence-to-sequence models perform well in semantic parsing, they often underutilize available linguistic resources. This study aims to improve performance by incorporating these resources and exploring their impact on Discourse Representation Structure Parsing. The study is motivated by the potential of linguistic resources in improving neural semantic parsing, drawing inspiration from successful applications in neural machine translation. The researchers aim to demonstrate the benefits of using these resources, particularly within a multi-encoder framework.",train,"Discourse Representation, encoders, linguistic resources, multi-encoder setup, neural machine translation, neural semantic parsing, semantic parsing tasks, sequence-to-sequence models"
445,1310,4632,ABC_c067711a58722737ef8b7ea987bcf3_26,ACL:W19-0504,fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,"We employ a recurrent sequence-to-sequence neural network with attention (Bahdanau et al., 2014) and two bi-LSTM layers, similar to the one used by Van Noord, Abzianidze, Toral, and Bos (2018) .",Uses,s2,"Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",test,1,"While sequence-to-sequence models perform well in semantic parsing, they often underutilize available linguistic resources. This study aims to improve performance by incorporating these resources and exploring their impact on Discourse Representation Structure Parsing. The study is motivated by the potential of linguistic resources in improving neural semantic parsing, drawing inspiration from successful applications in neural machine translation. The researchers aim to demonstrate the benefits of using these resources, particularly within a multi-encoder framework.",train,"Discourse Representation, encoders, linguistic resources, multi-encoder setup, neural machine translation, neural semantic parsing, semantic parsing tasks, sequence-to-sequence models"
446,1311,4633,ABC_c067711a58722737ef8b7ea987bcf3_26,ACL:W19-0504,aab5002a22b9b4244a8329b140bd0a86021aa2d1,"As previously indicated, Van Noord, Abzianidze, Toral, and Bos (2018) used a similar sequence-to-sequence model as our current approach, but implemented in OpenNMT and without the linguistic features.",Similar,s2,"Recently, sequence-to-sequence models have achieved impressive performance on a number of semantic parsing tasks. However, they often do not exploit available linguistic resources, while these, when employed correctly, are likely to increase performance even further. Research in neural machine translation has shown that employing this information has a lot of potential, especially when using a multi-encoder setup. We employ a range of semantic and syntactic resources to improve performance for the task of Discourse Representation Structure Parsing. We show that (i) linguistic features can be beneficial for neural semantic parsing and (ii) the best method of adding these features is by using multiple encoders.","We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.",test,0,"While sequence-to-sequence models perform well in semantic parsing, they often underutilize available linguistic resources. This study aims to improve performance by incorporating these resources and exploring their impact on Discourse Representation Structure Parsing. The study is motivated by the potential of linguistic resources in improving neural semantic parsing, drawing inspiration from successful applications in neural machine translation. The researchers aim to demonstrate the benefits of using these resources, particularly within a multi-encoder framework.",train,"Discourse Representation, encoders, linguistic resources, multi-encoder setup, neural machine translation, neural semantic parsing, semantic parsing tasks, sequence-to-sequence models"
447,1332,4688,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,3a7bbc46795929f0eace82b64c44c92a48682fb5,The default setup of Akbik et al. (2018) recommends contextual string embeddings to be used in combination with standard word embeddings.,Background,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",test,0,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
448,1333,4689,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,421fc2556836a6b441de806d7b393a35b6eaea58,"Recent work has moved away from the original ""one word, one embedding"" paradigm to investigate contextualized embedding models (Peters et al., 2017 (Peters et al., , 2018 Akbik et al., 2018) .",Background,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",test,0,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
449,1334,4690,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,421fc2556836a6b441de806d7b393a35b6eaea58,"Recently, Akbik et al. (2018) proposed a character-level contextualized embeddings ap- context.",Background,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",test,0,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
450,1335,4691,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,3a7bbc46795929f0eace82b64c44c92a48682fb5,"Our proposed approach dynamically builds up a ""memory"" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a sentence context (see Akbik et al. (2018) ).",Uses,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",test,1,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
451,1336,4692,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,3a7bbc46795929f0eace82b64c44c92a48682fb5,"Our proposed approach (see Figure 2 ) dynamically builds up a ""memory"" of contextualized embeddings and applies a pooling operation to distill a global contextualized embedding for each word. It requires an embed() function that produces a contextualized embedding for a given word in a 1 https://github.com/zalandoresearch/flair sentence context (see Akbik et al. (2018) ).",Uses,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",test,1,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
452,1337,4693,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,3a7bbc46795929f0eace82b64c44c92a48682fb5,"For our experiments, we follow the training and evaluation procedure outlined in Akbik et al. (2018) and follow most hyperparameter suggestions as given by the in-depth study presented in Reimers and Gurevych (2017) .",Uses,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",test,1,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
453,1338,4694,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,3a7bbc46795929f0eace82b64c44c92a48682fb5,"The default setup of Akbik et al. (2018) recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks.",Uses,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",test,1,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
454,1339,4697,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,3a7bbc46795929f0eace82b64c44c92a48682fb5,"Our baseline are contextual string embeddings without pooling, i.e. the original setup proposed in Akbik et al. (2018) 2 .",Uses,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",test,1,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
455,1340,4699,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,3a7bbc46795929f0eace82b64c44c92a48682fb5,"The default setup of Akbik et al. (2018) recommends contextual string embeddings to be used in combination with standard word embeddings. We use GLOVE embeddings (Pennington et al., 2014) for the English tasks and FASTTEXT embeddings (Bojanowski et al., 2017) for all newswire tasks.",Similar,s2,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","We present FLAIR, an NLP framework designed to facilitate training and distribution of state-of-the-art sequence labeling, text classification and language models. The core idea of the framework is to present a simple, unified interface for conceptually very different types of word and document embeddings. This effectively hides all embedding-specific engineering complexity and allows researchers to “mix and match” various embeddings with little effort. The framework also implements standard model training and hyperparameter selection routines, as well as a data fetching module that can download publicly available NLP datasets and convert them into data structures for quick set up of experiments. Finally, FLAIR also ships with a “model zoo” of pre-trained models to allow researchers to use state-of-the-art NLP models in their applications. This paper gives an overview of the framework and its functionality. The framework is available on GitHub at https://github.com/zalandoresearch/flair .",test,0,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
456,1557,5473,ABC_0fd87fbdbe64e7d002ca31783448fb_32,ARXIV:1905.03638,469d249a40639d4ffb62abfb2c25f5aab0812fa4,"Many researches have been conducted to involve AI into poem generation [Zhang and Lapata, 2014; Cheng et al., 2018] , creation of classical or pop music [Manzelli et al., 2018; Hadjeres et al., 2017] and automatic images generation [van den Oord et al., 2016; Yan et al., 2016; Xu et al., 2018] .",Background,s2,"We present a novel real-time, collaborative, and interactive AI painting system, Mappa Mundi, for artistic Mind Map creation. The system consists  of a voice-based input interface, an automatic topic expansion module, and an image projection module. The key innovation is to inject Artificial Imagination into painting creation by considering lexical and phonological similarities of language, learning and inheriting
artist’s original painting style, and applying the principles of Dadaism and impossibility of improvisation. Our system indicates that AI and artist can collaborate seamlessly to create imaginative artistic painting and Mappa Mundi has been applied in art exhibition in UCCA, Beijing.    ","Vision is a common source of inspiration for poetry. The objects and the sentimental imprints that one perceives from an image may lead to various feelings depending on the reader. In this paper, we present a system of poetry generation from images to mimic the process. Given an image, we first extract a few keywords representing objects and sentiments perceived from the image. These keywords are then expanded to related ones based on their associations in human written poems. Finally, verses are generated gradually from the keywords using recurrent neural networks trained on existing poems. Our approach is evaluated by human assessors and compared to other generation baselines. The results show that our method can generate poems that are more artistic than the baseline methods. This is one of the few attempts to generate poetry from images. By deploying our proposed approach, XiaoIce has already generated more than 12 million poems for users since its release in July 2017. A book of its poems has been published by Cheers Publishing, which claimed that the book is the first-ever poetry collection written by an AI in human history.",test,0,"The research problem is to develop a real-time, collaborative, and interactive AI painting system for artistic Mind Map creation that fosters imagination and artistic expression. The research is driven by the desire to explore the potential of AI in artistic creation, specifically in the realm of Mind Map painting. The goal is to develop a system that allows for collaborative and imaginative artwork generation through the integration of AI and human creativity.",train,"art exhibition, artistic Mind Map creation, automatic topic expansion module, image projection module, imaginative artistic painting, Mappa Mundi, painting creation"
457,1582,5585,ABC_cb57b8886be9ea4f0c50fd2c3a178a_33,ARXIV:1904.07094,2ba7fd1f72e1219ac1df3d78fc32de9ba5568b17,"In summary, our contributions are as follows: -We are the first to demonstrate that contextualized word representations can be successfully incorporated into existing neural architectures (PACRR [6] , KNRM [20] , and DRMM [5] ), allowing them to leverage contextual information to improve ad-hoc document ranking.",Unsure,s2,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several ex-sting neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.","Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text(local context) as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACER, a novel context-aware neural IR model. Extensive comparisons with established models on TREC Web Track data confirm that the proposed model can achieve superior search results. In addition, an ablation analysis is conducted to gain insights into the impact of and interactions between different components. We release our code to enable future comparisons.",test,0,The research problem is the lack of attention paid to term representations in neural ranking architectures. The study aims to investigate how pretrained contextualized language models (ELMo and BERT) can be utilized to improve ad-hoc document ranking performance. The research is motivated by the desire to improve ad-hoc document ranking performance by leveraging the benefits of contextualized language models. The goal is to develop a more effective approach that outperforms existing state-of-the-art methods while addressing practical concerns.,train,"ad-hoc document ranking, ad-hoc ranking baselines, contextualized language models, contextualized language models, Document Ranking, ex-sting neural ranking architectures, neural models, neural ranking architectures, ranking, TREC benchmarks"
458,1583,5586,ABC_cb57b8886be9ea4f0c50fd2c3a178a_33,ARXIV:1904.07094,2ba7fd1f72e1219ac1df3d78fc32de9ba5568b17,"5 Following prior work [6] , documents are truncated to 800 tokens.",Uses,s2,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several ex-sting neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.","Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text(local context) as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACER, a novel context-aware neural IR model. Extensive comparisons with established models on TREC Web Track data confirm that the proposed model can achieve superior search results. In addition, an ablation analysis is conducted to gain insights into the impact of and interactions between different components. We release our code to enable future comparisons.",test,1,The research problem is the lack of attention paid to term representations in neural ranking architectures. The study aims to investigate how pretrained contextualized language models (ELMo and BERT) can be utilized to improve ad-hoc document ranking performance. The research is motivated by the desire to improve ad-hoc document ranking performance by leveraging the benefits of contextualized language models. The goal is to develop a more effective approach that outperforms existing state-of-the-art methods while addressing practical concerns.,train,"ad-hoc document ranking, ad-hoc ranking baselines, contextualized language models, contextualized language models, Document Ranking, ex-sting neural ranking architectures, neural models, neural ranking architectures, ranking, TREC benchmarks"
459,1584,5587,ABC_cb57b8886be9ea4f0c50fd2c3a178a_33,ARXIV:1904.07094,2ba7fd1f72e1219ac1df3d78fc32de9ba5568b17,"We evaluate our methods on three neural relevance matching methods: PACRR [6] , KNRM [20] , and DRMM [5] .",Uses,s2,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several ex-sting neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.","Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text(local context) as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACER, a novel context-aware neural IR model. Extensive comparisons with established models on TREC Web Track data confirm that the proposed model can achieve superior search results. In addition, an ablation analysis is conducted to gain insights into the impact of and interactions between different components. We release our code to enable future comparisons.",test,1,The research problem is the lack of attention paid to term representations in neural ranking architectures. The study aims to investigate how pretrained contextualized language models (ELMo and BERT) can be utilized to improve ad-hoc document ranking performance. The research is motivated by the desire to improve ad-hoc document ranking performance by leveraging the benefits of contextualized language models. The goal is to develop a more effective approach that outperforms existing state-of-the-art methods while addressing practical concerns.,train,"ad-hoc document ranking, ad-hoc ranking baselines, contextualized language models, contextualized language models, Document Ranking, ex-sting neural ranking architectures, neural models, neural ranking architectures, ranking, TREC benchmarks"
460,1585,5589,ABC_cb57b8886be9ea4f0c50fd2c3a178a_33,ARXIV:1904.07094,2ba7fd1f72e1219ac1df3d78fc32de9ba5568b17,"Recently, there has been much work designing ranking architectures to effectively score query-document pairs, with encouraging results [5, 6, 20] .",Background,s2,"Although considerable attention has been given to neural ranking architectures recently, far less attention has been paid to the term representations that are used as input to these models. In this work, we investigate how two pretrained contextualized language models (ELMo and BERT) can be utilized for ad-hoc document ranking. Through experiments on TREC benchmarks, we find that several ex-sting neural ranking architectures can benefit from the additional context provided by contextualized language models. Furthermore, we propose a joint approach that incorporates BERT's classification vector into existing neural models and show that it outperforms state-of-the-art ad-hoc ranking baselines. We call this joint approach CEDR (Contextualized Embeddings for Document Ranking). We also address practical challenges in using these models for ranking, including the maximum input length imposed by BERT and runtime performance impacts of contextualized language models.","Neural IR models, such as DRMM and PACRR, have achieved strong results by successfully capturing relevance matching signals. We argue that the context of these matching signals is also important. Intuitively, when extracting, modeling, and combining matching signals, one would like to consider the surrounding text(local context) as well as other signals from the same document that can contribute to the overall relevance score. In this work, we highlight three potential shortcomings caused by not considering context information and propose three neural ingredients to address them: a disambiguation component, cascade k-max pooling, and a shuffling combination layer. Incorporating these components into the PACRR model yields Co-PACER, a novel context-aware neural IR model. Extensive comparisons with established models on TREC Web Track data confirm that the proposed model can achieve superior search results. In addition, an ablation analysis is conducted to gain insights into the impact of and interactions between different components. We release our code to enable future comparisons.",test,0,The research problem is the lack of attention paid to term representations in neural ranking architectures. The study aims to investigate how pretrained contextualized language models (ELMo and BERT) can be utilized to improve ad-hoc document ranking performance. The research is motivated by the desire to improve ad-hoc document ranking performance by leveraging the benefits of contextualized language models. The goal is to develop a more effective approach that outperforms existing state-of-the-art methods while addressing practical concerns.,train,"ad-hoc document ranking, ad-hoc ranking baselines, contextualized language models, contextualized language models, Document Ranking, ex-sting neural ranking architectures, neural models, neural ranking architectures, ranking, TREC benchmarks"
461,1628,5747,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"Several works even directly transfer semantic roles into opinion roles for ORL (Kim and Hovy, 2006; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates.",Background,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",test,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
462,1629,5748,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles (Kim and Hovy, 2006; Ruppenhofer et al., 2008) . The heuristic approach is unable to obtain high performance for ORL because there are large mismatches between SRL and ORL. We can exploit machine learning based method to solve the mismatching problem between ORL and SRL.",Motivation,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",test,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
463,1631,5750,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"Earlier work attempts to exploit a well-trained SRL model to recognize possible semantic roles for a given opinion expression, and then map the semantic roles into opinion roles (Kim and Hovy, 2006; Ruppenhofer et al., 2008) .",Background,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",test,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
464,1632,5751,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,2557adc02e1c07d0af0f6a93eb5547a1cce08eca,"Results show that SRL is highly effective for ORL, which is consistent with previous findings (Kim and Hovy, 2006; Ruppenhofer et al., 2008; Marasović and Frank, 2018) .",Uses,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question “Who expressed what kind of sentiment towards what?”. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.",test,1,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
465,1633,5752,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,2557adc02e1c07d0af0f6a93eb5547a1cce08eca,"Results show that SRL is highly effective for ORL, which is consistent with previous findings (Kim and Hovy, 2006; Ruppenhofer et al., 2008; Marasović and Frank, 2018) .",Similar,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question “Who expressed what kind of sentiment towards what?”. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.",test,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
466,1634,5753,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,2557adc02e1c07d0af0f6a93eb5547a1cce08eca,"The results show that SRL information is very helpful for ORL, which is consistent with previous studies (Kim and Hovy, 2006; Ruppenhofer et al., 2008; Marasović and Frank, 2018) .",Similar,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question “Who expressed what kind of sentiment towards what?”. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.",test,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
467,1635,5754,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"According to the above findings, we design a simple system by mapping SRL outputs into ORL directly (Kim and Hovy, 2006; Ruppenhofer et al., 2008) .",Uses,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",test,1,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
468,1636,5756,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,2557adc02e1c07d0af0f6a93eb5547a1cce08eca,"Results show that SRL is highly effective for ORL, which is consistent with previous findings (Kim and Hovy, 2006; Ruppenhofer et al., 2008; Marasović and Frank, 2018) . Meanwhile, our implicit SRL-SAWR method can achieve the best ORL performance, 2.23% higher F-scores than the second best method.",Difference,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","For over a decade, machine learning has been used to extract opinion-holder-target structures from text to answer the question “Who expressed what kind of sentiment towards what?”. Recent neural approaches do not outperform the state-of-the-art feature-based models for Opinion Role Labeling (ORL). We suspect this is due to the scarcity of labeled training data and address this issue using different multi-task learning (MTL) techniques with a related task which has substantially more data, i.e. Semantic Role Labeling (SRL). We show that two MTL models improve significantly over the single-task model for labeling of both holders and targets, on the development and the test sets. We found that the vanilla MTL model, which makes predictions using only shared ORL and SRL features, performs the best. With deeper analysis we determine what works and what might be done to make further improvements for ORL.",test,0,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
469,1637,5758,ABC_5cf0215cd20c86f329c8debc0daeb8_34,ACL:N19-1066,3f87718c0e50ea09c75b943f949f900b81b704c9,"Several works even directly transfer semantic roles into opinion roles for ORL (Kim and Hovy, 2006; Ruppenhofer et al., 2008) , treating opinion expressions as the major predicates. These systems can achieve good performances, indicating that SRL information can be greatly useful for ORL. Here we propose a novel method to encode the SRL information implicitly, enhancing ORL model with semantic-aware word representations from a neural SRL model (SRL-SAWR).",Extention,s2,"Opinion role labeling (ORL) is an important task for fine-grained opinion mining, which identifies important opinion arguments such as holder and target for a given opinion trigger. The task is highly correlative with semantic role labeling (SRL), which identifies important semantic arguments such as agent and patient for a given predicate. As predicate agents and patients usually correspond to opinion holders and targets respectively, SRL could be valuable for ORL. In this work, we propose a simple and novel method to enhance ORL by utilizing SRL, presenting semantic-aware word representations which are learned from SRL. The representations are then fed into a baseline neural ORL model as basic inputs. We verify the proposed method on a benchmark MPQA corpus. Experimental results show that the proposed method is highly effective. In addition, we compare the method with two representative methods of SRL integration as well, finding that our method can outperform the two methods significantly, achieving 1.47% higher F-scores than the better one.","As many popular text genres such as blogs or news contain opinions by multiple sources and about multiple targets, finding the sources and targets of subjective expressions becomes an important sub-task for automatic opinion analysis systems. We argue that while automatic semantic role labeling systems (ASRL) have an important contribution to make, they cannot solve the problem for all cases. Based on the experience of manually annotating opinions, sources, and targets in various genres, we present linguistic phenomena that require knowledge beyond that of ASRL systems. In particular, we address issues relating to the attribution of opinions to sources; sources and targets that are realized as zero-forms; and inferred opinions. We also discuss in some depth that for arguing attitudes we need to be able to recover propositions and not only argued-about entities. A recurrent theme of the discussion is that close attention to specific discourse contexts is needed to identify sources and targets correctly.",test,1,"The research problem is improving the accuracy of Opinion Role Labeling (ORL), a task crucial for fine-grained opinion mining, by leveraging the relationship between ORL and Semantic Role Labeling (SRL). The motivation behind this research stems from the close correlation between ORL and SRL, suggesting that SRL could be beneficial for improving ORL. The aim is to develop a more effective method for ORL by integrating SRL, as shown by the significant improvement in F-scores compared to existing methods.",train,"baseline neural ORL model, benchmark MPQA corpus, fine-grained opinion mining, Opinion role labeling ( ORL ), ORL, ORL, semantic role labeling ( SRL ), semantic-aware word representations, SRL, SRL, The representations, The task"
470,1931,6779,ABC_400bd47879aaed0aa1195bafe54e76_48,ARXIV:1906.09912,0fe73c19513dfd17372d8ef58da0d0149725832c,"Consequently, it is unknown if Indonesian word embeddings introduced in, e.g., (Al-Rfou et al., 2013) and (Grave et al., 2018) , capture syntactic or semantic information as measured by analogy tasks.",Background,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.",test,0,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
471,1932,6780,ABC_400bd47879aaed0aa1195bafe54e76_48,ARXIV:1906.09912,0fe73c19513dfd17372d8ef58da0d0149725832c,"We used fastText pretrained embeddings introduced in (Bojanowski et al., 2017 ) and (Grave et al., 2018) , which have been trained on Indonesian Wikipedia and Indonesian Wikipedia plus Common Crawl data respectively.",Uses,s2,"We introduced KaWAT (Kata Word Analogy Task), a new word analogy task dataset for Indonesian. We evaluated on it several existing pretrained Indonesian word embeddings and embeddings trained on Indonesian online news corpus. We also tested them on two downstream tasks and found that pretrained word embeddings helped either by reducing the training epochs or yielding significant performance gains.","Distributed word representations, or word vectors, have recently been applied to many tasks in natural language processing, leading to state-of-the-art performance. A key ingredient to the successful application of these representations is to train them on very large corpora, and use these pre-trained models in downstream tasks. In this paper, we describe how we trained such high quality word representations for 157 languages. We used two sources of data to train these models: the free online encyclopedia Wikipedia and data from the common crawl project. We also introduce three new word analogy datasets to evaluate these word vectors, for French, Hindi and Polish. Finally, we evaluate our pre-trained word vectors on 10 languages for which evaluation datasets exists, showing very strong performance compared to previous models.",test,1,"The research problem is the lack of suitable datasets for word analogy in Indonesian language processing, which hinders the development and evaluation of effective word embeddings for the language. The research is motivated by the need to assess the effectiveness of pretrained word embeddings in Indonesian language processing and to provide a benchmark dataset for future research in this area.",train,"Indonesian, Indonesian online news corpus, pretrained Indonesian word embeddings, pretrained word embeddings, word analogy task dataset"
472,2148,7554,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Specifically, Ashish et.al [34] compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively.",Background,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
473,2149,7555,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,c4744a7c2bb298e4a52289a1e085c71cc3d37bc6,"The second challenge is that the attention model after compressing can not be directly integrated into the encoder and decoder framework of Transformer [34, 7] .",Background,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
474,2150,7556,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,3febb2bed8865945e7fddc99efd791887bb7e14f,"Transformer [34] is based solely on the attention mechanism, and dispensing with recurrent and convolutions entirely.",Background,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
475,2151,7557,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,Previous work [34] gets the multi-head attention by multiple groups of linear mappings.,Background,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
476,2152,7558,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"In practice, Transformer [34] processes query, keys and values as matrices Q, K, and V respectively.",Background,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
477,2153,7559,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"where d is the number of columns of Q and K. In these work [34, 12, 7] , they all use the multi-head attention, as introduced in [34] , where matrices W Q i and In this work [34] , multiple groups of parameters (W Q i , W K i and W V i ) are used, which results in a large number of redundant parameters.",Background,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
478,2154,7560,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Specifically, Ashish et.al [34] compute the attention function on a set of queries simultaneously, packed together into a matrix Q, while the keys and values are also packed together into matrices K and V , respectively. The attention function then adopts a no-linear function sof tmax over three matrices Q, K and V . There are two challenges to find a high-quality compression method to compress the multi-head attention in Transformer.",Motivation,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
479,2155,7561,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"In order to address this challenge, we first prove that the output of the attention function of the self-attention model [34] can be linearly represented by a group of orthonormal base vectors.",Uses,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,1,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
480,2156,7562,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"After that, it can be integrated into the encoder and decoder framework of Transformer [34, 7] and trained end-to-end.",Uses,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,1,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
481,2157,7563,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,c4744a7c2bb298e4a52289a1e085c71cc3d37bc6,"Different from the architectures of convolutional neural network (CNNs) and recurrent neural networks (RNNs) language modeling, the Transformer [34] and its variants [7, 12, 10] achieve excellent results in language modeling processing.",Background,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
482,2158,7564,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Previous work [34] gets the multi-head attention by multiple groups of linear mappings. We use three linear ma for matrices Q, K and V , respectively.",Similar,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
483,2159,7566,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,e3ce71a26872c7755e6d8b8fc45bf00c8be64193,"In this task, we have trained the Transformer model [34] on WMT 2016 English-German dataset [30] .",Uses,s2,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","We participated in the WMT 2016 shared news translation task by building neural translation systems for four language pairs, each trained in both directions: English Czech, English German, English Romanian and English Russian. Our systems are based on an attentional encoder-decoder, using BPE subword segmentation for open-vocabulary translation with a fixed vocabulary. We experimented with using automatic back-translations of the monolingual News corpus as additional training data, pervasive dropout, and target-bidirectional models. All reported methods give substantial improvements, and we see improvements of 4.3--11.2 BLEU over our baseline systems. In the human evaluation, our systems were the (tied) best constrained system for 7 out of 8 translation directions in which we participated.",test,1,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
484,2794,9551,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"In our previous work (Papegnies et al., 2019) , we proposed a radically different method that completely ignores the textual content of the messages, and relies only on a graph-based modeling of the conversation.",Background,s2,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,0,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
485,2795,9552,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"The Graph-Based TF are discussed in depth in our previous article (Papegnies et al., 2019) .",Background,s2,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,0,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
486,2796,9553,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based (Papegnies et al., 2019 ) methods that we previously developed. We propose three different ways to combine them, and compare their performance on a corpus of chat logs originating from the community of a French multiplayer online game.",Extention,s2,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,1,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
487,2797,9554,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"For this purpose, we take advantage of the content- (Papegnies et al., 2017b) and graph-based (Papegnies et al., 2019 ) methods that we previously developed.",Uses,s2,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,1,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
488,2798,9555,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"We take advantage of the methods that we previously developed to leverage message content (Papegnies et al., 2017a) and interactions between users (Papegnies et al., 2019) , and create a new method using both types of information simultaneously.",Extention,s2,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,1,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
489,2799,9556,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"**GRAPH-BASED METHOD** This method corresponds to the top-left part of Figure 1 (in red). It completely ignores the content of the messages, and only focuses on the dynamics of the conversation, based on the interactions between its participants (Papegnies et al., 2019) .",Motivation,s2,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,0,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
490,2800,9557,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"Table 1 presents the Precision, Recall and F -measure scores obtained on the Abuse class, for both baselines (Content-based (Papegnies et al., 2017b) and Graph-based (Papegnies et al., 2019) ) and all three proposed fusion strategies (Early Fusion, Late Fusion and Hybrid Fusion).",Uses,s2,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,1,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
491,3062,10493,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"In this paper we leverage the Machine Translation of Noisy Text (MTNT) dataset<span style=""background: yellow; display: inline-block""> (Michel and Neubig, 2018)</span> to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data.",Uses,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,1,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
492,3063,10494,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"We present a series of experiments based on the Machine Translation of Noisy Text (MTNT) data set <span style=""background: yellow; display: inline-block"">(Michel and Neubig, 2018</span> ) through which we demonstrate improved resilience of a vanilla MT system by adaptation using artificially noised data.",Uses,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,1,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
493,3064,10495,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"Other model parameters reflect the implementation outlined in<span style=""background: yellow; display: inline-block""> Michel and Neubig (2018)</span> .",Similar,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,0,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
494,3065,10496,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"For this method, we inject artificial noise in the clean data according to the distribution of types of noise in MTNT specified in<span style=""background: yellow; display: inline-block""> Michel and Neubig (2018)</span> .",Uses,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,1,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
495,3066,10497,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"In this paper we leverage the Machine Translation of Noisy Text (MTNT) dataset<span style=""background: yellow; display: inline-block""> (Michel and Neubig, 2018)</span> to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data.",Similar,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,0,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
496,3067,10498,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"For this method, we inject artificial noise in the clean data according to the distribution of types of noise in MTNT specified in<span style=""background: yellow; display: inline-block""> Michel and Neubig (2018)</span> .",Similar,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,0,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
497,3068,10499,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"For expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in<span style=""background: yellow; display: inline-block""> Michel and Neubig (2018)</span> , which allows us to provide comparative results across a variety of settings.",Extention,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,1,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
498,3069,10500,ABC_dd603c79f87e98d23f6f8e13028ae9_23,ARXIV:1902.09508,ce89ee7aaeeea2c9d474707690f3ea9d948776a3,"For expediency and convenience of experimentation we have chosen to deploy a smaller, faster variant of the model used in<span style=""background: yellow; display: inline-block""> Michel and Neubig (2018)</span> , which allows us to provide comparative results across a variety of settings.",Difference,llm,"Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.","Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",test,0,"The research problem is the significant drop in accuracy of Modern Machine Translation (MT) systems when encountering noisy text, particularly prevalent in social media. This noise includes typos, slang, dialect, and idiolect, hindering the effectiveness of current MT systems. The research is motivated by the need to address the limitations of current MT systems in handling real-world text, particularly the prevalent noise found in social media. The aim is to improve the resilience of MT systems to this noise, enhancing their accuracy and usability in practical applications.",train,"clean , in-domain text, clean data, human generated text, MT systems, social media, Translation ( MT ) systems, vanilla MT system"
499,3089,10562,ABC_013f0e54384a8a4662a746eb4c30d9_26,ACL:N19-1078,421fc2556836a6b441de806d7b393a35b6eaea58,"Recently, <span style=""background: yellow; display: inline-block"">Akbik et al. (2018)</span> proposed a character-level contextualized embeddings ap- context. This leads to an underspecified contextual word embedding for the string ""Indra"" that ultimately causes a misclassification of ""Indra"" as an organization (ORG) instead of person (PER) in a downstream NER task.",Motivation,llm,"Contextual string embeddings are a recent type of contextualized word embedding that were shown to yield state-of-the-art results when utilized in a range of sequence labeling tasks. They are based on character-level language models which treat text as distributions over characters and are capable of generating embeddings for any string of characters within any textual context. However, such purely character-based approaches struggle to produce meaningful embeddings if a rare string is used in a underspecified context. To address this drawback, we propose a method in which we dynamically aggregate contextualized embeddings of each unique string that we encounter. We then use a pooling operation to distill a ”global” word representation from all contextualized instances. We evaluate these ”pooled contextualized embeddings” on common named entity recognition (NER) tasks such as CoNLL-03 and WNUT and show that our approach significantly improves the state-of-the-art for NER. We make all code and pre-trained models available to the research community for use and reproduction.","Recent advances in language modeling using recurrent neural networks have made it viable to model language as distributions over characters. By learning to predict the next character on the basis of previous characters, such models have been shown to automatically internalize linguistic concepts such as words, sentences, subclauses and even sentiment. In this paper, we propose to leverage the internal states of a trained character language model to produce a novel type of word embedding which we refer to as contextual string embeddings. Our proposed embeddings have the distinct properties that they (a) are trained without any explicit notion of words and thus fundamentally model words as sequences of characters, and (b) are contextualized by their surrounding text, meaning that the same word will have different embeddings depending on its contextual use. We conduct a comparative evaluation against previous embeddings and find that our embeddings are highly useful for downstream tasks: across four classic sequence labeling tasks we consistently outperform the previous state-of-the-art. In particular, we significantly outperform previous work on English and German named entity recognition (NER), allowing us to report new state-of-the-art F1-scores on the CoNLL03 shared task. We release all code and pre-trained language models in a simple-to-use framework to the research community, to enable reproduction of these experiments and application of our proposed embeddings to other tasks: https://github.com/zalandoresearch/flair",test,0,"The research problem is the inability of purely character-based contextual string embeddings to generate meaningful representations for rare strings in underspecified contexts. The research is motivated by the desire to overcome the limitations of existing character-based approaches and improve the performance of contextual string embeddings in sequence labeling tasks, especially in named entity recognition. By making the code and pre-trained models publicly available, the research aims to contribute to the broader research community and facilitate further advancement in the field.",train,"character-based approaches, character-level language models, Contextual string embeddings, contextualized word embedding, global ” word representation, named entity recognition ( NER ) tasks, pooled contextualized embeddings, pooling operation, sequence labeling tasks, WNUT"
500,3215,10900,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,510e26733aaff585d65701b9f1be7ca9d5afc586,"h is the number of heads in<span style=""background: yellow; display: inline-block""> [34]</span> , and d is the dimension of factor matrices.",Background,llm,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
501,3216,10901,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,510e26733aaff585d65701b9f1be7ca9d5afc586,"Vaswani et al.<span style=""background: yellow; display: inline-block""> [34]</span> uses a segment-level recurrence mechanism and a novel positional encoding scheme to resolve this question.",Background,llm,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
502,3217,10902,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,510e26733aaff585d65701b9f1be7ca9d5afc586,"Then, we describe in Section 2.2 multi-head attention<span style=""background: yellow; display: inline-block""> [34]</span> .",Uses,llm,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",test,1,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
503,3218,10903,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,510e26733aaff585d65701b9f1be7ca9d5afc586,"In this section, we only compared the results with Transformer<span style=""background: yellow; display: inline-block""> [34]</span> .",Uses,llm,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",test,1,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
504,3219,10904,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,510e26733aaff585d65701b9f1be7ca9d5afc586,"In our experiments, R is set as the number between 10 and 18 which is smaller than N . The minimum number of sequential operations in Multi-linear attention for different layers is lower than that of the self-attention in Transformer<span style=""background: yellow; display: inline-block""> [34]</span> .",Difference,llm,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",test,0,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
505,3220,10905,ABC_41de1ad534ca00b7a99260de7bb0b2_2,ARXIV:1906.09777,510e26733aaff585d65701b9f1be7ca9d5afc586,"For the other baseline, we use the basic Transformer architecture<span style=""background: yellow; display: inline-block""> [34]</span> .",Uses,llm,"Latest development of neural models has connected the encoder and decoder through a self-attention mechanism. In particular, Transformer, which is solely based on self-attention, has led to breakthroughs in Natural Language Processing (NLP) tasks. However, the multi-head attention mechanism, as a key component of Transformer, limits the effective deployment of the model to a resource-limited setting. In this paper, based on the ideas of tensor decomposition and parameters sharing, we propose a novel self-attention model (namely Multi-linear attention) with Block-Term Tensor Decomposition (BTD). We test and verify the proposed attention method on three language modeling tasks (i.e., PTB, WikiText-103 and One-billion) and a neural machine translation task (i.e., WMT-2016 English-German). Multi-linear attention can not only largely compress the model parameters but also obtain performance improvements, compared with a number of language modeling approaches, such as Transformer, Transformer-XL, and Transformer with tensor train decomposition.","The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.",test,1,"The research problem lies in the limitations of the multi-head attention mechanism, which hinders the efficient deployment of Transformer models in resource-constrained environments. The motivation for this research stems from the need to address the resource limitations of Transformer models. By introducing a new self-attention mechanism that compresses model parameters while maintaining or improving performance, the researchers aim to enable more efficient deployment of these models in resource-constrained settings.",train,"attention method, language modeling approaches, language modeling tasks, multi-head attention mechanism, Multi-linear attention, Multi-linear attention, Natural Language Processing ( NLP ) tasks, neural machine translation task, neural models, parameters sharing, PTB, resource-limited setting, self-attention mechanism, self-attention model, tensor decomposition, tensor train decomposition, Transformer, Transformer, Transformer, Transformer, Transformer-XL, WikiText-103, WMT-2016 English-German"
506,3291,11208,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"Additional references on abusive message detection and conversational network modeling can be found in<span style=""background: yellow; display: inline-block""> (Papegnies et al., 2019)</span> .",Background,llm,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,0,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
507,3292,11209,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"In this work, we use exactly the same measures as in<span style=""background: yellow; display: inline-block""> (Papegnies et al., 2019)</span> .",Uses,llm,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,1,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
508,3293,11210,ABC_8abb7b77fd6996a905395de9693d42_9,ARXIV:1905.07894,dd39c41accacaef0beb7f1d350c8e737b06ed08a,"We use the values matching the best performance, obtained during the greedy search of the parameter space performed in<span style=""background: yellow; display: inline-block""> (Papegnies et al., 2019)</span> .",Uses,llm,"In recent years, online social networks have allowed world-wide users to meet and discuss. As guarantors of these communities, the administrators of these platforms must prevent users from adopting inappropriate behaviors. This verification task, mainly done by humans, is more and more difficult due to the ever growing amount of messages to check. Methods have been proposed to automatize this moderation process, mainly by providing approaches based on the textual content of the exchanged messages. Recent work has also shown that characteristics derived from the structure of conversations, in the form of conversational graphs, can help detecting these abusive messages. In this paper, we propose to take advantage of both sources of information by proposing fusion methods integrating content- and graph-based features. Our experiments on raw chat logs show not only that the content of the messages, but also their dynamics within a conversation contain partially complementary information, allowing performance improvements on an abusive message classification task with a final F-measure of 93.26%.","Moderation of user-generated content in an online community is a challenge that has great socio-economic ramifications. However, the costs incurred by delegating this paper to human agents are high. For this reason, an automatic system able to detect abuse in user-generated content is of great interest. There are a number of ways to tackle this problem, but the most commonly seen in practice are word filtering or regular expression matching. The main limitations are their vulnerability to intentional obfuscation on the part of the users, and their context-insensitive nature. Moreover, they are language dependent and may require appropriate corpora for training. In this paper, we propose a system for automatic abuse detection that completely disregards message content. We first extract a conversational network from raw chat logs and characterize it through topological measures. We then use these as features to train a classifier on our abuse detection task. We thoroughly assess our system on a dataset of user comments originating from a French massively multiplayer online game. We identify the most appropriate network extraction parameters and discuss the discriminative power of our features, relatively to their topological and temporal nature. Our method reaches an  $F$ -measure of 83.89 when using the full feature set, improving on existing approaches. With a selection of the most discriminative features, we dramatically cut computing time while retaining the most of the performance (82.65).",test,1,The research problem lies in the challenge of preventing inappropriate behaviors in online social networks. This task is increasingly difficult due to the manual moderation process being overwhelmed by the growing number of messages. The motivation behind this research is to address the increasing difficulty of manually moderating online social networks and to develop more efficient methods for detecting and preventing inappropriate user behavior.,train,"abusive message classification task, fusion methods, moderation process, online social networks, raw chat logs, verification task"
509,452,816,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,bdaacde65e68f6f617381377ba0b968013dbb1c6,"Several sites have made significant progress to lower the WER to within the 5%-10% range on the Switchboard-CallHome subsets of the Hub5 2000 evaluation [2, 3, 4, 5] .",Background,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","In this paper we show how we have achieved the state-of-the-art performance on the industry-standard NIST 2000 Hub5 English evaluation set. We explore densely connected LSTMs, inspired by the densely connected convolutional networks recently introduced for image classification tasks. We also propose an acoustic model adaptation scheme that simply averages the parameters of a seed neural network acoustic model and its adapted version. This method was applied with the CallHome training corpus and improved individual system performances by on average 6.1% (relative) against the CallHome portion of the evaluation set with no performance loss on the Switchboard portion. With RNN-LM rescoring and lattice combination on the 5 systems trained across three different phone sets, our 2017 speech recognition system has obtained 5.0% and 9.1% on Switchboard and CallHome, respectively, both of which are the best word error rates reported thus far. According to IBM in their latest work to compare human and machine transcriptions, our reported Switchboard word error rate can be considered to surpass the human parity (5.1%) of transcribing conversational telephone speech.",train,0,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
510,453,817,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da,"In terms of the amount of training data available from the DARPA EARS program for training systems on CTS and BN, there are a few significant differences as well. The CTS acoustic training corpus consists of approximately 2000 hours of speech with human transcriptions [2] . In other words, models being developed for BN typically use lightly supervised transcripts for training [6] .",Background,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.",train,0,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
511,454,818,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,bf9f622a3964b4251e42bfbff552a5af3ad266d2,"In [2, 3] we describe state-of-the-art speech recognition systems on the CTS task using multiple LSTM and ResNet acoustic models trained on various acoustic features along with word and character LSTMs and convolutional WaveNet-style language models. In this paper we develop a similar but simpler variant for BN.",Motivation,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","Language models (LMs) based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks. In this paper, we extend an LSTM by adding highway networks inside an LSTM and use the resulting Highway LSTM (HW-LSTM) model for language modeling. The added highway networks increase the depth in the time dimension. Since a typical LSTM has two internal states, a memory cell and a hidden state, we compare various types of HW-LSTM by adding highway networks onto the memory cell and/or the hidden state. Experimental results on English broadcast news and conversational telephone speech recognition show that the proposed HW-LSTM LM improves speech recognition accuracy on top of a strong LSTM LM baseline. We report 5.1% and 9.9% on the Switchboard and CallHome subsets of the Hub5 2000 evaluation, which reaches the best performance numbers reported on these tasks to date.",train,0,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
512,456,820,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,bf9f622a3964b4251e42bfbff552a5af3ad266d2,"In [2, 3] we describe state-of-the-art speech recognition systems on the CTS task using multiple LSTM and ResNet acoustic models trained on various acoustic features along with word and character LSTMs and convolutional WaveNet-style language models. In this paper we develop a similar but simpler variant for BN.",Similar,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","Language models (LMs) based on Long Short Term Memory (LSTM) have shown good gains in many automatic speech recognition tasks. In this paper, we extend an LSTM by adding highway networks inside an LSTM and use the resulting Highway LSTM (HW-LSTM) model for language modeling. The added highway networks increase the depth in the time dimension. Since a typical LSTM has two internal states, a memory cell and a hidden state, we compare various types of HW-LSTM by adding highway networks onto the memory cell and/or the hidden state. Experimental results on English broadcast news and conversational telephone speech recognition show that the proposed HW-LSTM LM improves speech recognition accuracy on top of a strong LSTM LM baseline. We report 5.1% and 9.9% on the Switchboard and CallHome subsets of the Hub5 2000 evaluation, which reaches the best performance numbers reported on these tasks to date.",train,0,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
513,457,821,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da,"In addition to automatic speech recognition results, similar to [2] , we also present human performance on the same BN test sets.",Similar,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.",train,0,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
514,458,822,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da,"Similar to [2] , human performance measurements on two broadcast news tasks -RT04 and DEV04F -are carried out by Appen.",Similar,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.",train,0,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
515,459,823,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,bdaacde65e68f6f617381377ba0b968013dbb1c6,"Several sites have made significant progress to lower the WER to within the 5%-10% range on the Switchboard-CallHome subsets of the Hub5 2000 evaluation [2, 3, 4, 5] . Given the progress on conversational telephone speech, we focus on the other closely related broadcast news recognition task that received similar attention within the DARPA EARS program.",Motivation,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","In this paper we show how we have achieved the state-of-the-art performance on the industry-standard NIST 2000 Hub5 English evaluation set. We explore densely connected LSTMs, inspired by the densely connected convolutional networks recently introduced for image classification tasks. We also propose an acoustic model adaptation scheme that simply averages the parameters of a seed neural network acoustic model and its adapted version. This method was applied with the CallHome training corpus and improved individual system performances by on average 6.1% (relative) against the CallHome portion of the evaluation set with no performance loss on the Switchboard portion. With RNN-LM rescoring and lattice combination on the 5 systems trained across three different phone sets, our 2017 speech recognition system has obtained 5.0% and 9.1% on Switchboard and CallHome, respectively, both of which are the best word error rates reported thus far. According to IBM in their latest work to compare human and machine transcriptions, our reported Switchboard word error rate can be considered to surpass the human parity (5.1%) of transcribing conversational telephone speech.",train,0,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
516,460,824,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da,"The transcriptions were also filtered to remove non-speech markers, partial words, punctuation marks etc as described in [2] .",Uses,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.",train,1,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
517,461,825,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"Since our systems do not generate novel sequences, we follow Bryant and Briscoe (2018) and use simple heuristics to generate a confusion set of sentences that our language models score.",Uses,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
518,462,826,ABC_247bbc4eb671895222065ed425f968_5,ARXIV:1904.13258,c9bd15c7838c1d3cdd5f5113a2efd9440f86b3da,"To complement the LSTM acoustic model, we train a deep Residual Network based on the best performing architecture proposed in [2] .",Uses,s2,"With recent advances in deep learning, considerable attention has been given to achieving automatic speech recognition performance close to human performance on tasks like conversational telephone speech (CTS) recognition. In this paper we evaluate the usefulness of these proposed techniques on broadcast news (BN), a similar challenging task. We also perform a set of recognition measurements to understand how close the achieved automatic speech recognition results are to human performance on this task. On two publicly available BN test sets, DEV04F and RT04, our speech recognition system using LSTM and residual network based acoustic models with a combination of n-gram and neural network language models performs at 6.5% and 5.9% word error rate. By achieving new performance milestones on these test sets, our experiments show that techniques developed on other related tasks, like CTS, can be transferred to achieve similar performance. In contrast, the best measured human recognition performance on these test sets is much lower, at 3.6% and 2.8% respectively, indicating that there is still room for new techniques and improvements in this space, to reach human performance levels.","One of the most difficult speech recognition tasks is accurate recognition of human to human communication. Advances in deep learning over the last few years have produced major speech recognition improvements on the representative Switchboard conversational corpus. Word error rates that just a few years ago were 14% have dropped to 8.0%, then 6.6% and most recently 5.8%, and are now believed to be within striking range of human performance. This then raises two issues - what IS human performance, and how far down can we still drive speech recognition error rates? A recent paper by Microsoft suggests that we have already achieved human performance. In trying to verify this statement, we performed an independent set of human performance measurements on two conversational tasks and found that human performance may be considerably better than what was earlier reported, giving the community a significantly harder goal to achieve. We also report on our own efforts in this area, presenting a set of acoustic and language modeling techniques that lowered the word error rate of our own English conversational telephone LVCSR system to the level of 5.5%/10.3% on the Switchboard/CallHome subsets of the Hub5 2000 evaluation, which - at least at the writing of this paper - is a new performance milestone (albeit not at what we measure to be human performance!). On the acoustic side, we use a score fusion of three models: one LSTM with multiple feature inputs, a second LSTM trained with speaker-adversarial multi-task learning and a third residual net (ResNet) with 25 convolutional layers and time-dilated convolutions. On the language modeling side, we use word and character LSTMs and convolutional WaveNet-style language models.",train,1,"The primary research problem is to achieve human-level performance in automatic speech recognition, particularly for challenging tasks like broadcast news and conversational telephone speech recognition. The research is motivated by the desire to bridge the gap between automatic speech recognition performance and human performance, and to explore the transferability of techniques developed for one task to other related tasks.",test,"automatic speech recognition, automatic speech recognition, BN test sets, broadcast news ( BN ), conversational telephone speech ( CTS ) recognition, CTS, human recognition, n-gram and neural network language models, residual network based acoustic models"
519,463,827,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"While there is a substantial amount of work on statistical (Rozovskaya and Roth, 2016; Junczys-Dowmunt and Grundkiewicz, 2014; Yannakoudakis et al., 2017) and neural (Ji et al., 2017; Xie et al., 2016; Yuan and Briscoe, 2016; Chollampatt et al., 2016; Chollampatt and Ng, 2017; Chollampatt and Ng, 2018) machine translation methods for GEC, we follow the approach of Bryant and Briscoe (2018) and explore how such models would fare in this task when treated as simple language models.",Uses,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
520,464,828,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"Finally, for spelling mistakes, we, again, follow Bryant and Briscoe (2018) and use CyHunSpell 3 to generate alternatives for non-words.",Uses,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
521,465,829,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"Concretely, let P (s c ) be the probability of the candidate sentence and P (s o ) the probability of the Table 2 : Results of our Transformer-Language Model approach against similar approaches (Bryant and Briscoe, 2018) and state-of-the-art on Grammatical Error Correction.",Uses,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
522,466,830,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,Table 2 presents the results of our method comparing them against recent state-of-the-art supervised models and the simple n-gram language model used by Bryant and Briscoe (2018) .,Uses,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
523,467,831,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"More specifically, Bryant and Briscoe (2018) train a 5-gram language model on the One Billion Word Benchmark (Chelba et al., 2013) dataset and find that it produces competitive baseline results without any supervised training. In our work, we extend this work by substituting the n-gram model for several publicly available implementations of state-of-the-art Transformer language models trained on large linguistic corpora and assess their performance on GEC without any supervised training.",Extention,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
524,468,832,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"Note that in our method, we do not make use of the training sets commonly used with these datasets. However, we use the development sets used by Bryant and Briscoe (2018) to tune the hyperparameter τ .",Uses,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
525,469,833,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"In this work, we follow the setup from Bryant and Briscoe (2018) substituting the 5-gram language model for different language models based on the Transformer architecture.",Extention,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
526,470,834,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,Our key motivation was to corroborate and extend the results of Bryant and Briscoe (2018) to current state-of-the-art language models which have been trained in several languages and show that these models are tough baselines to beat for novel GEC systems.,Extention,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,1,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
527,471,835,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,Our key motivation was to corroborate and extend the results of Bryant and Briscoe (2018) to current state-of-the-art language models which have been trained in several languages and show that these models are tough baselines to beat for novel GEC systems.,Motivation,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,0,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
528,472,836,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"However, Bryant and Briscoe (2018) recently revived the idea, achieving competitive performance with the state-ofthe-art, demonstrating the effectiveness of the approaches to the task without using any annotated data for training.",Background,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,0,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
529,473,837,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"More specifically, Bryant and Briscoe (2018) train a 5-gram language model on the One Billion Word Benchmark (Chelba et al., 2013) dataset and find that it produces competitive baseline results without any supervised training. In our work, we extend this work by substituting the n-gram model for several publicly available implementations of state-of-the-art Transformer language models trained on large linguistic corpora and assess their performance on GEC without any supervised training.",Motivation,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,0,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
530,474,839,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"Note that in our method, we do not make use of the training sets commonly used with these datasets. However, we use the development sets used by Bryant and Briscoe (2018) to tune the hyperparameter τ .",Difference,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,0,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
531,475,840,ABC_1056d36c5ed22c7a34f6fe82b4962f_5,ACL:W19-4412,97e94ae64d494127723360cc52a390d79856b0c5,"Similar to Bryant and Briscoe (2018) , we report results on three metrics.",Similar,s2,"Recent work on Grammatical Error Correction (GEC) has highlighted the importance of language modeling in that it is certainly possible to achieve good performance by comparing the probabilities of the proposed edits. At the same time, advancements in language modeling have managed to generate linguistic output, which is almost indistinguishable from that of human-generated text. In this paper, we up the ante by exploring the potential of more sophisticated language models in GEC and offer some key insights on their strengths and weaknesses. We show that, in line with recent results in other NLP tasks, Transformer architectures achieve consistently high performance and provide a competitive baseline for future machine learning models.","Since the end of the CoNLL-2014 shared task on grammatical error correction (GEC), research into language model (LM) based approaches to GEC has largely stagnated. In this paper, we re-examine LMs in GEC and show that it is entirely possible to build a simple system that not only requires minimal annotated data (∼1000 sentences), but is also fairly competitive with several state-of-the-art systems. This approach should be of particular interest for languages where very little annotated training data exists, although we also hope to use it as a baseline to motivate future research.",train,0,"The research problem is to improve Grammatical Error Correction (GEC) by exploring the potential of more sophisticated language models. The research is motivated by the significant advancements in language modeling, which have led to generated text that is almost indistinguishable from human-generated text. This suggests that more sophisticated language models could be beneficial for improving GEC performance.",test,"GEC, Grammatical Error Correction ( GEC ), human-generated text, Transformer architectures"
532,506,950,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,3aa52436575cf6768a0a1a476601825f6a62e58f,"In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences.",Background,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",train,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
533,507,951,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,3aa52436575cf6768a0a1a476601825f6a62e58f,"All three previous work use uni-directional language-model-like models. Linzen et al. (2016) start with existing sentences from wikipedia that contain a present-tense verb. They feed each sentence word by word into an LSTM, stop right before the focus verb, and ask the model to predict a binary plural/singular decision (supervised setup) or compare the probability assigned by a pre-trained language model (LM) to the plural vs singular forms of the verb (LM setup).",Background,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",train,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
534,508,952,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,3aa52436575cf6768a0a1a476601825f6a62e58f,"The evaluation is then performed similarly to the LM setup of Linzen et al. (2016) : the sentence is fed into a pretraiend LSTM LM up to the focus verb, and the model is considered correct if the probability assigned to the correct inflection of the original verb form given the prefix is larger than that assigned to the incorrect inflection.",Background,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture’s grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.",train,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
535,509,953,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,997c55547aeca733dfc5dfebd12412612ecba022,"Indeed, Tran et al. (2018) finds that transformerbased models perform worse than LSTM models on the Linzen et al. (2016) agreement prediction dataset.",Difference,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recent work has shown that recurrent neural networks (RNNs) can implicitly capture and exploit hierarchical information when trained to solve common natural language processing tasks (Blevins et al., 2018) such as language modeling (Linzen et al., 2016; Gulordava et al., 2018) and neural machine translation (Shi et al., 2016). In contrast, the ability to model structured data with non-recurrent neural networks has received little attention despite their success in many NLP tasks (Gehring et al., 2017; Vaswani et al., 2017). In this work, we compare the two architectures—recurrent versus non-recurrent—with respect to their ability to model hierarchical structure and find that recurrency is indeed important for this purpose. The code and data used in our experiments is available at https://github.com/ ketranm/fan_vs_rnn",train,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
536,510,954,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1,"This differs from Linzen et al. (2016) and Gulordava et al. (2018) by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context.",Difference,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",train,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
537,511,955,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1,"I use the stimuli provided by (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",train,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
538,513,957,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I also discard the agreement cases involving the verbs is or are in Linzen et al. (2016) and in Gulordava et al. (2018) , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",train,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
539,514,958,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1,"Indeed, Tran et al. (2018) finds that transformerbased models perform worse than LSTM models on the Linzen et al. (2016) agreement prediction dataset. In contrast, (Tang et al., 2018) find that self-attention performs on par with LSTM for syntax sensitive dependencies in the context of machine-translation, and performance on syntactic tasks is correlated with the number of attention heads in multi-head attention. I adapt the evaluation protocol and stimuli of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).",Uses,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",train,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
540,515,959,ABC_d5144370a9361ff870dd3cb2e064ff_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I similarly discard 680 sentences from (Linzen et al., 2016) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from (Gulordava et al., 2018) .",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",train,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
541,1104,2123,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,Zhiyuan Chen [2] ever proposed a approach to determine which domain dose a word have the sentiment orientation to achieve the goal of lifelong learning.,Background,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
542,1105,2124,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,Zhiyuan and Bing [2] improved the sentiment classification by involving knowledge.,Background,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
543,1106,2125,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,• Knowledge-Base Learner (KBL): The Knowledge-Based Learner [2] aims to retrieve and transfer previous knowledge to the current task.,Background,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
544,1107,2126,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,Previous classical paper [2] chose the sentiment classification as the learning target because it is could be regarded as a task as well as a group of subtasks in different domain.,Background,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
545,1108,2127,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"""Lifelong Sentiment Classification"" (""LSC"" for simple below) [2] records that which domain does a word have the sentiment orientation.",Background,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
546,1140,2178,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,6dfc2ff03534a4325d06c6f88c3144831996629b,Several large real image VQA datasets have recently emerged [8] [9] [10] [11] [12] [13] [14] .,Background,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","Visual understanding goes well beyond object recognition. With one glance at an image, we can effortlessly imagine the world beyond the pixels: for instance, we can infer people's actions, goals, and mental states. While this task is easy for humans, it is tremendously difficult for today's vision systems, requiring higher-order cognition and commonsense reasoning about the world. We formalize this task as Visual Commonsense Reasoning. Given a challenging question about an image, a machine must answer correctly and then provide a rationale justifying its answer. Next, we introduce a new dataset, VCR, consisting of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching, a new approach to transform rich annotations into multiple choice questions with minimal bias. Experimental results show that while humans find VCR easy (over 90% accuracy), state-of-the-art vision models struggle (~45%). To move towards cognition-level understanding, we present a new reasoning engine, Recognition to Cognition Networks (R2C), that models the necessary layered inferences for grounding, contextualization, and reasoning. R2C helps narrow the gap between humans and machines (~65%); still, the challenge is far from solved, and we provide analysis that suggests avenues for future work.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
547,1141,2179,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,VQA models are inclined to learn unimodal biases from the datasets [10] .,Background,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
548,1142,2180,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,"However, even with this additional balancing, statistical biases from the question remain and can be leveraged [10] .",Motivation,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
549,1143,2181,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,VQA-CP v2 and VQA-CP v1 [10] were recently introduced as diagnostic datasets containing different answer distributions for each questiontype between train and test splits.,Background,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
550,1144,2182,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,c122fa378a774ba202d418cf71c5c356cf2f902f,"However, it has been shown that they tend to exploit statistical regularities between answer occurrences and certain patterns in the question [24, 10, 25, 23, 13] . While they are designed to merge information from both modalities, in practice they often answer without considering the image modality.",Motivation,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","We introduce GQA, a new dataset for real-world visual reasoning and compositional question answering, seeking to address key shortcomings of previous VQA datasets. We have developed a strong and robust question engine that leverages scene graph structures to create 22M diverse reasoning questions, all come with functional programs that represent their semantics. We use the programs to gain tight control over the answer distribution and present a new tunable smoothing technique to mitigate language biases. Accompanying the dataset is a suite of new metrics that evaluate essential qualities such as consistency, grounding and plausibility. An extensive analysis is performed for baselines as well as state-of-the-art models, providing fine-grained results for different question types and topologies. Whereas a blind LSTM obtains mere 42.1%, and strong VQA models achieve 54.1%, human performance tops at 89.3\%, offering ample opportunity for new research to explore. We strongly hope GQA will provide an enabling resource for the next generation of models with enhanced robustness, improved consistency, and deeper semantic understanding for images and language.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
551,1145,2183,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,We run extensive experiments on VQA-CP v2 [10] and demonstrate the ability of RUBi to surpass current state-of-the-art results from a significant margin.,Uses,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,1,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
552,1146,2184,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,45ec1446f42c0a7c7fe74319118335c76e0f7b19,"However, when evaluated on a test set that displays different statistical regularities, they usually suffer from a significant drop in accuracy [10, 25] .",Motivation,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training such as overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings. In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary -- discouraging the VQA model from capturing language biases in its question encoding. Further,we leverage this question-only model to estimate the increase in model confidence after considering the image, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models -- achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
553,1147,2185,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,Experimental setup We train and evaluate our models on VQA-CP v2 [10] .,Uses,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,1,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
554,1148,2186,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,"VQA-CP v2 and VQA-CP v1 [10] were recently introduced as diagnostic datasets containing different answer distributions for each questiontype between train and test splits. Consequentially, models biased towards the question modality fail on these benchmarks. We use the more challenging VQA-CP v2 dataset extensively in order to show the ability of our approach to reduce the learning of biases coming from the question modality.",Uses,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,1,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
555,1149,2188,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,"This accuracy corresponds to a gain of +5.94 percentage points over the current state-of-the-art UpDn + Q-Adv + DoE. It also corresponds to a gain of +15.88 over GVQA [10] , which is a specific architecture designed for VQA-CP.",Difference,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
556,1150,2189,ABC_06db17253d76150772c0926e11131d_12,ARXIV:1906.10169,90873a97aa9a43775e5aeea01b03aea54b28bfbd,"We report a drop of 1.94 percentage points with respect to our baseline, while [10] report a drop of 3.78 between GVQA and their SAN baseline.",Difference,s2,"Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. 
We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. 
Our code is available: this http URL","A number of studies have found that today's Visual Question Answering (VQA) models are heavily driven by superficial correlations in the training data and lack sufficient image grounding. To encourage development of models geared towards the latter, we propose a new setting for VQA where for every question type, train and test sets have different prior distributions of answers. Specifically, we present new splits of the VQA v1 and VQA v2 datasets, which we call Visual Question Answering under Changing Priors (VQA-CP v1 and VQA-CP v2 respectively). First, we evaluate several existing VQA models under this new setting and show that their performance degrades significantly compared to the original VQA setting. Second, we propose a novel Grounded Visual Question Answering model (GVQA) that contains inductive biases and restrictions in the architecture specifically designed to prevent the model from 'cheating' by primarily relying on priors in the training data. Specifically, GVQA explicitly disentangles the recognition of visual concepts present in the image from the identification of plausible answer space for a given question, enabling the model to more robustly generalize across different distributions of answers. GVQA is built off an existing VQA model - Stacked Attention Networks (SAN). Our experiments demonstrate that GVQA significantly outperforms SAN on both VQA-CP v1 and VQA-CP v2 datasets. Interestingly, it also outperforms more powerful VQA models such as Multimodal Compact Bilinear Pooling (MCB) in several cases. GVQA offers strengths complementary to SAN when trained and evaluated on the original VQA v1 and VQA v2 datasets. Finally, GVQA is more transparent and interpretable than existing VQA models.",train,0,"The research focuses on addressing the issue of biases in Visual Question Answering (VQA) models. These models often rely on biases, leading to poor performance when evaluated on data outside their training distribution, making them unsuitable for real-world applications. The research is driven by the need to improve the robustness and reliability of VQA models in real-world settings. The current reliance on biases in VQA models makes them unsuitable for practical applications, leading to a need for solutions that can mitigate this issue and enhance model performance.",test,"Answering ( VQA ), learning strategy, question-only model, VQA model, VQA model, VQA model, VQA models, VQA models"
557,1429,2749,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,"Similar to [13] , we apply a 2DLSTM layer to combine the acoustic model (the LSTM encoder) and the language model (the decoder) without any attention components.",Similar,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
558,1430,2750,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,Our model is similar to an architecture used in machine translation described in [13] .,Similar,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
559,1431,2751,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,The additional connections are marked in blue [13] .,Background,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
560,1432,2752,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,"Recently, the 2DLSTM layer also has been used for sequence-to-sequence modeling in machine translation [13] where it implicitly updates the source representation conditioned on the generated target words.",Background,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
561,1433,2753,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,"Similar to [13] , we then equip the network by a 2DLSTM layer to relate the encoder and the decoder states.",Similar,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
562,1434,2754,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,"Similar to [13] , we apply a 2DLSTM layer to combine the acoustic model (the LSTM encoder) and the language model (the decoder) without any attention components. Compared to [13] , our model is much deeper. We use max-pooling to select the most relevant encoder state whereas [13] uses the last horizontal state of the 2DLSTM.",Difference,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
563,1435,2755,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,"This algorithm is faster than [13] , where at each output step, they recompute all previous states of 2DLSTM from scratch which are not required.",Difference,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
564,1436,2756,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,"As written in Equation 5 , its activation is computed analogously to the other gates [13, 11] .",Background,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,0,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
565,1437,2759,ABC_b0083488650bc98477fb10a9c5a808_15,CorpusID:89604414,be16a4e95bbb232fca75d3c9209dab1948193241,"Similar to [13] , we apply a 2DLSTM layer to combine the acoustic model (the LSTM encoder) and the language model (the decoder) without any attention components. Compared to [13] , our model is much deeper. We use max-pooling to select the most relevant encoder state whereas [13] uses the last horizontal state of the 2DLSTM.",Extention,s2,"Attention-based sequence-to-sequence models have shown promising results in automatic speech recognition. Using these architectures, one-dimensional input and output sequences are related by an attention approach, thereby replacing more explicit alignment processes, like in classical HMM-based modeling. In contrast, here we apply a novel two-dimensional long short-term memory (2DLSTM) architecture to directly model the input/output relation between audio/feature vector sequences and word sequences. The proposed model is an alternative model such that instead of using any type of attention components, we apply a 2DLSTM layer to assimilate the context from both input observations and output transcriptions. The experimental evaluation on the Switchboard 300h automatic speech recognition task shows word error rates for the 2DLSTM model that are competitive to end-to-end attention-based model.","This work investigates an alternative model for neural machine translation (NMT) and proposes a novel architecture, where we employ a multi-dimensional long short-term memory (MDLSTM) for translation modelling. In the state-of-the-art methods, source and target sentences are treated as one-dimensional sequences over time, while we view translation as a two-dimensional (2D) mapping using an MDLSTM layer to define the correspondence between source and target words. We extend beyond the current sequence to sequence backbone NMT models to a 2D structure in which the source and target sentences are aligned with each other in a 2D grid. Our proposed topology shows consistent improvements over attention-based sequence to sequence model on two WMT 2017 tasks, German<->English.",train,1,"The research problem is to improve automatic speech recognition by exploring alternative approaches to attention-based sequence-to-sequence models. Specifically, the research focuses on addressing the need for a more direct and efficient method to model the relationship between audio input and word output sequences. The motivation behind this research is to explore alternative methods for automatic speech recognition, specifically focusing on developing a model that can directly model the relationship between audio and word sequences without relying on attention mechanisms.",test,"2DLSTM layer, 2DLSTM model, alignment processes, attention approach, attention components, Attention-based sequence-to-sequence models, audio/feature vector sequences, automatic speech recognition, end-to-end attention-based model, HMM-based modeling, Switchboard 300h automatic speech recognition task, two-dimensional long short-term memory ( 2DLSTM ) architecture, word sequences"
566,2883,5892,ABC_b2a6ec11403fe73b9bae7742c1c5a2_36,ARXIV:1909.05308,7ee7e580798e9177dc8ff9bb94c895cd3d4ab780,"Prior natural language processing (NLP) approaches to student revision analysis have focused on identifying revisions during argumentative writing and classifying their purposes and other properties [12, 11, 7, 1] .",Background,s2,incorrect id format for ARXIV:1909.05308,"Studies of writing revisions rarely focus on revision quality. To address this issue, we introduce a corpus of between-draft revisions of student argumentative essays, annotated as to whether each revision improves essay quality. We demonstrate a potential usage of our annotations by developing a machine learning model to predict revision improvement. With the goal of expanding training data, we also extract revisions from a dataset edited by expert proofreaders. Our results indicate that blending expert and non-expert revisions increases model performance, with expert data particularly important for predicting low-quality revisions.",train,0,"The research aims to identify and define the different editor roles students adopt during the revision process of argumentative writing, and to investigate the relationship between these roles, revision purposes, and writing improvement. The research is motivated by a need to better understand the revision process in argumentative writing. By identifying the different editor roles students take on and their impact on writing, the research aims to provide insights that could potentially inform instructional strategies and improve student writing outcomes.",test,"argumentative writing, instructional strategies, revision process, revision process of argumentative writing, revision purposes, roles, writing, writing improvement"
567,2884,5893,ABC_b2a6ec11403fe73b9bae7742c1c5a2_36,ARXIV:1909.05308,eabc3aa7541757bca55ef47d16c5d7229b71fba6,"Nonidentical aligned sentences were extracted as the revisions, resulting in three types of revision operations -Add, Delete, M odif y. Each extracted revision was manually annotated with a purpose following the revision schema shown in Figure 1 (modified compared to [12] by adding the Precision category).",Extention,s2,incorrect id format for ARXIV:1909.05308,"This paper explores the annotation and classification of students’ revision behaviors in argumentative writing. A sentence-level revision schema is proposed to capture why and how students make revisions. Based on the proposed schema, a small corpus of student essays and revisions was annotated. Studies show that manual annotation is reliable with the schema and the annotated information helpful for revision analysis. Furthermore, features and methods are explored for the automatic classification of revisions. Intrinsic evaluations demonstrate promising performance in high-level revision classification (surface vs. text-based). Extrinsic evaluations demonstrate that our method for automatic revision classification can be used to predict a writer’s improvement.",train,1,"The research aims to identify and define the different editor roles students adopt during the revision process of argumentative writing, and to investigate the relationship between these roles, revision purposes, and writing improvement. The research is motivated by a need to better understand the revision process in argumentative writing. By identifying the different editor roles students take on and their impact on writing, the research aims to provide insights that could potentially inform instructional strategies and improve student writing outcomes.",test,"argumentative writing, instructional strategies, revision process, revision process of argumentative writing, revision purposes, roles, writing, writing improvement"
568,2885,5894,ABC_b2a6ec11403fe73b9bae7742c1c5a2_36,ARXIV:1909.05308,eabc3aa7541757bca55ef47d16c5d7229b71fba6,"A corpus study in [12] showed that content changes are correlated with argumentative writing improvement, reaffirming the statement of [4] . Using a similar method, we investigate if our editor roles are related to writing improvement.",Similar,s2,incorrect id format for ARXIV:1909.05308,"This paper explores the annotation and classification of students’ revision behaviors in argumentative writing. A sentence-level revision schema is proposed to capture why and how students make revisions. Based on the proposed schema, a small corpus of student essays and revisions was annotated. Studies show that manual annotation is reliable with the schema and the annotated information helpful for revision analysis. Furthermore, features and methods are explored for the automatic classification of revisions. Intrinsic evaluations demonstrate promising performance in high-level revision classification (surface vs. text-based). Extrinsic evaluations demonstrate that our method for automatic revision classification can be used to predict a writer’s improvement.",train,0,"The research aims to identify and define the different editor roles students adopt during the revision process of argumentative writing, and to investigate the relationship between these roles, revision purposes, and writing improvement. The research is motivated by a need to better understand the revision process in argumentative writing. By identifying the different editor roles students take on and their impact on writing, the research aims to provide insights that could potentially inform instructional strategies and improve student writing outcomes.",test,"argumentative writing, instructional strategies, revision process, revision process of argumentative writing, revision purposes, roles, writing, writing improvement"
569,2886,5895,ABC_b2a6ec11403fe73b9bae7742c1c5a2_36,ARXIV:1909.05308,eabc3aa7541757bca55ef47d16c5d7229b71fba6,"A corpus study in [12] showed that content changes are correlated with argumentative writing improvement, reaffirming the statement of [4] .",Background,s2,incorrect id format for ARXIV:1909.05308,"This paper explores the annotation and classification of students’ revision behaviors in argumentative writing. A sentence-level revision schema is proposed to capture why and how students make revisions. Based on the proposed schema, a small corpus of student essays and revisions was annotated. Studies show that manual annotation is reliable with the schema and the annotated information helpful for revision analysis. Furthermore, features and methods are explored for the automatic classification of revisions. Intrinsic evaluations demonstrate promising performance in high-level revision classification (surface vs. text-based). Extrinsic evaluations demonstrate that our method for automatic revision classification can be used to predict a writer’s improvement.",train,0,"The research aims to identify and define the different editor roles students adopt during the revision process of argumentative writing, and to investigate the relationship between these roles, revision purposes, and writing improvement. The research is motivated by a need to better understand the revision process in argumentative writing. By identifying the different editor roles students take on and their impact on writing, the research aims to provide insights that could potentially inform instructional strategies and improve student writing outcomes.",test,"argumentative writing, instructional strategies, revision process, revision process of argumentative writing, revision purposes, roles, writing, writing improvement"
570,2887,5898,ABC_b2a6ec11403fe73b9bae7742c1c5a2_36,ARXIV:1909.05308,448330564c8a050affdf904ef8838acda582991b,"Our work takes advantage of several corpora of multiple drafts of argumentative essays written by both high-school and college students [12, 11] , where all data has been annotated for revision using the framework of [12] .",Uses,s2,incorrect id format for ARXIV:1909.05308,"This paper presents ArgRewrite, a corpus of between-draft revisions of argumentative essays. Drafts are manually aligned at the sentence level, and the writer’s purpose for each revision is annotated with categories analogous to those used in argument mining and discourse analysis. The corpus should enable advanced research in writing comparison and revision analysis, as demonstrated via our own studies of student revision behavior and of automatic revision purpose prediction.",train,1,"The research aims to identify and define the different editor roles students adopt during the revision process of argumentative writing, and to investigate the relationship between these roles, revision purposes, and writing improvement. The research is motivated by a need to better understand the revision process in argumentative writing. By identifying the different editor roles students take on and their impact on writing, the research aims to provide insights that could potentially inform instructional strategies and improve student writing outcomes.",test,"argumentative writing, instructional strategies, revision process, revision process of argumentative writing, revision purposes, roles, writing, writing improvement"
571,3213,6508,ABC_7d7895690c84fb1af46c30f858470e_42,ARXIV:1901.04936,8c1b00128e74f1cd92aede3959690615695d5101,"However, on tasks like Question Answering, in all the existing well-performing models, RNNs are employed in a bidirectional way, or a self-attention mechanism is employed [11, 2, 4, 8] .",Background,s2,"Any system which performs goal-directed continual learning must not only learn incrementally but process and absorb information incrementally. Such a system also has to understand when its goals have been achieved. In this paper, we consider these issues in the context of question answering. Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.","Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",train,0,"The research problem is developing question answering systems that can perform goal-directed continual learning by incrementally learning, processing information, and recognizing goal achievement. The research is motivated by the limitations of current question answering models that rely on reasoning over entire passages instead of incrementally. The researchers aim to overcome the inadequacy of naive incremental reading approaches by developing a more robust and accurate solution.",test,"DocQA [ 2 ] model, goal-directed continual learning, incremental reading, incremental reading, question answering models, unidirectional language models"
572,3214,6509,ABC_7d7895690c84fb1af46c30f858470e_42,ARXIV:1901.04936,8c1b00128e74f1cd92aede3959690615695d5101,"However, on tasks like Question Answering, in all the existing well-performing models, RNNs are employed in a bidirectional way, or a self-attention mechanism is employed [11, 2, 4, 8] . This means these models need to processes the whole input sequence to compute the final answer. This is a reasonable approach if the input sequence is as short as a sentence, but it becomes less effective and efficient as the length of the input sequence increases.",Motivation,s2,"Any system which performs goal-directed continual learning must not only learn incrementally but process and absorb information incrementally. Such a system also has to understand when its goals have been achieved. In this paper, we consider these issues in the context of question answering. Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.","Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",train,0,"The research problem is developing question answering systems that can perform goal-directed continual learning by incrementally learning, processing information, and recognizing goal achievement. The research is motivated by the limitations of current question answering models that rely on reasoning over entire passages instead of incrementally. The researchers aim to overcome the inadequacy of naive incremental reading approaches by developing a more robust and accurate solution.",test,"DocQA [ 2 ] model, goal-directed continual learning, incremental reading, incremental reading, question answering models, unidirectional language models"
573,3215,6510,ABC_7d7895690c84fb1af46c30f858470e_42,ARXIV:1901.04936,8c1b00128e74f1cd92aede3959690615695d5101,"In standard question answering, we do not care how the context is presented to the model, and for the models that achieve state of the art results, e.g. [11, 2] , they process the full context before making any decisions.",Difference,s2,"Any system which performs goal-directed continual learning must not only learn incrementally but process and absorb information incrementally. Such a system also has to understand when its goals have been achieved. In this paper, we consider these issues in the context of question answering. Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.","Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",train,0,"The research problem is developing question answering systems that can perform goal-directed continual learning by incrementally learning, processing information, and recognizing goal achievement. The research is motivated by the limitations of current question answering models that rely on reasoning over entire passages instead of incrementally. The researchers aim to overcome the inadequacy of naive incremental reading approaches by developing a more robust and accurate solution.",test,"DocQA [ 2 ] model, goal-directed continual learning, incremental reading, incremental reading, question answering models, unidirectional language models"
574,3216,6511,ABC_7d7895690c84fb1af46c30f858470e_42,ARXIV:1901.04936,8c1b00128e74f1cd92aede3959690615695d5101,"In standard question answering, we do not care how the context is presented to the model, and for the models that achieve state of the art results, e.g. [11, 2] , they process the full context before making any decisions. We show that it is possible to modify these models to be incremental while achieving similar performance.",Extention,s2,"Any system which performs goal-directed continual learning must not only learn incrementally but process and absorb information incrementally. Such a system also has to understand when its goals have been achieved. In this paper, we consider these issues in the context of question answering. Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.","Current end-to-end machine reading and question answering (Q\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.",train,1,"The research problem is developing question answering systems that can perform goal-directed continual learning by incrementally learning, processing information, and recognizing goal achievement. The research is motivated by the limitations of current question answering models that rely on reasoning over entire passages instead of incrementally. The researchers aim to overcome the inadequacy of naive incremental reading approaches by developing a more robust and accurate solution.",test,"DocQA [ 2 ] model, goal-directed continual learning, incremental reading, incremental reading, question answering models, unidirectional language models"
575,3217,6512,ABC_7d7895690c84fb1af46c30f858470e_42,ARXIV:1901.04936,c25a67ad7e8629a9d12b9e2fc356cd73af99a060,We use DocQA as the baseline model [2] .,Uses,s2,"Any system which performs goal-directed continual learning must not only learn incrementally but process and absorb information incrementally. Such a system also has to understand when its goals have been achieved. In this paper, we consider these issues in the context of question answering. Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.","Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q&A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.",train,1,"The research problem is developing question answering systems that can perform goal-directed continual learning by incrementally learning, processing information, and recognizing goal achievement. The research is motivated by the limitations of current question answering models that rely on reasoning over entire passages instead of incrementally. The researchers aim to overcome the inadequacy of naive incremental reading approaches by developing a more robust and accurate solution.",test,"DocQA [ 2 ] model, goal-directed continual learning, incremental reading, incremental reading, question answering models, unidirectional language models"
576,4289,8831,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,049f4c438ce9eefa622ae5ba5fb7e34443b86133,"Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019) , it is not the case for the latest techniques of contextualized word embeddings.",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
577,4290,8832,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Word embeddings representation spaces are known to present geometrical phenomena mimicking relations and analogies between words (e.g. man is to woman as king is to queen). Following this property of finding relations or analogies, one popular example of gender bias is the word association between man to computer programmer as woman to homemaker (Bolukbasi et al., 2016) .",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
578,4291,8833,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,e235ad7dcf6e97cd372f09724dc947c5b1efac79,"Only just recently, Zhao et al. (2019) present a first analysis on the topic based on the proposed methods in Bolukbasi et al. (2016) .",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo’s contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
579,4292,8834,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Human-generated corpora suffer from social biases. Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them (Bolukbasi et al., 2016; Caliskan et al., 2017) .",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
580,4293,8835,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,049f4c438ce9eefa622ae5ba5fb7e34443b86133,"Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it. The transformations proposed by both Bolukbasi et al. (2016) and Zhao et al. (2018b) are downstream task-agnostic. This fact is used in the work of Gonen and Goldberg (2019) to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations.",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
581,4294,8836,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Bolukbasi et al. (2016) studied from a geometrical point of view the presence of gender bias in word embeddings. For this, they compute the subspace where the gender information concentrates by computing the principal components of the difference of vector representations of male and female gender-defining word pairs. With the gender subspace, the authors identify direct and indirect biases in profession words. Finally, they mitigate the bias by nullifying the information in the gender subspace for words that should not be associated to gender, and also equalize their distance to both elements of gender-defining word pairs.",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
582,4295,8837,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"We compare our results to previous results from debiased and non-debiased word embeddings (Bolukbasi et al., 2016) . Bolukbasi et al. (2016) propose to identify gender bias in word representations by computing the direction between representations of male and female word pairs from the Definitional List ( − → he-−→ she, −−→ man-− −−−− → woman) and computing their principal components.",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
583,4296,8838,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in Bolukbasi et al. (2016) . This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings. Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
584,4297,8839,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,049f4c438ce9eefa622ae5ba5fb7e34443b86133,"Recent progress in word embedding techniques has been achieved with contextualized word embeddings (Peters et al., 2018) which provide different vector representations for the same word in different contexts. While gender bias has been studied, detected and partially addressed for standard word embeddings techniques (Bolukbasi et al., 2016; Zhao et al., 2018a; Gonen and Goldberg, 2019) , it is not the case for the latest techniques of contextualized word embeddings.",Motivation,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
585,4298,8840,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,049f4c438ce9eefa622ae5ba5fb7e34443b86133,"To address these questions, we adapt and contrast with the evaluation measures proposed by Bolukbasi et al. (2016) and Gonen and Goldberg (2019) .",Difference,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
586,4299,8841,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,049f4c438ce9eefa622ae5ba5fb7e34443b86133,"Once the embeddings are trained, the gender protected attribute can be simply removed from the vector representation, therefore eliminating any gender bias present in it. The transformations proposed by both Bolukbasi et al. (2016) and Zhao et al. (2018b) are downstream task-agnostic. This fact is used in the work of Gonen and Goldberg (2019) to showcase that, while apparently the embedding information is removed, there is still gender information remaining in the vector representations.",Motivation,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
587,4300,8842,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Similarly to Bolukbasi et al. (2016) , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings. We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.",Background,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
588,4301,8843,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"To perform our analysis we used a set of lists from previous work (Bolukbasi et al., 2016; Gonen and Goldberg, 2019) . We refer to the list of definitional pairs 2 as 'Definitonal List' (e.g. shehe, girl-boy). We refer to the list of female and male professions 3 as 'Professional List' (e.g. accountant, surgeon). The 'Biased List' is the list used in the clustering experiment and it consists of biased male and female words (500 female biased tokens and 500 male biased token). This list is generated by taking the most biased words, where the bias of a word is computed by taking its projection on the gender direction ( − → he-−→ she) (e.g. breastfeeding, bridal and diet for female and hero, cigar and teammates for male). The 'Extended Biased List' is the list used in classification experiment , which contains 5000 male and female biased tokens, 2500 for each gender, generated in the same way of the Biased List 4 .",Uses,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,1,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
589,4302,8844,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Human-generated corpora suffer from social biases. Those biases are reflected in the cooccurrence statistics, and therefore learned into word embeddings trained in those corpora, amplifying them (Bolukbasi et al., 2016; Caliskan et al., 2017) .",Motivation,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
590,4303,8845,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,049f4c438ce9eefa622ae5ba5fb7e34443b86133,"A note to be considered, is that the lists we used in our experiments (and obtained from Bolukbasi et al. (2016) and Gonen and Goldberg (2019) ) may contain words that are missing in our corpus and so we can not obtain contextualized embeddings for them.",Uses,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",train,1,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
591,4304,8846,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"Similarly to Bolukbasi et al. (2016) , figure 1 shows that the first eigenvalue is significantly larger than the rest and that there is also a single direction describing the majority of variance in these vectors, still the difference between the percentage of variances is less in case of contextualized embeddings, which may refer that there is less bias in such embeddings. We can easily note the difference in the case of random, where there is a smooth and gradual decrease in eigenvalues, and hence the variance percentage.",Similar,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
592,4305,8847,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"We applied the definition of direct bias from Bolukbasi et al. (2016) on the ELMo representations of the professional words in these sentences. where N is the amount of gender neutral words, g the gender direction, and w the word vector of each profession.",Similar,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
593,4306,8848,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,ccf6a69a7f33bcf052aa7def176d3b9de495beb7,"We got direct bias of 0.03, compared to 0.08 from standard word2vec embeddings described in Bolukbasi et al. (2016) . This reduction on the direct bias confirms that the substantial component along the gender direction that is present in standard word embeddings is less for the contextualized word embeddings. Probably, this reduction comes from the fact that we are using different word embeddings for the same profession depending on the sentence which is a direct consequence and advantage of using contextualized embeddings.",Difference,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",train,0,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
594,4307,8850,ABC_d70e69bb3eaa6b46ee3b7110126129_6,ACL:W19-3805,049f4c438ce9eefa622ae5ba5fb7e34443b86133,"In this section, we adapt gender bias measures for word embedding methods from previous work (Bolukbasi et al., 2016) and (Gonen and Goldberg, 2019) to be applicable to contextualized word embeddings.",Extention,s2,"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in. In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","Word embeddings are widely used in NLP for a vast range of tasks. It was shown that word embeddings derived from text corpora reflect gender biases in society. This phenomenon is pervasive and consistent across different word embedding models, causing serious concern. Several recent works tackle this problem, and propose methods for significantly reducing this gender bias in word embeddings, demonstrating convincing results. However, we argue that this removal is superficial. While the bias is indeed substantially reduced according to the provided bias definition, the actual effect is mostly hiding the bias, not removing it. The gender bias information is still reflected in the distances between “gender-neutralized” words in the debiased embeddings, and can be recovered from them. We present a series of experiments to support this claim, for two debiasing methods. We conclude that existing bias removal techniques are insufficient, and should not be trusted for providing gender-neutral modeling.",train,1,"The research problem is the presence of gender bias in natural language processing applications, specifically how word embeddings perpetuate and amplify these biases. The motivation for the research is to understand whether contextualized word embeddings, a recent advancement in word embedding techniques, can mitigate gender bias compared to standard word embeddings.",test,"bias, contextualized word embeddings, contextualized word embeddings, word embedding computation, word embedding techniques, word vector representations"
595,4944,10308,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"• Knowledge Base (KB): The knowledge Base <span style=""background: yellow; display: inline-block"">[2]</span> mainly used to maintain the previous knowledge.",Background,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
596,4945,10309,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"LSC <span style=""background: yellow; display: inline-block"">[2]</span> discussed a possible solution of P(w |c j ).",Background,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
597,4946,10310,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Zhiyuan Chen <span style=""background: yellow; display: inline-block"">[2]</span> ever proposed a approach to determine which domain dose a word have the sentiment orientation to achieve the goal of lifelong learning. He made a big progress but the supervised learning still is needed.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
598,4947,10311,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Previous classical paper <span style=""background: yellow; display: inline-block"">[2]</span> chose the sentiment classification as the learning target because it is could be regarded as a task as well as a group of subtasks in different domain. These sub-tasks related to each other but a model trained on a domain is unable to perform well in rest domains.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
599,4948,10312,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Although LSC <span style=""background: yellow; display: inline-block"">[2]</span> already raised a lifelong approach, it only aims to improve the classification accuracy.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
600,4949,10313,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Although LSC <span style=""background: yellow; display: inline-block"">[2]</span> considered the difference among domains, it still is a typical supervised learning approach.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
601,4950,10314,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Use use the same formula as LSC <span style=""background: yellow; display: inline-block"">[2]</span> used below.",Uses,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,1,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
602,4951,10315,ABC_e3ee86bbaca6ae00906e7ec64f0ac0_12,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"In the experiment, we use the same datasets as LSC <span style=""background: yellow; display: inline-block"">[2]</span> used.",Uses,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",train,1,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
603,5163,10702,ABC_7d7895690c84fb1af46c30f858470e_42,ARXIV:1901.04936,3c78c6df5eb1695b6a399e346dde880af27d1016,"We present extensions to the DocQA <span style=""background: yellow; display: inline-block"">[2]</span> model to allow incremental reading without loss of accuracy.",Extention,llm,"Any system which performs goal-directed continual learning must not only learn incrementally but process and absorb information incrementally. Such a system also has to understand when its goals have been achieved. In this paper, we consider these issues in the context of question answering. Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.","We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.",train,1,"The research problem is developing question answering systems that can perform goal-directed continual learning by incrementally learning, processing information, and recognizing goal achievement. The research is motivated by the limitations of current question answering models that rely on reasoning over entire passages instead of incrementally. The researchers aim to overcome the inadequacy of naive incremental reading approaches by developing a more robust and accurate solution.",test,"DocQA [ 2 ] model, goal-directed continual learning, incremental reading, incremental reading, question answering models, unidirectional language models"
604,5164,10704,ABC_7d7895690c84fb1af46c30f858470e_42,ARXIV:1901.04936,3c78c6df5eb1695b6a399e346dde880af27d1016,"We introduce a new incremental model based on DocQA <span style=""background: yellow; display: inline-block"">[2]</span> , which is an RNN based model proposed for QA.",Extention,llm,"Any system which performs goal-directed continual learning must not only learn incrementally but process and absorb information incrementally. Such a system also has to understand when its goals have been achieved. In this paper, we consider these issues in the context of question answering. Current state-of-the-art question answering models reason over an entire passage, not incrementally. As we will show, naive approaches to incremental reading, such as restriction to unidirectional language models in the model, perform poorly. We present extensions to the DocQA [2] model to allow incremental reading without loss of accuracy. The model also jointly learns to provide the best answer given the text that is seen so far and predict whether this best-so-far answer is sufficient.","We introduce a method of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Most current question answering models cannot scale to document or multi-document input, and naively applying these models to each paragraph independently often results in them being distracted by irrelevant text. We show that it is possible to significantly improve performance by using a modified training scheme that teaches the model to ignore non-answer containing paragraphs. Our method involves sampling multiple paragraphs from each document, and using an objective function that requires the model to produce globally correct output. We additionally identify and improve upon a number of other design decisions that arise when working with document-level data. Experiments on TriviaQA and SQuAD shows our method advances the state of the art, including a 10 point gain on TriviaQA.",train,1,"The research problem is developing question answering systems that can perform goal-directed continual learning by incrementally learning, processing information, and recognizing goal achievement. The research is motivated by the limitations of current question answering models that rely on reasoning over entire passages instead of incrementally. The researchers aim to overcome the inadequacy of naive incremental reading approaches by developing a more robust and accurate solution.",test,"DocQA [ 2 ] model, goal-directed continual learning, incremental reading, incremental reading, question answering models, unidirectional language models"
605,200,1271,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"Seminal work from [5] presents a comparative style analysis of hyperpartisan news, evaluating features such as characters n-grams, stop words, part-of-speech, readability scores, and ratios of quoted words and external links.",Background,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,0,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
606,201,1275,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"We used the BuzzedFeed-Webis Fake News Corpus 2016 collected by [5] whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ).",Uses,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
607,202,1276,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"In order to compare our results with those reported in [5] , we also used accuracy, precision, and recall.",Uses,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
608,203,1277,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"We build upon previous work and use the dataset from [5] : this way we can investigate hyperpartisan-biased news (i.e., extremely one-sided) that have been manually fact-checked by professional journalists from BuzzFeed.",Uses,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
609,204,1278,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"4 Following the settings of [5] , we balance the training set using random duplicate oversampling.",Uses,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
610,205,1279,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,We compare with [5] against their topic and style-based methods.,Uses,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
611,206,1280,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,Evaluation: We performed 3-fold cross-validation with the same configuration used in [5] .,Uses,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
612,207,1281,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"However, perhaps surprisingly, we are able to achieve the overall best performance by simply using higher-length n-grams than those used in the original work from [5] : this seems to indicate a strong lexical overlap between different sources with the same orientation, which, in turn, calls for more challenging datasets and task formulations to encourage the development of models covering more subtle, i.e., implicit, forms of bias.",Extention,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
613,208,1282,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"In order to compare our results with those reported in [5] , we report the same measures the authors used.",Uses,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
614,209,1283,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"We used the BuzzedFeed-Webis Fake News Corpus 2016 collected by [5] whose articles were labeled with respect to three political orientations: mainstream, left-wing, and right-wing (see Table 2 ). During initial data analysis and prototyping we identified a variety of issues with the original dataset: we cleaned the data excluding articles with empty or bogus texts, e.g. 'The document has moved here' (23 and 14 articles respectively).",Extention,s2,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
615,525,2577,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"Recent work demonstrated that word embeddings induced from large text collections encode many human biases (e.g., Bolukbasi et al., 2016; Caliskan et al., 2017) .",Background,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
616,526,2578,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"In order to measure the extent to which various societal biases are captured by word embeddings, Caliskan et al. (2017) proposed the Word Embedding Association Test (WEAT).",Background,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
617,527,2579,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"The Word Embedding Association Test (WEAT) (Caliskan et al., 2017) is an adaptation of the Implicit Association Test (IAT) (Nosek et al., 2002) .",Background,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
618,528,2580,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"Recent work demonstrated that word embeddings induced from large text collections encode many human biases (e.g., Bolukbasi et al., 2016; Caliskan et al., 2017) .",Motivation,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
619,529,2581,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"The tested statistic is the difference between X and Y in average similarity of their terms with terms from A and B: with association difference for term t computed as: where t is the distributional vector of term t and f is a similarity or distance metric, fixed to cosine similarity in the original work (Caliskan et al., 2017) .",Background,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
620,530,2582,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"We first introduce the WEAT dataset (Caliskan et al., 2017) and then describe XWEAT, our multilingual and cross-lingual extension of WEAT designed for comparative bias analyses across languages and in cross-lingual embedding spaces.",Uses,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,1,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
621,531,2583,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"We first describe the WEAT framework (Caliskan et al., 2017) .",Uses,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,1,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
622,532,2584,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"To this end, we have extended previous analyses based on the WEAT test (Caliskan et al., 2017; McCurdy and Serbetci, 2017) in multiple dimensions: across seven languages, four embedding models, and three different types of text.",Uses,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,1,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
623,533,2585,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"To this end, we have extended previous analyses based on the WEAT test (Caliskan et al., 2017; McCurdy and Serbetci, 2017) in multiple dimensions: across seven languages, four embedding models, and three different types of text.",Extention,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,1,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
624,534,2586,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,5 This is consistent with the original results obtained by Caliskan et al. (2017) .,Similar,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,0,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
625,535,2587,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"We adopt the general bias-testing framework from Caliskan et al. (2017) , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora.",Uses,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,1,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
626,536,2588,ABC_de9eb9b7dff69743252b3ff0ef8894_14,ACL:S19-1010,5966d7c7f60898d610812e24c64d4d57855ad86a,"We adopt the general bias-testing framework from Caliskan et al. (2017) , but we span our study over multiple dimensions: (1) corpora -we analyze the consistency of biases across distributional vectors induced from different types of text; (2) embedding models -we compare biases across distributional vectors induced by different embedding models (on the same corpora); and (3) languageswe measure biases for word embeddings of different languages, trained from comparable corpora.",Extention,s2,"Word embeddings have recently been shown to reflect many of the pronounced societal biases (e.g., gender bias or racial bias). Existing studies are, however, limited in scope and do not investigate the consistency of biases across relevant dimensions like embedding models, types of texts, and different languages. In this work, we present a systematic study of biases encoded in distributional word vector spaces: we analyze how consistent the bias effects are across languages, corpora, and embedding models. Furthermore, we analyze the cross-lingual biases encoded in bilingual embedding spaces, indicative of the effects of bias transfer encompassed in cross-lingual transfer of NLP models. Our study yields some unexpected findings, e.g., that biases can be emphasized or downplayed by different embedding models or that user-generated content may be less biased than encyclopedic text. We hope our work catalyzes bias research in NLP and informs the development of bias reduction techniques.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",dev,1,"The research problem is the existence of societal biases (e.g., gender and racial bias) within word embeddings and the lack of understanding about the consistency of these biases across different embedding models, text types, and languages. The research is motivated by the desire to advance bias research in NLP and inform the development of techniques to reduce bias in word embeddings.",test,"bias reduction techniques, encyclopedic text, user-generated content"
627,689,3216,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"Recently, Yang et al. (2019) showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of Yang et al. (2019) represents a simple method to combining BERT with off-the-shelf IR.",Background,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,0,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
628,690,3218,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"Experiments show that, using the same reader model as Yang et al. (2019) , our simple data-augmentation techniques yield additional large improvements.",Uses,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,1,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
629,691,3219,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"Following Yang et al. (2019) , to evaluate answers in an end-to-end setup, we disregard the paragraph context from the original datasets and use only the answer spans.",Uses,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,1,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
630,692,3220,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"Recently, Yang et al. (2019) showed that combining a BERT-based reader with passage retrieval using the Anserini IR toolkit yields a large improvement in question answering directly from a Wikipedia corpus, measured in terms of exact match on a standard benchmark (Chen et al., 2017) . Interestingly, the approach of Yang et al. (2019) represents a simple method to combining BERT with off-the-shelf IR. In this paper, we build on these initial successes to explore how much further we can push this simple architecture by data augmentation, taking advantage of distant supervision techniques to gather more and higher-quality * equal contribution training data to fine tune BERT.",Extention,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,1,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
631,693,3221,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"We use the same exact setup as the ""paragraph"" variant of BERTserini (Yang et al., 2019) , where the input corpus is pre-segmented into paragraphs at index time, each of which is treated as a ""document"" for retrieval purposes.",Uses,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,1,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
632,694,3222,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"One major shortcoming with BERTserini is that Yang et al. (2019) only fine tune on SQuAD, which means that the BERT reader is exposed to an impoverished set of examples; all SQuAD data come from a total of only 442 documents.",Difference,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,0,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
633,695,3223,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"The row marked ""SRC"" indicates fine tuning with SQuAD data only and matches the BERTserini condition of Yang et al. (2019) ; we report higher scores due to engineering improvements (primarily a Lucene version upgrade).",Difference,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,0,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
634,696,3224,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"In addition, we compute recall (R), the fraction of questions for which the correct answer appears in any retrieved paragraph; to make our results comparable to Yang et al. (2019) , Anserini returns the top k = 100 paragraphs to feed into the BERT reader.",Similar,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,0,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
635,697,3227,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"Experiments show that, using the same reader model as Yang et al. (2019) , our simple data-augmentation techniques yield additional large improvements.",Difference,s2,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,0,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
636,840,3763,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,"Therefore, in conjunction with this task, we present the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019) .",Similar,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,0,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
637,841,3764,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,OLID was annotated using a hierarchical three-level annotation model introduced in Zampieri et al. (2019) .,Similar,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,0,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
638,842,3765,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on Zampieri et al. (2019) .,Similar,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,0,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
639,843,3766,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,"While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID (Zampieri et al., 2019) and used in OffensEval aims to capture this.",Similar,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,0,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
640,844,3767,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,13ff28f98474858d84c339e5301cb3bf468ac68b,A detailed description of the data collection process and annotation is presented in Zampieri et al. (2019) .,Similar,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.",dev,0,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
641,845,3768,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,"In OffensEval we used OLID (Zampieri et al., 2019) , a dataset containing English tweets annotated with a hierarchical three-layer annotation model which considers 1) whether a message is offensive or not (sub-task A); 2) what is the type of the offensive 7 In the camera-ready version of this report we will be including a Table with references to all system descriptions papers.",Similar,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,0,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
642,846,3769,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,"In OffensEval 1 we use OLID (Zampieri et al., 2019) and propose one sub-task for each layer of annotation as presented in Section 3.",Difference,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,0,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
643,847,3770,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,"In OffensEval 1 we use OLID (Zampieri et al., 2019) and propose one sub-task for each layer of annotation as presented in Section 3.",Extention,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,1,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
644,848,3771,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,The remainder of this paper is organized as follows: Section 3 presents the shared task description and the sub-tasks included in OffensEval and Section 4 includes a brief description of OLID based on Zampieri et al. (2019) .,Uses,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,1,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
645,849,3772,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,"While each of these sub-tasks tackle a particular type of abuse or offense, they share similar properties and the hierarchical annotation model pro-posed proposed in OLID (Zampieri et al., 2019) and used in OffensEval aims to capture this.",Uses,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,1,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
646,850,3773,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,OLID was annotated using a hierarchical three-level annotation model introduced in Zampieri et al. (2019) .,Uses,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,1,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
647,851,3774,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,13ff28f98474858d84c339e5301cb3bf468ac68b,A detailed description of the data collection process and annotation is presented in Zampieri et al. (2019) .,Uses,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID.",dev,1,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
648,853,3776,ABC_1ffadfc2d4961beeb1621502298a70_21,ACL:S19-2010,e7bdbba18f557b5a1793692c8e3739cbe6cae15d,"In OffensEval we used OLID (Zampieri et al., 2019) , a dataset containing English tweets annotated with a hierarchical three-layer annotation model which considers 1) whether a message is offensive or not (sub-task A); 2) what is the type of the offensive 7 In the camera-ready version of this report we will be including a Table with references to all system descriptions papers.",Uses,s2,"We present the results and the main findings of SemEval-2019 Task 6 on Identifying and Categorizing Offensive Language in Social Media (OffensEval). The task was based on a new dataset, the Offensive Language Identification Dataset (OLID), which contains over 14,000 English tweets, and it featured three sub-tasks. In sub-task A, systems were asked to discriminate between offensive and non-offensive posts. In sub-task B, systems had to identify the type of offensive content in the post. Finally, in sub-task C, systems had to detect the target of the offensive posts. OffensEval attracted a large number of participants and it was one of the most popular tasks in SemEval-2019. In total, nearly 800 teams signed up to participate in the task and 115 of them submitted results, which are presented and analyzed in this report.","In this study we deal with the problem of identifying and categorizing offensive language in social media. Our group, BNU-HKBU UIC NLP Team2, use supervised classification along with multiple version of data generated by different ways of pre-processing the data. We then use the state-of-the-art model Bidirectional Encoder Representations from Transformers, or BERT (Devlin et al, 2018), to capture linguistic, syntactic and semantic features. Long range dependencies between each part of a sentence can be captured by BERT’s bidirectional encoder representations. Our results show 85.12% accuracy and 80.57% F1 scores in Subtask A (offensive language identification), 87.92% accuracy and 50% F1 scores in Subtask B (categorization of offense types), and 69.95% accuracy and 50.47% F1 score in Subtask C (offense target identification). Analysis of the results shows that distinguishing between targeted and untargeted offensive language is not a simple task. More work needs to be done on the unbalance data problem in Subtasks B and C. Some future work is also discussed.",dev,1,"The research problem is to identify and categorize offensive language in social media. The study focuses on the OffensEval task, which involved classifying offensive and non-offensive posts, identifying the type of offensive content, and detecting the target of offensive posts. The motivation for this research is to analyze the results of the popular OffensEval task at SemEval-2019, which attracted a large number of participants. The research aims to provide insight into the approaches and performance of systems designed to identify and categorize offensive language in social media.",test,Offensive Language Identification Dataset ( OLID )
649,1061,4536,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,3b2a962f977a081637fd683c1dc9582e12b344dd,"Our study focuses on racial bias in hate speech and abusive language detection datasets (Waseem, 2016; Waseem and Hovy, 2016; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter.",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","A fundamental part of conducting cross-disciplinary web science research is having useful, high-quality datasets that provide value to studies across disciplines. In this paper, we introduce a large, hand-coded corpus of online harassment data. A team of researchers collaboratively developed a codebook using grounded theory and labeled 35,000 tweets. Our resulting dataset has roughly 15% positive harassment examples and 85% negative examples. This data is useful for training machine learning models, identifying textual and linguistic features of online harassment, and for studying the nature of harassing comments and the culture of trolling.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
650,1062,4537,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,3b2a962f977a081637fd683c1dc9582e12b344dd,"Our study focuses on racial bias in hate speech and abusive language detection datasets (Waseem, 2016; Waseem and Hovy, 2016; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter.",Motivation,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","A fundamental part of conducting cross-disciplinary web science research is having useful, high-quality datasets that provide value to studies across disciplines. In this paper, we introduce a large, hand-coded corpus of online harassment data. A team of researchers collaboratively developed a codebook using grounded theory and labeled 35,000 tweets. Our resulting dataset has roughly 15% positive harassment examples and 85% negative examples. This data is useful for training machine learning models, identifying textual and linguistic features of online harassment, and for studying the nature of harassing comments and the culture of trolling.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
651,1063,4538,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,df704cca917666dace4e42b4d3a50f65597b8f06,Waseem and Hovy (2016) collected 130k tweets containing one of seventeen different terms or phrases they considered to be hateful.,Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lomas, 2015). We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
652,1064,4539,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,df704cca917666dace4e42b4d3a50f65597b8f06,"The Waseem and Hovy (2016) classifier is particularly sensitive to the word ""b*tch"" with 96% of black-aligned and 94% of white-aligned tweets predicted to belong to this class.",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lomas, 2015). We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
653,1065,4540,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,3b2a962f977a081637fd683c1dc9582e12b344dd,"The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017; Waseem and Hovy, 2016) and crowdworkers Founta et al., 2018) to activists (Waseem, 2016) .",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","A fundamental part of conducting cross-disciplinary web science research is having useful, high-quality datasets that provide value to studies across disciplines. In this paper, we introduce a large, hand-coded corpus of online harassment data. A team of researchers collaboratively developed a codebook using grounded theory and labeled 35,000 tweets. Our resulting dataset has roughly 15% positive harassment examples and 85% negative examples. This data is useful for training machine learning models, identifying textual and linguistic features of online harassment, and for studying the nature of harassing comments and the culture of trolling.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
654,1066,4541,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,903acbc89ed6788907ac8ee4bc3f2b234d78160e,"Some studies sampled tweets using small, ad hoc sets of keywords created by the authors (Waseem and Hovy, 2016; Waseem, 2016; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) .",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","The (unheralded) first step in many applications of automated text analysis involves selecting keywords to choose documents from a large text corpus for further study. Although all substantive results depend on this choice, researchers usually pick keywords in ad hoc ways that are far from optimal and usually biased. Most seem to think that keyword selection is easy, since they do Google searches every day, but we demonstrate that humans perform exceedingly poorly at this basic task. We offer a better approach, one that also can help with following conversations where participants rapidly innovate language to evade authorities, seek political advantage, or express creativity; generic web searching; eDiscovery; look-alike modeling; industry and intelligence analysis; and sentiment and topic analysis. We develop a computer-assisted (as opposed to fully automated or human-only) statistical approach that suggests keywords from available text without needing structured data as inputs. This framing poses the statistical problem in a new way, which leads to a widely applicable algorithm. Our specific approach is based on training classifiers, extracting information from (rather than correcting) their mistakes, and summarizing results with easy-to-understand Boolean search strings. We illustrate how the technique works with analyses of English texts about the Boston Marathon bombings, Chinese social media posts designed to evade censorship, and others.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
655,1067,4542,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,"Table 3 shows that for tweets containing the word ""n*gga"", classifiers trained on Waseem and Hovy (2016) and Waseem (2016) are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.",Similar,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
656,1068,4543,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,df704cca917666dace4e42b4d3a50f65597b8f06,"In all but one of the comparisons, there are statistically significant (p < 0.001) differences and in all but one of these we see that tweets in the black-aligned corpus are assigned negative labels more frequently than those by whites. The only case where blackaligned tweets are classified into a negative class less frequently than white-aligned tweets is the racism class in the Waseem and Hovy (2016) classifier.",Difference,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racist and sexist remarks are a common occurrence on social media. For that reason, many social media services address the problem of identifying hate speech, but the definition of hate speech varies markedly and is largely a manual effort (BBC, 2015; Lomas, 2015). We provide a list of criteria founded in critical race theory, and use them to annotate a publicly available corpus of more than 16k tweets. We analyze the impact of various extra-linguistic features in conjunction with character n-grams for hatespeech detection. We also present a dictionary based the most indicative words in our data.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
657,1069,4544,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,"Table 3 shows that for tweets containing the word ""n*gga"", classifiers trained on Waseem and Hovy (2016) and Waseem (2016) are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.",Uses,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",dev,1,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
658,1070,4545,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,Classifiers trained on data from Waseem and Hovy (2016) and Waseem (2016) only predicted a small fraction of the tweets to be racism.,Difference,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
659,1071,4546,ABC_60c1245eff625441383913f947a8b1_25,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,We see similar results for Waseem and Hovy (2016) and Waseem (2016) .,Similar,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",dev,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
660,1670,7421,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"The starting point for our investigation is the recent work of Kottur et al. (2017) , which investigates compositionality using a cooperative reference game between two agents.",Uses,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,1,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
661,1671,7422,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"The starting point of our study is the goal oriented dialogue task of Kottur et al. (2017) , summarized in Fig. 2 .",Uses,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,1,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
662,1672,7423,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,4814c10f84863e016d75e6af42e790f60759b9f4,"This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents (Kottur et al., 2017; Mordatch & Abbeel, 2018; Choi et al., 2018) .",Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","One of the distinguishing aspects of human language is its compositionality, which allows us to describe complex environments with limited vocabulary. Previously, it has been shown that neural network agents can learn to communicate in a highly structured, possibly compositional language based on disentangled input (e.g. hand- engineered features). Humans, however, do not learn to communicate based on well-summarized features. In this work, we train neural agents to simultaneously develop visual perception from raw image pixels, and learn to communicate with a sequence of discrete symbols. The agents play an image description game where the image contains factors such as colors and shapes. We train the agents using the obverter technique where an agent introspects to generate messages that maximize its own understanding. Through qualitative analysis, visualization and a zero-shot test, we show that the agents can develop, out of raw image pixels, a language with compositional properties, given a proper pressure from the environment.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
663,1673,7424,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018; Kottur et al., 2017) , but it only investigates how language changes within a generation.",Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
664,1674,7425,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,In Kottur et al. (2017) it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents,Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
665,1676,7427,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Previous work also measures generalization to held out compositions of attributes to measure compositionality (Kottur et al., 2017; Kirby et al., 2015) .",Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
666,1677,7428,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,In Kottur et al. (2017) there is only one pair of agents (N Q = N A = 1) so we cannot replace both agents at the same round because all existing language would be lost.,Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
667,1678,7429,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"The models considered in Kottur et al. (2017) were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete.",Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
668,1679,7430,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Both Mordatch & Abbeel (2018) and Kottur et al. (2017) find that limiting the vocabulary size so that there aren't too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000) .",Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
669,1680,7431,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018; Kottur et al., 2017) , but it only investigates how language changes within a generation.",Motivation,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
670,1681,7432,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018; Kottur et al., 2017) .",Background,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
671,1682,7433,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"As in Kottur et al. (2017) , our world contains objects with 3 attributes (shape, size, color) such that each attribute has 4 possible values.",Similar,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
672,1683,7434,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Our A-bots and Q-bots have the same architecture and hyperparameter variations as in Kottur et al. (2017) , but with our cultural transmission training procedure and some other differences identified below.",Similar,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
673,1684,7435,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"As in Kottur et al. (2017) , we implement Q, A, and U as neural networks.",Similar,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
674,1685,7436,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Like Kottur et al. (2017) , our hyperparameter variations consider the number of vocab words Q-bot (V Q ) and A-bot (V A ) may utter and whether or not A-bot has memory between dialog rounds.",Similar,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
675,1686,7437,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"The models considered in Kottur et al. (2017) were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller-mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings.",Similar,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
676,1687,7438,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"This agrees with factors noted elsewhere (Kottur et al., 2017; Mordatch & Abbeel, 2018; Nowak et al., 2000) .",Similar,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
677,1688,7439,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Unlike Kottur et al. (2017) , we use a slightly harder version of their dataset which aligns better with the goal of compositional language.",Difference,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
678,1689,7440,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"7 This differs from Kottur et al. (2017) , which stopped once train accuracy reached 100%.",Difference,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
679,1690,7441,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Test set accuracies (with standard deviations) are reported against our new harder dataset using models similar to those in (Kottur et al., 2017) .",Similar,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
680,1691,7444,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Our A-bots and Q-bots have the same architecture and hyperparameter variations as in Kottur et al. (2017) , but with our cultural transmission training procedure and some other differences identified below.",Difference,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
681,1692,7445,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"Thus we compare to the Replace All baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the No Replacement baseline 6 This is slightly different from Small Vocab in (Kottur et al., 2017) .",Difference,s2,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,0,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
682,2271,10179,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"The last two rows show the results obtained by applying the system from <span style=""background: yellow; display: inline-block"">[5]</span> 6 to our cleaned dataset (Section 3).",Uses,llm,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,1,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
683,2272,10180,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"In line with what was already pointed out in <span style=""background: yellow; display: inline-block"">[5]</span> , the left-wing orientation is harder to predict, possibly because this class is represented with fewer examples in the dataset.",Similar,llm,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,0,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
684,2273,10181,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"However, the differences between the results of the two evaluated approaches are much higher (0.66 vs. 0.57 according to Macro F 1 ) than those shown in <span style=""background: yellow; display: inline-block"">[5]</span> .",Difference,llm,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,0,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
685,2274,10182,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"Similar to <span style=""background: yellow; display: inline-block"">[5]</span> , the topic-based model achieves better results than the style-related model.",Similar,llm,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,0,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
686,2275,10183,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"In fact, comparing the results of <span style=""background: yellow; display: inline-block"">[5]</span> against our baseline model, it is possible to note that even without masking any word, the classifier obtains better results.",Difference,llm,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,0,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
687,2276,10184,ABC_55bcdca5052745160dc861e22e7401_7,ARXIV:1906.04836,ed31e1225f6a76b469dfe4d022b235dc70be4390,"These results confirm that perhaps the performance of our approach overcomes the models proposed in <span style=""background: yellow; display: inline-block"">[5]</span> because of the length of the n-grams 7 .",Difference,llm,incorrect id format for ARXIV:1906.04836,"We report on a comparative style analysis of hyperpartisan (extremely one-sided) news and fake news. A corpus of 1,627 articles from 9 political publishers, three each from the mainstream, the hyperpartisan left, and the hyperpartisan right, have been fact-checked by professional journalists at BuzzFeed: 97% of the 299 fake news articles identified are also hyperpartisan. We show how a style analysis can distinguish hyperpartisan news from the mainstream (F1 = 0.78), and satire from both (F1 = 0.81). But stylometry is no silver bullet as style-based fake news detection does not work (F1 = 0.46). We further reveal that left-wing and right-wing news share significantly more stylistic similarities than either does with the mainstream. This result is robust: it has been confirmed by three different modeling approaches, one of which employs Unmasking in a novel way. Applications of our results include partisanship detection and pre-screening for semi-automatic fake news detection.",dev,0,"The main research problem is the detection of hyperpartisanship in news, focusing on disentangling the impact of stylistic features versus content-related features. The research is motivated by the need to better understand and detect hyperpartisanship in news, particularly by exploring the relative importance of style and content.  The authors also highlight the limitations of current approaches and datasets in capturing more subtle, implicit forms of bias, suggesting a need for more challenging benchmarks.",test,"approaches, content-related features, datasets, detection of hyperpartisanship, hyperpartisanship, news, stylistic features"
688,2362,10404,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"BERTserini<span style=""background: yellow; display: inline-block""> (Yang et al., 2019)</span> 38.6 46.1 85.9 SRC 41.8 49.5 85.9 DS(+) 44.0 51.4 85.9 DS(±) 48.7 56.5 85.9 SRC + DS(±) 45.7 53.5 85.9 DS(±) → SRC 47.4 55.0 85.9 SRC → DS(±) 50.2 58.2 85.9 Table 2 : Results on SQuAD helps, an even larger boost comes from leveraging negative examples using DS(±). In both these cases, we only fine tune BERT with the augmented data, ignoring the source data.",Uses,llm,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,1,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
689,2363,10405,ABC_f633ceffdf53849159574a2891eda1_17,ARXIV:1904.06652,2fe7dba5a58aee5156594b4d78634ecd6c7dcabd,"BERTserini<span style=""background: yellow; display: inline-block""> (Yang et al., 2019)</span> 38.6 46.1 85.9 SRC 41.8 49.5 85.9 DS(+) 44.0 51.4 85.9 DS(±) 48.7 56.5 85.9 SRC + DS(±) 45.7 53.5 85.9 DS(±) → SRC 47.4 55.0 85.9 SRC → DS(±) 50.2 58.2 85.9 Table 2 : Results on SQuAD helps, an even larger boost comes from leveraging negative examples using DS(±).",Unsure,llm,"Recently, a simple combination of passage retrieval using off-the-shelf IR techniques and a BERT reader was found to be very effective for question answering directly on Wikipedia, yielding a large improvement over the previous state of the art on a standard benchmark dataset. In this paper, we present a data augmentation technique using distant supervision that exploits positive as well as negative examples. We apply a stage-wise approach to fine tuning BERT on multiple datasets, starting with data that is ""furthest"" from the test data and ending with the ""closest"". Experimental results show large gains in effectiveness over previous approaches on English QA datasets, and we establish new baselines on two recent Chinese QA datasets.","We demonstrate an end-to-end question answering system that integrates BERT with the open-source Anserini information retrieval toolkit. In contrast to most question answering and reading comprehension models today, which operate over small amounts of input text, our system integrates best practices from IR with a BERT-based reader to identify answers from a large corpus of Wikipedia articles in an end-to-end fashion. We report large improvements over previous results on a standard benchmark test collection, showing that fine-tuning pretrained BERT with SQuAD is sufficient to achieve high accuracy in identifying answer spans.",dev,0,"The research problem is to improve the effectiveness of question answering (QA) systems, specifically on Wikipedia, aiming to surpass the existing state-of-the-art performance on standard benchmark datasets. The research is motivated by the desire to enhance the effectiveness of question answering systems by leveraging a novel data augmentation technique and a stage-wise BERT fine-tuning strategy.",test,"benchmark dataset, Chinese QA datasets, data augmentation technique, English QA datasets, fine tuning BERT, IR techniques, stage-wise approach"
690,2434,10892,ABC_50cdfe539f84d793ec50873b5ab066_1,ARXIV:1904.09067,2caa021d85d4878d3369000e0068f617576d6cca,"This approach-summarized in the black lines (4-9) of Algorithm 1-is our starting point. In <span style=""background: yellow; display: inline-block"">Kottur et al. (2017)</span> it was used to generate a somewhat compositional language given Algorithm 1: Training with Replacement and Multiple Agents",Uses,llm,"Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional-i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.","A number of recent works have proposed techniques for end-to-end learning of communication protocols among cooperative multi-agent populations, and have simultaneously found the emergence of grounded human-interpretable language in the protocols developed by the agents, learned without any human supervision! In this paper, using a Task & Talk reference game between two agents as a testbed, we present a sequence of ‘negative’ results culminating in a ‘positive’ one – showing that while most agent-invented languages are effective (i.e. achieve near-perfect task rewards), they are decidedly not interpretable or compositional. In essence, we find that natural language does not emerge ‘naturally’,despite the semblance of ease of natural-language-emergence that one may gather from recent literature. We discuss how it is possible to coax the invented languages to become more and more human-like and compositional by increasing restrictions on how two agents may communicate.",dev,1,"The research problem is understanding how cultural transmission influences the emergence of compositional language in collaborative tasks, particularly when agents are using deep reinforcement learning.  Specifically, the research aims to bridge the gap between the insights from evolutionary linguistics and deep learning models, which have previously focused on structural priors rather than cultural dynamics. The research is motivated by the desire to incorporate cultural evolutionary dynamics, specifically cultural transmission, into models of language emergence. This is motivated by the findings in evolutionary linguistics, which suggest a crucial role of cultural transmission in promoting compositional language.  By integrating these dynamics into deep learning models, the researchers aim to gain a deeper understanding of the factors contributing to the emergence of compositional language and create more realistic and effective models for language acquisition.",test,"collaborative task, compositional generalization, cultural dynamics, cultural evolutionary dynamics, deep reinforcement learning agents, emergence of compositionality, Evolutionary linguists, language emergence, populations of deep agents"
691,273,975,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I also discard the agreement cases involving the verbs is or are in Linzen et al. (2016) and in Gulordava et al. (2018) , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.",Difference,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
692,274,976,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"(Gulordava et al., 2018 ) also consider subject-verb agreement, but in a ""colorless green ideas"" setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues. In particular, in (Linzen et al., 2016) we assess the ability of LSTMs to learn subject-verb agreement patterns in English, and evaluate on naturally occurring wikipedia sentences.",Difference,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
693,275,977,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1,"This differs from Linzen et al. (2016) and Gulordava et al. (2018) by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.",Motivation,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",test,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
694,276,978,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"Gulordava et al. (2018) also start with existing sentences. However, in order to control for the possibillity of the model learning to rely on ""semantic"" selectional-preferences cues rather than syntactic ones, they replace each content word with random words from the same part-ofspeech and inflection. This results in ""coloreless green ideas"" nonce sentences.",Background,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
695,277,979,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"(Gulordava et al., 2018 ) also consider subject-verb agreement, but in a ""colorless green ideas"" setting in which content words in naturally occurring sentences are replaced with random words with the same partof-speech and inflection, thus ensuring a focus on syntax rather than on selectional-preferences based cues.",Background,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
696,278,980,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1,"I use the stimuli provided by (Linzen et al., 2016; Gulordava et al., 2018; Marvin and Linzen, 2018) , but change the experimental protocol to adapt it to the bidirectional nature of the BERT model.",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",test,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
697,279,981,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I similarly discard 680 sentences from (Linzen et al., 2016) where the focus verb or its inflection were one of 108 out-ofvocabulary tokens, 6 and 28 sentence-pairs (8 tokens 7 ) from (Gulordava et al., 2018) .",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
698,280,982,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1,"I adapt the evaluation protocol and stimuli of Linzen et al. (2016) , Gulordava et al. (2018) and Marvin and Linzen (2018) to the bidirectional setting required by BERT, and evaluate the pretrained BERT models (both the LARGE and the BASE models).",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",test,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
699,281,983,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,e3ee61f49cd2639c15c8662a45f1d0c2b83a60c1,"This differs from Linzen et al. (2016) and Gulordava et al. (2018) by considering the entire sentence (excluding the verb) and not just its prefix leading to the verb, and differs from Marvin and Linzen (2018) by conditioning the focus verb on bidirectional context. I use the PyTorch implementation of BERT, with the pre-trained models supplied by Google.",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recently, non-recurrent architectures (convolutional, self-attentional) have outperformed RNNs in neural machine translation. CNNs and self-attentional networks can connect distant words via shorter network paths than RNNs, and it has been speculated that this improves their ability to model long-range dependencies. However, this theoretical argument has not been tested empirically, nor have alternative explanations for their strong performance been explored in-depth. We hypothesize that the strong performance of CNNs and self-attentional networks could also be due to their ability to extract semantic features from the source text, and we evaluate RNNs, CNNs and self-attention networks on two tasks: subject-verb agreement (where capturing long-range dependencies is required) and word sense disambiguation (where semantic feature extraction is required). Our experimental results show that: 1) self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances; 2) self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.",test,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
700,282,984,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I also discard the agreement cases involving the verbs is or are in Linzen et al. (2016) and in Gulordava et al. (2018) , because some of them are copular construction, in which strong agreement hints can be found also on the object following the verb.",Extention,s2,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
701,741,2791,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,3b2a962f977a081637fd683c1dc9582e12b344dd,"Our study focuses on racial bias in hate speech and abusive language detection datasets (Waseem, 2016; Waseem and Hovy, 2016; Golbeck et al., 2017; Founta et al., 2018) , all of which use data collected from Twitter.",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","A fundamental part of conducting cross-disciplinary web science research is having useful, high-quality datasets that provide value to studies across disciplines. In this paper, we introduce a large, hand-coded corpus of online harassment data. A team of researchers collaboratively developed a codebook using grounded theory and labeled 35,000 tweets. Our resulting dataset has roughly 15% positive harassment examples and 85% negative examples. This data is useful for training machine learning models, identifying textual and linguistic features of online harassment, and for studying the nature of harassing comments and the culture of trolling.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
702,742,2792,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,8dd6a2c9c88c9b3465484228c93f4dcc11cfeab9,"While not directly measuring bias, prior work has explored how annotation schemes and the identity of the annotators (Waseem, 2016 ) might be manipulated to help to avoid bias.",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","
 
 A key challenge for automatic hate-speech detection on social media is the separation of hate speech from other instances of offensive language. Lexical detection methods tend to have low precision because they classify all messages containing particular terms as hate speech and previous work using supervised learning has failed to distinguish between the two categories. We used a crowd-sourced hate speech lexicon to collect tweets containing hate speech keywords. We use crowd-sourcing to label a sample of these tweets into three categories: those containing hate speech, only offensive language, and those with neither. We train a multi-class classifier to distinguish between these different categories. Close analysis of the predictions and the errors shows when we can reliably separate hate speech from other offensive language and when this differentiation is more difficult. We find that racist and homophobic tweets are more likely to be classified as hate speech but that sexist tweets are generally classified as offensive. Tweets without explicit hate keywords are also more difficult to classify.
 
",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
703,743,2795,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,"Table 3 shows that for tweets containing the word ""n*gga"", classifiers trained on Waseem and Hovy (2016) and Waseem (2016) are both predict black-aligned tweets to be instances of sexism approximately 1.5 times as often as white-aligned tweets.",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
704,744,2796,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,903acbc89ed6788907ac8ee4bc3f2b234d78160e,"Some studies sampled tweets using small, ad hoc sets of keywords created by the authors (Waseem and Hovy, 2016; Waseem, 2016; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) .",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","The (unheralded) first step in many applications of automated text analysis involves selecting keywords to choose documents from a large text corpus for further study. Although all substantive results depend on this choice, researchers usually pick keywords in ad hoc ways that are far from optimal and usually biased. Most seem to think that keyword selection is easy, since they do Google searches every day, but we demonstrate that humans perform exceedingly poorly at this basic task. We offer a better approach, one that also can help with following conversations where participants rapidly innovate language to evade authorities, seek political advantage, or express creativity; generic web searching; eDiscovery; look-alike modeling; industry and intelligence analysis; and sentiment and topic analysis. We develop a computer-assisted (as opposed to fully automated or human-only) statistical approach that suggests keywords from available text without needing structured data as inputs. This framing poses the statistical problem in a new way, which leads to a widely applicable algorithm. Our specific approach is based on training classifiers, extracting information from (rather than correcting) their mistakes, and summarizing results with easy-to-understand Boolean search strings. We illustrate how the technique works with analyses of English texts about the Boston Marathon bombings, Chinese social media posts designed to evade censorship, and others.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
705,745,2797,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,"For Waseem (2016) we see that there is no significant difference in the estimated rates at which tweets are classified as racist across groups, although the rates remain low.",Similar,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
706,746,2798,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,"To account for potential bias in the previous dataset, Waseem (2016) relabeled 2876 tweets in the dataset, along with a new sample from the tweets originally collected.",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
707,747,2799,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,8ad3bc604adc58c828c30e55e9adef0a81bf7e81,"Classifiers trained on data from Waseem and Hovy (2016) and Waseem (2016) only predicted a small fraction of the tweets to be racism. Looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black-aligned corpus as sexism at a substantially higher rate than those in the white-aligned corpus. Given this result, and the gender biases identified in these data by Park et al. (2018), it not apparent that the purportedly expert annotators were any less biased than amateur annotators (Waseem, 2016) .",Background,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, “You are a good woman” was considered “sexist” when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98% and can be extended to correct model bias in other scenarios.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
708,748,2800,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,903acbc89ed6788907ac8ee4bc3f2b234d78160e,"Some studies sampled tweets using small, ad hoc sets of keywords created by the authors (Waseem and Hovy, 2016; Waseem, 2016; Golbeck et al., 2017) , an approach demonstrated to produce poor results (King et al., 2017) .",Uses,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","The (unheralded) first step in many applications of automated text analysis involves selecting keywords to choose documents from a large text corpus for further study. Although all substantive results depend on this choice, researchers usually pick keywords in ad hoc ways that are far from optimal and usually biased. Most seem to think that keyword selection is easy, since they do Google searches every day, but we demonstrate that humans perform exceedingly poorly at this basic task. We offer a better approach, one that also can help with following conversations where participants rapidly innovate language to evade authorities, seek political advantage, or express creativity; generic web searching; eDiscovery; look-alike modeling; industry and intelligence analysis; and sentiment and topic analysis. We develop a computer-assisted (as opposed to fully automated or human-only) statistical approach that suggests keywords from available text without needing structured data as inputs. This framing poses the statistical problem in a new way, which leads to a widely applicable algorithm. Our specific approach is based on training classifiers, extracting information from (rather than correcting) their mistakes, and summarizing results with easy-to-understand Boolean search strings. We illustrate how the technique works with analyses of English texts about the Boston Marathon bombings, Chinese social media posts designed to evade censorship, and others.",test,1,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
709,749,2801,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,"The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017; Waseem and Hovy, 2016) and crowdworkers Founta et al., 2018) to activists (Waseem, 2016) . Even the classifier trained on expert-labeled data (Waseem, 2016) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets.",Difference,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
710,750,2802,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,We see similar results for Waseem and Hovy (2016) and Waseem (2016) . In both cases the classifiers trained upon their data are still more likely to flag black-aligned tweets as sexism.,Similar,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
711,751,2803,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,3eebb7907a9b94f8d65f969f63b76ff5f643f6d3,"The datasets considered here relied upon a range of different annotators, from the authors (Golbeck et al., 2017; Waseem and Hovy, 2016) and crowdworkers Founta et al., 2018) to activists (Waseem, 2016) . Even the classifier trained on expert-labeled data (Waseem, 2016) flags black-aligned tweets as sexist at almost twice the rate of white-aligned tweets.",Extention,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Hate speech in the form of racism and sexism is commonplace on the internet (Waseem and Hovy, 2016). For this reason, there has been both an academic and an industry interest in detection of hate speech. The volume of data to be reviewed for creating data sets encourages a use of crowd sourcing for the annotation efforts. In this paper, we provide an examination of the inﬂuence of annotator knowledge of hate speech on classiﬁcation models by comparing classiﬁcation results obtained from training on expert and amateur annotations. We provide an evaluation on our own data set and run our models on the data set released by Waseem and Hovy (2016). We ﬁnd that amateur annotators are more likely than expert annotators to label items as hate speech, and that systems trained on expert annotations outperform systems trained on amateur annotations.",test,1,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
712,752,2804,ABC_57e65909baf823ff00a9a10a64fffd_15,ACL:W19-3504,8ad3bc604adc58c828c30e55e9adef0a81bf7e81,"Classifiers trained on data from Waseem and Hovy (2016) and Waseem (2016) only predicted a small fraction of the tweets to be racism. Looking at the sexism class on the other hand, we see that both models were consistently classifying tweets in the black-aligned corpus as sexism at a substantially higher rate than those in the white-aligned corpus. Given this result, and the gender biases identified in these data by Park et al. (2018), it not apparent that the purportedly expert annotators were any less biased than amateur annotators (Waseem, 2016) .",Similar,s2,"Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.","Abusive language detection models tend to have a problem of being biased toward identity words of a certain group of people because of imbalanced training datasets. For example, “You are a good woman” was considered “sexist” when trained on an existing dataset. Such model bias is an obstacle for models to be robust enough for practical use. In this work, we measure them on models trained with different datasets, while analyzing the effect of different pre-trained word embeddings and model architectures. We also experiment with three mitigation methods: (1) debiased word embeddings, (2) gender swap data augmentation, and (3) fine-tuning with a larger corpus. These methods can effectively reduce model bias by 90-98% and can be extended to correct model bias in other scenarios.",test,0,"The research problem is the lack of consideration for potential biases in abusive language detection technologies, specifically racial bias. The study focuses on examining how these biases manifest in classifiers trained on Twitter data. The study is motivated by the concern that existing abusive language detection systems may be biased and have a disproportionate negative impact on certain groups, particularly African-American social media users. The researchers aim to highlight the potential for discrimination and the need for addressing bias in these technologies.",test,"abusive language detection systems, African-American English, African-American English, African-American social media users, Standard American English"
713,1597,5658,ABC_2e636754342e9bb857068922519dbc_34,ARXIV:1906.11085,ad31866da7f14ae21bd38df0a3b1ffd1a1438122,"More specifically, these techniques enhanced NLP algorithms through the use of contextualized text embeddings at word, sentence, and paragraph levels (Mikolov et al., 2013; Le and Mikolov, 2014; Peters et al., 2017; Devlin et al., 2018; Logeswaran and Lee, 2018; Radford et al., 2018) .",Background,s2,"In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.","In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and its context, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.",test,0,"The research addresses the problem of extracting PIO elements from medical abstracts, aiming to improve the accuracy and reduce ambiguity in the process. The research is motivated by the need to improve the accuracy and efficiency of PIO element extraction from medical abstracts, aiming to enhance automatic mining of medical information. The research also seeks to explore the benefits of contextualized embeddings and ensemble methods for classification tasks in this specific domain.",test,"abstracts of medical papers, automatic mining of medical abstracts, base learners, base learners, BERT embedding, BioBERT, BioBERT embedding, classification tasks, classification tool, contextualized classification model, contextualized embedding, embedding contextualization, ensemble method, full medical articles, full text data, LGBM algorithm, LGBM model, medical corpora, model boosting, PICONET, PIO element dataset, PIO elements, PIO elements classification, The model"
714,1598,5659,ABC_2e636754342e9bb857068922519dbc_34,ARXIV:1906.11085,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer (Vaswani et al., 2017; Devlin et al., 2018) .",Background,s2,"In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research addresses the problem of extracting PIO elements from medical abstracts, aiming to improve the accuracy and reduce ambiguity in the process. The research is motivated by the need to improve the accuracy and efficiency of PIO element extraction from medical abstracts, aiming to enhance automatic mining of medical information. The research also seeks to explore the benefits of contextualized embeddings and ensemble methods for classification tasks in this specific domain.",test,"abstracts of medical papers, automatic mining of medical abstracts, base learners, base learners, BERT embedding, BioBERT, BioBERT embedding, classification tasks, classification tool, contextualized classification model, contextualized embedding, embedding contextualization, ensemble method, full medical articles, full text data, LGBM algorithm, LGBM model, medical corpora, model boosting, PICONET, PIO element dataset, PIO elements, PIO elements classification, The model"
715,1599,5660,ABC_2e636754342e9bb857068922519dbc_34,ARXIV:1906.11085,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"Furthermore, we built a multi-label PIO classifier, along with a boosting framework, based on the state of the art text embedding, BERT. This embedding model has been proven to offer a better contextualization compared to a bidirectional LSTM model (Devlin et al., 2018) .",Difference,s2,"In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",test,0,"The research addresses the problem of extracting PIO elements from medical abstracts, aiming to improve the accuracy and reduce ambiguity in the process. The research is motivated by the need to improve the accuracy and efficiency of PIO element extraction from medical abstracts, aiming to enhance automatic mining of medical information. The research also seeks to explore the benefits of contextualized embeddings and ensemble methods for classification tasks in this specific domain.",test,"abstracts of medical papers, automatic mining of medical abstracts, base learners, base learners, BERT embedding, BioBERT, BioBERT embedding, classification tasks, classification tool, contextualized classification model, contextualized embedding, embedding contextualization, ensemble method, full medical articles, full text data, LGBM algorithm, LGBM model, medical corpora, model boosting, PICONET, PIO element dataset, PIO elements, PIO elements classification, The model"
716,1600,5661,ABC_2e636754342e9bb857068922519dbc_34,ARXIV:1906.11085,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"The idea behind this model is to pre-train a bidirectional representation by jointly conditioning on both left and right contexts in all layers using a transformer (Vaswani et al., 2017; Devlin et al., 2018) .",Motivation,s2,"In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research addresses the problem of extracting PIO elements from medical abstracts, aiming to improve the accuracy and reduce ambiguity in the process. The research is motivated by the need to improve the accuracy and efficiency of PIO element extraction from medical abstracts, aiming to enhance automatic mining of medical information. The research also seeks to explore the benefits of contextualized embeddings and ensemble methods for classification tasks in this specific domain.",test,"abstracts of medical papers, automatic mining of medical abstracts, base learners, base learners, BERT embedding, BioBERT, BioBERT embedding, classification tasks, classification tool, contextualized classification model, contextualized embedding, embedding contextualization, ensemble method, full medical articles, full text data, LGBM algorithm, LGBM model, medical corpora, model boosting, PICONET, PIO element dataset, PIO elements, PIO elements classification, The model"
717,1601,5663,ABC_2e636754342e9bb857068922519dbc_34,ARXIV:1906.11085,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"The first version is based on the original BERT release (Devlin et al., 2018) .",Uses,s2,"In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",test,1,"The research addresses the problem of extracting PIO elements from medical abstracts, aiming to improve the accuracy and reduce ambiguity in the process. The research is motivated by the need to improve the accuracy and efficiency of PIO element extraction from medical abstracts, aiming to enhance automatic mining of medical information. The research also seeks to explore the benefits of contextualized embeddings and ensemble methods for classification tasks in this specific domain.",test,"abstracts of medical papers, automatic mining of medical abstracts, base learners, base learners, BERT embedding, BioBERT, BioBERT embedding, classification tasks, classification tool, contextualized classification model, contextualized embedding, embedding contextualization, ensemble method, full medical articles, full text data, LGBM algorithm, LGBM model, medical corpora, model boosting, PICONET, PIO element dataset, PIO elements, PIO elements classification, The model"
718,1602,5664,ABC_2e636754342e9bb857068922519dbc_34,ARXIV:1906.11085,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"The model has 12 attention layers and all texts are converted to lowercase by the tokenizer (Devlin et al., 2018) .",Uses,s2,"In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",test,1,"The research addresses the problem of extracting PIO elements from medical abstracts, aiming to improve the accuracy and reduce ambiguity in the process. The research is motivated by the need to improve the accuracy and efficiency of PIO element extraction from medical abstracts, aiming to enhance automatic mining of medical information. The research also seeks to explore the benefits of contextualized embeddings and ensemble methods for classification tasks in this specific domain.",test,"abstracts of medical papers, automatic mining of medical abstracts, base learners, base learners, BERT embedding, BioBERT, BioBERT embedding, classification tasks, classification tool, contextualized classification model, contextualized embedding, embedding contextualization, ensemble method, full medical articles, full text data, LGBM algorithm, LGBM model, medical corpora, model boosting, PICONET, PIO element dataset, PIO elements, PIO elements classification, The model"
719,1603,5665,ABC_2e636754342e9bb857068922519dbc_34,ARXIV:1906.11085,1e077413b25c4d34945cc2707e17e46ed4fe784a,"The advantage of this approach is that few parameters need to be learned from scratch (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018) .",Unsure,s2,"In this paper, we presented an improved methodology to extract PIO elements, from abstracts of medical papers, that reduces ambiguity. The proposed technique was used to build a dataset of PIO elements that we call PICONET. We further proposed a model of PIO elements classification using state of the art BERT embedding. In addition, we investigated a contextualized embedding, BioBERT, trained on medical corpora. It has been found that using the BioBERT embedding improved the classification accuracy, outperforming the BERT-based model. This result reinforces the idea of the importance of embedding contextualization in subsequent classification tasks in this specific context.Furthermore, to enhance the accuracy of the model, we have investigated an ensemble method based on the LGBM algorithm. We trained the LGBM model, with the above models as base learners, to learn a linear combination of the predicted probabilities for the 3 classes with the TF-IDF score and the QIEF that optimizes the classification. The results indicate that these text features were good features to consider in order to boost the deeply contextualized classification model. We compared the performance of the classifier when using the features with one of the base learners and the case where we combine the base learners along with the features. We obtained the highest score in terms of AUC when we combine the base learners.The present work resulted in the creation of a PIO element dataset, PICONET, and a classification tool. These constitute and important component of our system of automatic mining of medical abstracts. We intend to extend the dataset to full medical articles. The model will be modified to take into account the higher complexity of full text data and more efficient features for model boosting will be investigated.","Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"The research addresses the problem of extracting PIO elements from medical abstracts, aiming to improve the accuracy and reduce ambiguity in the process. The research is motivated by the need to improve the accuracy and efficiency of PIO element extraction from medical abstracts, aiming to enhance automatic mining of medical information. The research also seeks to explore the benefits of contextualized embeddings and ensemble methods for classification tasks in this specific domain.",test,"abstracts of medical papers, automatic mining of medical abstracts, base learners, base learners, BERT embedding, BioBERT, BioBERT embedding, classification tasks, classification tool, contextualized classification model, contextualized embedding, embedding contextualization, ensemble method, full medical articles, full text data, LGBM algorithm, LGBM model, medical corpora, model boosting, PICONET, PIO element dataset, PIO elements, PIO elements classification, The model"
720,1905,6683,ABC_8c530e0c9f7256ac44b1a2adfaf6a9_45,ACL:S19-2055,9ea51567b3b19c9e71f4e62a2e77b1bdb4c3693c,"Instead of using basic CNN, a new recurrent sequential CNN is used by Zahiri and Choi (2018) .",Background,s2,"Emotion identification is a process of identifying the emotions automatically from text, speech or images. Emotion identification from textual conversations is a challenging problem due to absence of gestures, vocal intonation and facial expressions. It enables conversational agents, chat bots and messengers to detect and report the emotions to the user instantly for a healthy conversation by avoiding emotional cues and miscommunications. We have adopted a Seq2Seq deep neural network to identify the emotions present in the text sequences. Several layers namely embedding layer, encoding-decoding layer, softmax layer and a loss layer are used to map the sequences from textual conversations to the emotions namely Angry, Happy, Sad and Others. We have evaluated our approach on the EmoContext@SemEval2019 dataset and we have obtained the micro-averaged F1 scores as 0.595 and 0.6568 for the pre-evaluation dataset and final evaluation test set respectively. Our approach improved the base line score by 7% for final evaluation test set.","While there have been significant advances in detecting emotions from speech and image recognition, emotion detection on text is still under-explored and remained as an active research field. This paper introduces a corpus for text-based emotion detection on multiparty dialogue as well as deep neural models that outperform the existing approaches for document classification. We first present a new corpus that provides annotation of seven emotions on consecutive utterances in dialogues extracted from the show, Friends. We then suggest four types of sequence-based convolutional neural network models with attention that leverage the sequence information encapsulated in dialogue. Our best model shows the accuracies of 37.9% and 54% for fine- and coarse-grained emotions, respectively. Given the difficulty of this task, this is promising.",test,0,"The research addresses the challenge of emotion identification from textual conversations, specifically the difficulty arising from the absence of nonverbal cues such as gestures, intonation, and facial expressions. The research is motivated by the desire to improve communication in conversational agents, chatbots, and messengers. By enabling these systems to detect and respond to user emotions, the research aims to foster healthier conversations and prevent miscommunication.",test,"chat bots, EmoContext @ SemEval2019 dataset, Emotion identification, Emotion identification, encoding-decoding layer, evaluation test set, evaluation test set, Happy, healthy conversation, loss layer, messengers, pre-evaluation dataset, Seq2Seq deep neural network, softmax layer, text sequences, textual conversations, textual conversations"
721,1906,6684,ABC_8c530e0c9f7256ac44b1a2adfaf6a9_45,ACL:S19-2055,9ea51567b3b19c9e71f4e62a2e77b1bdb4c3693c,"Instead of using basic CNN, a new recurrent sequential CNN is used by Zahiri and Choi (2018) . All the models discussed above show that the emotion prediction can be handled using variants of deep neural network such as C-GRU, G-RNN and Sequential-CNN. The commonality between the above models are the variations of RNN or LSTM. This motivated us to use the Sequenceto-Sequence (Seq2Seq) model which consists of stacked LSTMs to predic the emotion labels conditioned on the given utterance sequences.",Motivation,s2,"Emotion identification is a process of identifying the emotions automatically from text, speech or images. Emotion identification from textual conversations is a challenging problem due to absence of gestures, vocal intonation and facial expressions. It enables conversational agents, chat bots and messengers to detect and report the emotions to the user instantly for a healthy conversation by avoiding emotional cues and miscommunications. We have adopted a Seq2Seq deep neural network to identify the emotions present in the text sequences. Several layers namely embedding layer, encoding-decoding layer, softmax layer and a loss layer are used to map the sequences from textual conversations to the emotions namely Angry, Happy, Sad and Others. We have evaluated our approach on the EmoContext@SemEval2019 dataset and we have obtained the micro-averaged F1 scores as 0.595 and 0.6568 for the pre-evaluation dataset and final evaluation test set respectively. Our approach improved the base line score by 7% for final evaluation test set.","While there have been significant advances in detecting emotions from speech and image recognition, emotion detection on text is still under-explored and remained as an active research field. This paper introduces a corpus for text-based emotion detection on multiparty dialogue as well as deep neural models that outperform the existing approaches for document classification. We first present a new corpus that provides annotation of seven emotions on consecutive utterances in dialogues extracted from the show, Friends. We then suggest four types of sequence-based convolutional neural network models with attention that leverage the sequence information encapsulated in dialogue. Our best model shows the accuracies of 37.9% and 54% for fine- and coarse-grained emotions, respectively. Given the difficulty of this task, this is promising.",test,0,"The research addresses the challenge of emotion identification from textual conversations, specifically the difficulty arising from the absence of nonverbal cues such as gestures, intonation, and facial expressions. The research is motivated by the desire to improve communication in conversational agents, chatbots, and messengers. By enabling these systems to detect and respond to user emotions, the research aims to foster healthier conversations and prevent miscommunication.",test,"chat bots, EmoContext @ SemEval2019 dataset, Emotion identification, Emotion identification, encoding-decoding layer, evaluation test set, evaluation test set, Happy, healthy conversation, loss layer, messengers, pre-evaluation dataset, Seq2Seq deep neural network, softmax layer, text sequences, textual conversations, textual conversations"
722,1909,6687,ABC_8c530e0c9f7256ac44b1a2adfaf6a9_45,ACL:S19-2055,4c48b8237557c94f643792b44855c83ae98f3eda,"This section reviews the research work reported for emotion detection from text / tweets (Perikos and Hatzilygeroudis, 2013; Rao, 2016; AbdulMageed and Ungar, 2017; Samy et al., 2018; AlBalooshi et al., 2018; Gaind et al., 2019 ) and text conversations (Phan et al., 2016; Sharma et al., 2017; Zahiri and Choi, 2018) .",Uses,s2,"Emotion identification is a process of identifying the emotions automatically from text, speech or images. Emotion identification from textual conversations is a challenging problem due to absence of gestures, vocal intonation and facial expressions. It enables conversational agents, chat bots and messengers to detect and report the emotions to the user instantly for a healthy conversation by avoiding emotional cues and miscommunications. We have adopted a Seq2Seq deep neural network to identify the emotions present in the text sequences. Several layers namely embedding layer, encoding-decoding layer, softmax layer and a loss layer are used to map the sequences from textual conversations to the emotions namely Angry, Happy, Sad and Others. We have evaluated our approach on the EmoContext@SemEval2019 dataset and we have obtained the micro-averaged F1 scores as 0.595 and 0.6568 for the pre-evaluation dataset and final evaluation test set respectively. Our approach improved the base line score by 7% for final evaluation test set.","In this paper, we address the problem of detection, classification and quantification of emotions of text in any form. We consider English text collected from social media like Twitter, which can provide information having utility in a variety of ways, especially opinion mining. Social media like Twitter and Facebook is full of emotions, feelings and opinions of people all over the world. However, analyzing and classifying text on the basis of emotions is a big challenge and can be considered as an advanced form of Sentiment Analysis. This paper proposes a method to classify text into six different Emotion-Categories: Happiness, Sadness, Fear, Anger, Surprise and Disgust. In our model, we use two different approaches and combine them to effectively extract these emotions from text. The first approach is based on Natural Language Processing, and uses several textual features like emoticons, degree words and negations, Parts Of Speech and other grammatical analysis. The second approach is based on Machine Learning classification algorithms. We have also successfully devised a method to automate the creation of the training-set itself, so as to eliminate the need of manual annotation of large datasets. Moreover, we have managed to create a large bag of emotional words, along with their emotion-intensities. On testing, it is shown that our model provides significant accuracy in classifying tweets taken from Twitter.",test,1,"The research addresses the challenge of emotion identification from textual conversations, specifically the difficulty arising from the absence of nonverbal cues such as gestures, intonation, and facial expressions. The research is motivated by the desire to improve communication in conversational agents, chatbots, and messengers. By enabling these systems to detect and respond to user emotions, the research aims to foster healthier conversations and prevent miscommunication.",test,"chat bots, EmoContext @ SemEval2019 dataset, Emotion identification, Emotion identification, encoding-decoding layer, evaluation test set, evaluation test set, Happy, healthy conversation, loss layer, messengers, pre-evaluation dataset, Seq2Seq deep neural network, softmax layer, text sequences, textual conversations, textual conversations"
723,2554,8711,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,ade0c116120b54b57a91da51235108b75c28375a,"Previous experiments with tasks like language modelling [Bengio et al., 2009] , Dependency Parsing, and entailment [Hashimoto et al., 2016] have shown faster convergence and performance gains by following a curriculum training regimen in the order of increasingly complicated syntactic and semantic tasks.",Background,s2,incorrect id format for ARXIV:1906.07382,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task’s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
724,2555,8712,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,ade0c116120b54b57a91da51235108b75c28375a,"[Hashimoto et al., 2016] propose a hierarchical multitask neural architecture with the lower layers performing syntactic tasks, and the higher layers performing the more involved semantic tasks while using the lower layer predictions.",Background,s2,incorrect id format for ARXIV:1906.07382,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task’s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
725,2556,8713,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"Considering the codemixed nature of texts and linguistic hierarchy of information, we propose the tasks in the order of : Language Identification, Part of Speech Tagging, Language Modelling and further semantic tasks like sentiment analysis. Since tokens in codemixed texts have distinct semantic spaces based on their source language, Language Identification can incorporate this disparity among the learnt trigram representations. Following this, the Part of Speech Tagging groups the words based on their logical semantic categories, and encodes simpler word category information in a sequence. Also, as in [Singh et al., 2018a; Sharma et al., 2016] , Language Tag and Part of Speech Tag have previously been provided as manual handcrafted features for a range of downstream syntactic and semantic tasks. In addition to the above tasks, Language Model pretraining has shown significant performance gains as reported by [Howard and Ruder, 2018] .",Background,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
726,2557,8714,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"As noted in earlier efforts [Howard and Ruder, 2018 ] towards finetuning pretrained models for NLP tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage. On the other hand, too cautious finetuning can cause slow convergence and overfitting.",Background,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
727,2558,8715,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,ade0c116120b54b57a91da51235108b75c28375a,"Like [Hashimoto et al., 2016] , they hypothesize the incorporation of simpler syntactic information into semantic tasks, and provide empirical evidence for the same.",Background,s2,incorrect id format for ARXIV:1906.07382,"Transfer and multi-task learning have traditionally focused on either a single source-target pair or very few, similar tasks. Ideally, the linguistic levels of morphology, syntax and semantics would benefit each other by being trained in a single model. We introduce a joint many-task model together with a strategy for successively growing its depth to solve increasingly complex tasks. Higher layers include shortcut connections to lower-level task predictions to reflect linguistic hierarchies. We use a simple regularization term to allow for optimizing all model weights to improve one task’s loss without exhibiting catastrophic interference of the other tasks. Our single end-to-end model obtains state-of-the-art or competitive results on five different tasks from tagging, parsing, relatedness, and entailment tasks.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
728,2559,8716,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"As discussed in Section 4.3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by [Howard and Ruder, 2018] .",Background,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
729,2560,8717,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"We experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to [Howard and Ruder, 2018] , and observe increase in model accuracy by 2.2% on sentiment analysis.",Background,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
730,2561,8718,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009; Howard and Ruder, 2018] .",Background,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
731,2562,8719,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"Gradual Unfreezing: Similar to [Howard and Ruder, 2018] , rather than updating all the layers together for finetuning, we explore gradual ordered unfreezing of layers. Thus, initially we freeze all the layers. Then starting from the last layer, we train the model for a certain number of epochs before unfreezing the layer below it. Thus for Sentiment Analysis finetuning, for the first epoch, only θ sentiment receives the gradient updates, after which we unfreeze the θ lstm2 , and subsequently unfreeze the lower layers in a similar manner.",Similar,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
732,2563,8720,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"As noted in earlier efforts [Howard and Ruder, 2018 ] towards finetuning pretrained models for NLP tasks, aggressive finetuning can cause catastrophic forgetting, thus causing the model to simply fit over the target task and forget any capabilities gained during the pretraining stage. On the other hand, too cautious finetuning can cause slow convergence and overfitting.",Motivation,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
733,2564,8721,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"We note the convergence of our model with and without curriculum training, and observe that the curriculum training regimen causes faster convergence, as has been observed in previous works [Bengio et al., 2009; Howard and Ruder, 2018] .",Similar,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
734,2565,8722,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"With this purview, similar to [Howard and Ruder, 2018] , we propose optimizing different layers in our model to different extents, and keep lower step sizes for the deeper pretrained layers while finetuning on a downstream task. We thus split the parameters as {θ 1 , ..., θ l } , where θ i corresponds to the parameters of layer i, and optimize them with separate learning rates {η 1 , ...., η l } . Also, when finetuning a pretrained layer for a downstream task, we keep η i < η j ; ∀i < j. Thus, while finetuning the POS + Lang Id pretrained model for Language Modeling, we propose to keep the learning rates for Embedding Layer and LSTM Layer 1 lower than the LSTM Layer 2 weights. Similarly, when finetuning the Language Model for Sentiment Analysis, we keep the learning rates of the deeper layers lower than that of the shallower ones.",Similar,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
735,2566,8723,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"As discussed in Section 4.3, for our transfer learning optimization experiments, we segment the optimization of different parameters of our model with different learning rates, in order to limit catastrophic forgetting and interference among the tasks, as proposed by [Howard and Ruder, 2018] .",Similar,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
736,2567,8724,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"Gradual Unfreezing: Similar to [Howard and Ruder, 2018] , rather than updating all the layers together for finetuning, we explore gradual ordered unfreezing of layers. Thus, initially we freeze all the layers. Then starting from the last layer, we train the model for a certain number of epochs before unfreezing the layer below it. Thus for Sentiment Analysis finetuning, for the first epoch, only θ sentiment receives the gradient updates, after which we unfreeze the θ lstm2 , and subsequently unfreeze the lower layers in a similar manner.",Uses,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,1,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
737,2568,8725,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"We experiment with average pooling and max pooling concatenation over hidden states for semantic prediction, similar to [Howard and Ruder, 2018] , and observe increase in model accuracy by 2.2% on sentiment analysis.",Similar,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
738,2570,8727,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,1e077413b25c4d34945cc2707e17e46ed4fe784a,"With this purview, similar to [Howard and Ruder, 2018] , we propose optimizing different layers in our model to different extents, and keep lower step sizes for the deeper pretrained layers while finetuning on a downstream task. We thus split the parameters as {θ 1 , ..., θ l } , where θ i corresponds to the parameters of layer i, and optimize them with separate learning rates {η 1 , ...., η l } . Also, when finetuning a pretrained layer for a downstream task, we keep η i < η j ; ∀i < j. Thus, while finetuning the POS + Lang Id pretrained model for Language Modeling, we propose to keep the learning rates for Embedding Layer and LSTM Layer 1 lower than the LSTM Layer 2 weights. Similarly, when finetuning the Language Model for Sentiment Analysis, we keep the learning rates of the deeper layers lower than that of the shallower ones.",Uses,s2,incorrect id format for ARXIV:1906.07382,"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100 times more data. We open-source our pretrained models and code.",test,1,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
739,2693,9063,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"""Lifelong Sentiment Classification"" (""LSC"" for simple below) [2] records that which domain does a word have the sentiment orientation.",Background,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
740,2694,9064,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,• Knowledge-Base Learner (KBL): The Knowledge-Based Learner [2] aims to retrieve and transfer previous knowledge to the current task.,Background,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
741,2695,9065,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,26ee5a9205b241f43f485c00419c266504852534,We use the same formula below as in the LSC [2] .,Uses,s2,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","When applying text classification to complex tasks, it is tedious and expensive to hand-label the large amounts of training data necessary for good performance. This paper presents an alternative approach to text classification that requires no labeled documents; instead, it uses a small set of keywords per class, a class hierarchy and a large quantity of easilyobtained unlabeled documents. The keywords are used to assign approximate labels to the unlabeled documents by termmatching. These preliminary labels become the starting point for a bootstrapping process that learns a naive Bayes classifier using Expectation-Maximization and hierarchical shrinkage. When classifying a complex data set of computer science research papers into a 70-leaf topic hierarchy, the keywords alone provide 45% accuracy. The classifier learned by bootstrapping reaches 66% accuracy, a level close to human agreement.",test,1,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
742,2814,9601,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,13dbb2c8a2c7cb3fa066c2d44f93e4a86418596e,"Hence, there has been a massive increase in work on MT systems that involve more than two languages (Dong et al., 2015; Firat et al., 2016a; Cheng et al., 2017; Johnson et al., 2017; Chen et al., 2017 Neubig and Hu, 2018) etc.",Background,s2,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.","Modern natural language processing and understanding applications have enjoyed a great boost utilizing neural networks models. However, this is not the case for most languages especially low-resource ones with insufficient annotated training data. Cross-lingual transfer learning methods improve the performance on a low-resource target language by leveraging labeled data from other (source) languages, typically with the help of cross-lingual resources such as parallel corpora. In this work, we propose a zero-resource multilingual transfer learning model that can utilize training data in multiple source languages, while not requiring target language training data nor cross-lingual supervision. Unlike existing methods that only rely on language-invariant features for cross-lingual transfer, our approach utilizes both language-invariant and language-specific features in a coherent way. Our model leverages adversarial networks to learn language-invariant features and mixture-of-experts models to dynamically exploit the relation between the target language and each individual source language. This enables our model to learn effectively what to share between various languages in the multilingual setup. It results in significant performance gains over prior art, as shown in an extensive set of experiments over multiple text classification and sequence tagging tasks including a large-scale real-world industry dataset.",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
743,2815,9602,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,ede8e4c43f4a3af66703a7641d990fa93606e616,"In addition, MNMT systems will be compact, because a single model handles translations for multiple languages (Johnson et al., 2017) .",Background,s2,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.","Neural Machine Translation (NMT) is a recently-emerged paradigm for Machine Translation (MT) that has shown promising results as well as a great potential to solve challenging MT tasks. One such a task is how to provide good MT for languages with sparse training data. In this paper we investigate a Zero Shot Translation (ZST) approach for such language combinations. ZST is a multilingual translation mechanism which uses a single NMT engine to translate between multiple languages, even such languages for which no direct parallel data was provided during training. After assessing ZST feasibility, by training a proof-of-concept engine ZST on French $ English and Italian $ English data, we focus on languages with sparse training data. In particular, we address the Tamil $ Hindi language pair. Our analysis shows the potential and effectiveness of ZST in such scenarios. To train and translate with ZST engines, we extend the training and translation pipelines of a commercial MT provider – KantanMT – with ZST capabilities, making this technology available to all users of the platform.",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
744,2816,9603,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,198ac64703e83d00eb0f51a4c4a7c77cb08a7e5c,"Mini-batches can be comprised of a mix of samples from different language pairs (Johnson et al., 2017) or the training schedule can cycle through mini-batches consisting of a language pair only (Firat et al., 2016a) .",Background,s2,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.","In this paper, we propose a novel finetuning algorithm for the recently introduced multi-way, mulitlingual neural machine translate that enables zero-resource machine translation. When used together with novel many-to-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivot-based translation strategy, while keeping only one additional copy of attention-related parameters.",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
745,2817,9604,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,420e89b9c7d77dfd13e7aaaf18c9c47917d9acff,"In fact, the number of parameters is only a small multiple of the compact model (the multiplication factor accounts for the language embedding size) (Johnson et al., 2017) , but the language embeddings can directly impact the model parameters instead of the weak influence that language tags have.",Background,s2,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.",",",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
746,2818,9605,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,420e89b9c7d77dfd13e7aaaf18c9c47917d9acff,"explored multiple methods for supporting target languages: (a) target language tag at beginning of the decoder, (b) target language dependent positional embeddings, and (c) divide hidden units of each decoder layer into shared and language-dependent ones. Each of these methods provide gains over Johnson et al. (2017) , and combining all gave the best results.",Background,s2,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.",",",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
747,2819,9606,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,ede8e4c43f4a3af66703a7641d990fa93606e616,"The compact MNMT models can handle code-mixed input, but code-mixed output remains an open problem (Johnson et al., 2017) .",Future Work,s2,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.","Neural Machine Translation (NMT) is a recently-emerged paradigm for Machine Translation (MT) that has shown promising results as well as a great potential to solve challenging MT tasks. One such a task is how to provide good MT for languages with sparse training data. In this paper we investigate a Zero Shot Translation (ZST) approach for such language combinations. ZST is a multilingual translation mechanism which uses a single NMT engine to translate between multiple languages, even such languages for which no direct parallel data was provided during training. After assessing ZST feasibility, by training a proof-of-concept engine ZST on French $ English and Italian $ English data, we focus on languages with sparse training data. In particular, we address the Tamil $ Hindi language pair. Our analysis shows the potential and effectiveness of ZST in such scenarios. To train and translate with ZST engines, we extend the training and translation pipelines of a commercial MT provider – KantanMT – with ZST capabilities, making this technology available to all users of the platform.",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
748,2967,10091,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) <span style=""background: yellow; display: inline-block"">""coloreless green ideas""</span> subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena.",Motivation,llm,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
749,2968,10092,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results. The <span style=""background: yellow; display: inline-block"">Gulordava et al. (2018)</span> and Marvin and Linzen (2018) conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.",Background,llm,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,0,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
750,2969,10093,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"I expected the attentionbased mechanism to fail on these (compared to the LSTM-based models), and am surprised by these results. The <span style=""background: yellow; display: inline-block"">Gulordava et al. (2018)</span> and Marvin and Linzen (2018) conditions rule out the possibility of overly relying on selectional preference cues or memorizing the wikipedia training data, and suggest real syntactic generalization is taking place.",Uses,llm,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
751,2970,10094,ABC_bebcad79900e9a4a25020ed0d886b5_5,ARXIV:1901.05287,3d42ddf7c5ce59ae04d1d27085be9f736d1be04b,"While not strictly comparable, the numbers reported by <span style=""background: yellow; display: inline-block"">Gulordava et al. (2018)</span> for the LSTM in this condition (on All) is 74.1 ± 1.6.",Uses,llm,"I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) ""coloreless green ideas"" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.","Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.",test,1,"The research aims to assess the ability of the BERT model to capture English syntactic phenomena. The motivation for this research is not explicitly stated in the abstract. However, it can be inferred that the research is driven by a desire to understand the linguistic capabilities of the BERT model and its potential applications in understanding and analyzing language.",test,"English syntactic phenomena, manually crafted stimuli, naturally-occurring subject-verb agreement stimuli, reflexive anaphora phenomena"
752,3259,11098,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,8de174ab5419b9d3127695405efd079808e956e8,"Curriculum learning can be seen as a sequence of training criteria<span style=""background: yellow; display: inline-block""> [Bengio et al., 2009]</span> , with increasing task or sample difficulty as the training progresses. It is also closely related with transfer learning by pretraining, especially in the case when the tasks form a logical hierarchy and contribute to the downstream tasks. With this purview, we propose a linguistic hierarchy of training tasks for codemixed languages, with further layers abstracting over the previous ones to achieve increasingly complicated tasks.",Background,llm,incorrect id format for ARXIV:1906.07382,"Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ""curriculum learning"". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
753,3260,11099,ABC_74cd12a801d1f8a95f8898a8cef9c0_6,ARXIV:1906.07382,8de174ab5419b9d3127695405efd079808e956e8,"Curriculum learning can be seen as a sequence of training criteria<span style=""background: yellow; display: inline-block""> [Bengio et al., 2009]</span> , with increasing task or sample difficulty as the training progresses. It is also closely related with transfer learning by pretraining, especially in the case when the tasks form a logical hierarchy and contribute to the downstream tasks. With this purview, we propose a linguistic hierarchy of training tasks for codemixed languages, with further layers abstracting over the previous ones to achieve increasingly complicated tasks.",Motivation,llm,incorrect id format for ARXIV:1906.07382,"Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ""curriculum learning"". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).",test,0,"Accurately performing semantic NLP tasks, specifically sentiment analysis, on code-mixed social media text is challenging due to the informal language, code-mixing, and lack of resources for training, especially in code-mixed language pairs like Hindi-English. The research is driven by the need to analyze public opinion and understand social media content, which is challenging due to the prevalence of code-mixing in multilingual societies. The lack of effective methods for analyzing code-mixed text, especially for resource-scarce languages, further motivates this research.",test,"code-mixed language pairs, code-mixed social media text, code-mixed text, code-mixing, informal language, methods, multilingual societies, resource-scarce languages, semantic NLP tasks, sentiment analysis, social media content"
754,3267,11128,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"If we can achieve this learning goal, the algorithms are able to solve new tasks without teaching. Zhiyuan Chen and etc. <span style=""background: yellow; display: inline-block"">[2]</span> ever proposed a approach to close the goal.",Background,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
755,3268,11129,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"• Knowledge Base (KB): The knowledge Base <span style=""background: yellow; display: inline-block"">[2]</span> mainly used to maintain the previous knowledge.",Background,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
756,3269,11130,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Previous classical paper <span style=""background: yellow; display: inline-block"">[2]</span> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains.",Background,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
757,3270,11131,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"As we known, not all words have sentimental polarity like ""a"", ""one"" and etc. while some words always have polarity like ""good"", ""hate"", ""excellent"" and so on. Hence, in order to achieve the goal of the lifelong learning. We need to find the words always have sentiment polarity and be careful for those words only shows polarity in specific domains.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
758,3271,11132,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Previous classical paper <span style=""background: yellow; display: inline-block"">[2]</span> chose the sentiment classification as the learning target because it could be regarded as a large task as well as a group of related sub-tasks in the different domains. Although these sub-tasks are related to each other but a model only trained on a single subtasks is unable to perform well in the rest sub-tasks.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
759,3272,11133,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Although LSC <span style=""background: yellow; display: inline-block"">[2]</span> already raised a lifelong approach, it only aims to improve the classification accuracy. It still is under the setting of the supervised learning and also is unable to deliver an explicit knowledge to guild further learning.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
760,3273,11134,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"As the previous work <span style=""background: yellow; display: inline-block"">[2]</span> , this paper also uses Naïve Bayes as the knowledge can be presented by the probability.",Uses,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,1,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
761,3274,11135,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Although LSC <span style=""background: yellow; display: inline-block"">[2]</span> considered the difference among domains, it still is a typical supervised learning approach.",Motivation,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,0,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
762,3275,11136,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"In the experiment, we use the same datasets as LSC <span style=""background: yellow; display: inline-block"">[2]</span> used.",Uses,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,1,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
763,3276,11137,ABC_bb74dd634a8fc5cdb2f4f3294b6bc5_7,ARXIV:1905.01988,998b690b4f1fac0c59deccdd31f987160f354489,"Although LSC <span style=""background: yellow; display: inline-block"">[2]</span> already raised a lifelong approach, it only aims to improve the classification accuracy. Based on the LSC, this paper advances the lifelong learning in sentiment classification and have two main contributions: • A improved lifelong learning paradigm is proposed to solve the sentiment classification problem under unsupervised learning setting with previous knowledge. • We introduce a novel approach to discover and store the words with sentiment polarity for reuse.",Extention,llm,"Lifelong machine learning is a novel machine learning paradigm which can continually accumulate knowledge during learning. The knowledge extracting and reusing abilities enable the lifelong machine learning to solve the related problems. The traditional approaches like Naïve Bayes and some neural network based approaches only aim to achieve the best performance upon a single task. Unlike them, the lifelong machine learning in this paper focus on how to accumulate knowledge during learning and leverage them for the further tasks. Meanwhile, the demand for labeled data for training also be significantly decreased with the knowledge reusing. This paper suggests that the aim of the lifelong learning is to use less labeled data and computational cost to achieve the performance as well as or even better than the supervised learning.","This paper proposes a novel lifelong learning (LL) approach to sentiment classification. LL mimics the human continuous learning process, i.e., retaining the knowledge learned from past tasks and use it to help future learning. In this paper, we first discuss LL in general and then LL for sentiment classification in particular. The proposed LL approach adopts a Bayesian optimization framework based on stochastic gradient descent. Our experimental results show that the proposed method outperforms baseline methods significantly, which demonstrates that lifelong learning is a promising research direction.",test,1,"The research problem is the limitations of traditional machine learning approaches in tackling multiple tasks and the need for significant labeled data. The paper aims to address this by introducing and exploring the concept of lifelong machine learning, a paradigm that focuses on continuously accumulating and reusing knowledge for improved performance. The research is motivated by the desire to overcome the limitations of traditional machine learning methods. The motivation lies in achieving comparable or better performance with less labeled data and computational resources, making it more efficient and cost-effective.",test,"knowledge reusing, lifelong learning, Lifelong machine learning, lifelong machine learning, lifelong machine learning, machine learning paradigm, neural network based approaches"
764,3295,11224,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,a486e2839291111bb44fa1f07731ada123539f75,"Zeroshot translation: Translating between language pairs without parallel corpora<span style=""background: yellow; display: inline-block""> (Johnson et al., 2017)</span> .",Background,llm,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.","We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
765,3296,11225,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,a486e2839291111bb44fa1f07731ada123539f75,"As there are always vocabulary overlaps between different domains, there are no zero-shot translation<span style=""background: yellow; display: inline-block""> (Johnson et al., 2017)</span> settings in domain adaptation.",Background,llm,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.","We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
766,3297,11226,ABC_b1c9b8e24916b136948610383f8ea2_10,ARXIV:1905.05395,a486e2839291111bb44fa1f07731ada123539f75,"Zero-shot NMT was first demonstrated by<span style=""background: yellow; display: inline-block""> Johnson et al. (2017)</span> .",Background,llm,"We present a survey on multilingual neural machine translation (MNMT), which has gained a lot of traction in the recent years. MNMT has been useful in improving translation quality as a result of knowledge transfer. MNMT is more promising and interesting than its statistical machine translation counterpart because end-to-end modeling and distributed representations open new avenues. Many approaches have been proposed in order to exploit multilingual parallel corpora for improving translation quality. However, the lack of a comprehensive survey makes it difficult to determine which approaches are promising and hence deserve further exploration. In this paper, we present an in-depth survey of existing literature on MNMT. We categorize various approaches based on the resource scenarios as well as underlying modeling principles. We hope this paper will serve as a starting point for researchers and engineers interested in MNMT.","We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT’14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT’14 and WMT’15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",test,0,"The lack of a comprehensive survey on Multilingual Neural Machine Translation (MNMT) makes it difficult to identify promising approaches and prioritize further research efforts in the field. The research is motivated by the growing popularity and potential of MNMT, which offers advantages over traditional statistical machine translation methods. The survey aims to provide a valuable resource for researchers and engineers interested in MNMT, helping them understand the current landscape and identify promising research directions.",test,"distributed representations, end-to-end modeling, MNMT, multilingual neural machine translation ( MNMT ), multilingual parallel corpora, statistical machine translation counterpart"
767,85,130,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"Conneau et al. (2018) improved this approach with post-mapping refinements, showing impressive results for several language pairs.",Background,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
768,86,131,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"This was first proposed by Miceli Barone (2016) , who initially used an adversarial network similar to Conneau et al. (2018) , and found that the mapper (which is also the encoder) translates everything to a single embedding, known commonly as the mode collapse issue (Goodfellow, 2017) .",Background,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
769,87,132,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,Conneau et al. (2018) show impressive results with adversarial training and refinement with the Procrustes solution.,Background,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
770,88,133,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"For finding the nearest neighbors, we use the Cross-domain Similarity Local Scaling (CSLS) which works better in mitigating the hubness problem (Conneau et al., 2018) .",Background,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
771,89,134,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,dda2cee72ffa3e5c70db302a87595ba9e5e72910,Conneau et al. (2018) and Artetxe et al. (2018b) propose fine-tuning methods to refine the initial mappings.,Background,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
772,90,135,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,dda2cee72ffa3e5c70db302a87595ba9e5e72910,"In particular, Artetxe et al. (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a,b) fail for many language pairs.",Background,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
773,91,136,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"However, this is not surprising as it has been shown that iterative fine-tuning with Procrustes solution is a robust method that can recover many errors made in the initial mapping (Conneau et al., 2018) .",Background,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
774,92,137,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,fbd97148a8df29b638478df91f4309a4ac9dd8ae,"Although successful, adversarial training has been criticized for not being stable and failing to converge, inspiring researchers to propose nonadversarial methods more recently (Xu et al., 2018a; Hoshen and Wolf, 2018; Alvarez-Melis and Jaakkola, 2018; Artetxe et al., 2018b) . In particular, Artetxe et al. (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings.",Motivation,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
775,93,138,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,dda2cee72ffa3e5c70db302a87595ba9e5e72910,"In particular, Artetxe et al. (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings.",Extention,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
776,94,139,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"Our aim is to learn a mapping f (x) in an unsupervised way (i.e., no bilingual dictionary given) such that for every x i , f (x) corresponds to its translation in Y. Our overall approach follows the same sequence of steps as Conneau et al. (2018): 1.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
777,95,140,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,Our discriminators have the same architecture as Conneau et al. (2018) .,Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
778,96,141,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"Through experiments, our goal is to assess: 1. Does the unsupervised mapping method based on our proposed adversarial autoencoder model improve over the best existing adversarial method of Conneau et al. (2018) in terms of mapping accuracy and convergence (Section 5.1)?",Motivation,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
779,97,142,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,dda2cee72ffa3e5c70db302a87595ba9e5e72910,"In particular, Artetxe et al. (2018b) show that the adversarial methods of Conneau et al. (2018) and Zhang et al. (2017a,b) fail for many language pairs. In this paper, we revisit adversarial training and propose a number of key improvements that yield more robust training and improved mappings.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
780,98,143,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,dda2cee72ffa3e5c70db302a87595ba9e5e72910,"Conneau et al. (2018) and Artetxe et al. (2018b) propose fine-tuning methods to refine the initial mappings. Similar to Conneau et al. (2018) ), we finetune our initial mappings (G and F ) by iteratively solving the Procrustes problem and applying a dictionary induction step. This method uses singular value decomposition or SVD of Z T y Z x to find the optimal mappings G (similarly SVD(Z T x Z y ) for F ) given the approximate alignment of words from the previous step.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
781,99,144,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"We evaluate our model on two different datasets. The first one is from Conneau et al. (2018) , which consists of FastText monolingual embeddings of (d =) 300 dimensions (Bojanowski et al., 2017) trained on Wikipedia monolingual corpus and gold dictionaries for 110 language pairs.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
782,100,145,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"Our training setting is similar to Conneau et al. (2018) , and we apply the same pre-and postprocessing steps.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
783,101,146,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"To evaluate how our unsupervised method compares with methods that rely on a bilingual seed dictionary, we follow Conneau et al. (2018) , and compute a supervised baseline that uses the Procrustes solution directly on the seed dictionary (5000 pairs) to learn the mapping function, and then uses CSLS to do the nearest neighbor search.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
784,102,147,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"We present our results on European languages on the datasets of Conneau et al. (2018) and Dinu et al. (2015) in Tables 1 and 3 , while the results on non-European languages are shown in Table 2 .",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
785,103,148,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,**COMPARISON WITH CONNEAU ET AL. (2018)**,Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
786,104,149,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"For a fair comparison with respect to the quality of the learned mappings (or induced seed dictionary), here we only consider the results of our approach that use the refinement procedure of Conneau et al. (2018) .",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
787,105,150,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,fbd97148a8df29b638478df91f4309a4ac9dd8ae,"We compare our method with the unsupervised models of Conneau et al. (2018) , Artetxe et al. (2018b) , Alvarez-Melis and Jaakkola (2018) , Xu et al. (2018a) , and Hoshen and Wolf (2018) . For some of the baselines, results are reported from their papers, while for the rest we report results by running the publicly available codes on our machine.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
788,106,151,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"In Table 1 , we see that our Adversarial autoencoder + Conneau et al. (2018) Refinement outperforms Conneau et al. (2018) in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%. We ran their code 10 times for Ms→En but failed every time.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
789,107,152,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"In Section 5.3, we compare our model with Conneau et al. (2018) more rigorously by evaluating them with and without fine-tuning and measuring their performance on P@1, P@5, and P@10.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
790,108,153,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"In this section, we compare our model with other state-of-the-art methods that do not follow the same procedure as us and Conneau et al. (2018) .",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
791,109,154,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"In our first experiment, we use their method to induce the initial seed dictionary and then apply iterative Procrustes solution (same refinement procedure of Conneau et al. (2018) ) for refinement.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
792,110,155,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"This setup allows us to compare our model directly with the adversarial model of Conneau et al. (2018) , putting the effect of finetuning aside.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
793,111,156,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,Our results show that our model is more robust and yields significant gains over Conneau et al. (2018) for all translation tasks in all evaluation measures.,Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
794,112,157,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"Conneau et al. (2018) show impressive results with adversarial training and refinement with the Procrustes solution. However, while all these methods learn the mapping in the original embedding space, our approach learns it in the latent code space considering both the mapper and the target encoder as adversary.",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
795,113,158,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"Let us first consider the results for European language pairs on the dataset of Conneau et al. (2018) in Table 1 . Our Adversarial autoencoder + Conneau et al. (2018) Refinement performs better than most of the other methods on this dataset, achieving the highest accuracy for 4 out of 6 translation tasks.",Uses,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
796,114,159,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"This is in contrast with most existing methods (e.g., Conneau et al. (2018) ; Artetxe et al. (2017) ) that directly map the distribution of the source word embeddings p(x) to the distribution of the target p(y).",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
797,115,160,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"In Table 1 , we see that our Adversarial autoencoder + Conneau et al. (2018) Refinement outperforms Conneau et al. (2018) in all the six translation tasks involving European language pairs, yielding gains in the range 0.3 -1.3%. Our method is also superior to theirs for the non-European and low-resource language pairs in Table 2 . We found their model to be very fragile for En from/to Ms, and does not converge at all for Ms→En. We ran their code 10 times for Ms→En but failed every time. Compared to that, our method is more robust and converged most of the time we ran.",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
798,116,161,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"If we compare our method with the method of Conneau et al. (2018) on the more challenging Dinu-Artexe dataset in Table 3 , we see here also our method performs better than their method in all the four translation tasks involving European language pairs. In this dataset, our method shows more robustness compared to their method. For example, their method had difficulties in converging for En from/to Es translations; for En→Es, it converges only 2 times out of 10 attempts, while for Es→En it did not converge a single time in 10 attempts. Compared to that, our method was more robust, converging 4 times out of 10 attempts.",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
799,117,162,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"This setup allows us to compare our model directly with the adversarial model of Conneau et al. (2018) , putting the effect of finetuning aside.",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
800,118,163,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,fbd97148a8df29b638478df91f4309a4ac9dd8ae,"We compare our method with the unsupervised models of Conneau et al. (2018) , Artetxe et al. (2018b) , Alvarez-Melis and Jaakkola (2018) , Xu et al. (2018a) , and Hoshen and Wolf (2018) .",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
801,119,164,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"As we compare our full model with the model of Conneau et al. (2018) in the without fine-tuning setting, we notice large improvements in all measures across all datasets: 5.1 -7.3% in En→Es, 3 -6% in Es→En, 3.4 -4.3% in En→De, 1 -3% in De→En, 3.4 -4.3% in En→It, and 0.3 -3.7% in It→En.",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
802,120,165,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"In Section 5.3, we compare our model with Conneau et al. (2018) more rigorously by evaluating them with and without fine-tuning and measuring their performance on P@1, P@5, and P@10.",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
803,121,166,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"Through extensive experimentations on six different language pairs comprising European, nonEuropean and low-resource languages from two different data sources, we demonstrate that our method outperforms the method of Conneau et al. (2018) for all translation tasks in all measures (P@{1,5,10}) across all settings (with and without fine-tuning).",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
804,122,167,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,Our discriminators have the same architecture as Conneau et al. (2018) .,Similar,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
805,123,168,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,fbd97148a8df29b638478df91f4309a4ac9dd8ae,"We compare our method with the unsupervised models of Conneau et al. (2018) , Artetxe et al. (2018b) , Alvarez-Melis and Jaakkola (2018) , Xu et al. (2018a) , and Hoshen and Wolf (2018) .",Similar,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","Cross-lingual transfer of word embeddings aims to establish the semantic mappings among words in different languages by learning the transformation functions over the corresponding word embedding spaces. Successfully solving this problem would benefit many downstream tasks such as to translate text classification models from resource-rich languages (e.g. English) to low-resource languages. Supervised methods for this problem rely on the availability of cross-lingual supervision, either using parallel corpora or bilingual lexicons as the labeled data for training, which may not be available for many low resource languages. This paper proposes an unsupervised learning approach that does not require any cross-lingual labeled data. Given two monolingual word embedding spaces for any language pair, our algorithm optimizes the transformation functions in both directions simultaneously based on distributional matching as well as minimizing the back-translation losses. We use a neural network implementation to calculate the Sinkhorn distance, a well-defined distributional similarity measure, and optimize our objective through back-propagation. Our evaluation on benchmark datasets for bilingual lexicon induction and cross-lingual word similarity prediction shows stronger or competitive performance of the proposed method compared to other state-of-the-art supervised and unsupervised baseline methods over many language pairs.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
806,124,169,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"In Section 5.3, we compare our model with Conneau et al. (2018) more rigorously by evaluating them with and without fine-tuning and measuring their performance on P@1, P@5, and P@10.",Similar,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
807,125,170,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"to note that in contrast to Conneau et al. (2018) , our mapping is performed at the code space.",Difference,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
808,126,171,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,562c09c112df56c5696c010d90a815d6018a86c8,"This setup allows us to compare our model directly with the adversarial model of Conneau et al. (2018) , putting the effect of finetuning aside.",Similar,s2,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","State-of-the-art methods for learning cross-lingual word embeddings have relied on bilingual dictionaries or parallel corpora. Recent studies showed that the need for parallel data supervision can be alleviated with character-level information. While these methods showed encouraging results, they are not on par with their supervised counterparts and are limited to pairs of languages sharing a common alphabet. In this work, we show that we can build a bilingual dictionary between two languages without using any parallel corpora, by aligning monolingual word embedding spaces in an unsupervised way. Without using any character information, our model even outperforms existing supervised methods on cross-lingual tasks for some language pairs. Our experiments demonstrate that our method works very well also for distant language pairs, like English-Russian or English-Chinese. We finally describe experiments on the English-Esperanto low-resource language pair, on which there only exists a limited amount of parallel data, to show the potential impact of our method in fully unsupervised machine translation. Our code, embeddings and dictionaries are publicly available.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
809,1208,2347,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,7ed8dd8a10ebf1508947c29c1e82f8a0197f9f78,"Evaluation Metrics Despite differences in the exact definitions, the majority (e.g., Hsu et al., 2018; Celikyilmaz et al., 2018; Narayan et al., 2018b; Chen and Bansal, 2018; Peyrard and Gurevych, 2018) agree on both or either one of two broad quality definitions: coverage determines how much of the salient content of the source document is captured in the summary, and informativeness, how much of the content captured in the summary is salient with regards to the original document.",Background,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.","Supervised summarization systems usually rely on supervision at the sentence or n-gram level provided by automatic metrics like ROUGE, which act as noisy proxies for human judgments. In this work, we learn a summary-level scoring function \theta including human judgments as supervision and automatically generated data as regularization. We extract summaries with a genetic algorithm using \theta as a fitness function. We observe strong and promising performances across datasets in both automatic and manual evaluation.",train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
810,1209,2348,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"Research in automatic summarization has made headway over the years with single document summarization as the front-runner due to the availability of large datasets (Sandhaus, 2008; Hermann et al., 2015; Narayan et al., 2018b) which has enabled the development of novel methods, many of them employing recent advances in neural networks (See et al., 2017; Narayan et al., 2018c; , inter alia).",Background,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
811,1210,2349,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias.",Background,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
812,1211,2350,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"Those that conduct manual assessment of the content, typically use a single reference summary, either directly (Celikyilmaz et al., 2018; Tan et al., 2017) or through questions (Narayan et al., 2018b,c) and thus are also likely to exhibit reference bias. In this paper we propose a novel approach for manual evaluation, HIGHlight-based Referenceless Evaluation of document Summarization (HIGHRES), in which a summary is assessed against the source document via manually highlighted salient content in the latter (see Figure 1 for an example).",Difference,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
813,1212,2351,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,ddfa4ba42cc20e54b620640f320082d2f10fc89c,"Most recent work uses a single human judgment to capture all linguistic qualities of the summary (Hsu et al., 2018; Kryściński et al., 2018; Narayan et al., 2018b; Song et al., 2018; Guo et al., 2018) ; we group them under ""Fluency"" in Table 1 with an exception of ""Clarity"" which was evaluated in the DUC evaluation campaigns (Dang, 2005) .",Uses,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.","Abstractive text summarization aims to shorten long text documents into a human readable form that contains the most important facts from the original document. However, the level of actual abstraction as measured by novel phrases that do not appear in the source document remains low in existing approaches. We propose two techniques to improve the level of abstraction of generated summaries. First, we decompose the decoder into a contextual network that retrieves relevant parts of the source document, and a pretrained language model that incorporates prior knowledge about language generation. Second, we propose a novelty metric that is optimized directly through policy learning to encourage the generation of novel phrases. Our model achieves results comparable to state-of-the-art models, as determined by ROUGE scores and human evaluations, while achieving a significantly higher level of abstraction as measured by n-gram overlap with the source document.",train,1,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
814,1213,2352,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"Absolute assessment was also employed in combination with the question answering approach for content evaluation (Narayan et al., 2018b; Mendes et al., 2019) .",Uses,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,1,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
815,1214,2353,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"Clarke and Lapata (2010) proposed a question-answering based approach to improve the agreement among human evaluations for the quality of summary content, which was recently employed by Narayan et al. (2018b) and Narayan et al. (2018c) (QA in Table 1 ).",Background,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
816,1215,2354,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"To validate our proposed approach we use the recently introduced eXtreme SUMmarization dataset (XSUM, Narayan et al., 2018b) to evaluate two state-of-the-art abstractive summarization methods, Pointer Generator Networks (See et al., 2017) and Topic-aware Convolutional Networks (Narayan et al., 2018b) , using crowd-sourcing for both highlight annotation and quality judgments.",Uses,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,1,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
817,1216,2355,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"We use the extreme summarization dataset (XSUM, Narayan et al., 2018b) 2 which comprises BBC articles paired with their singlesentence summaries, provided by the journalists writing the articles.",Uses,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,1,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
818,1217,2356,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"Following Narayan et al. (2018b) , we didn't use the whole test set portion, but sampled 50 articles from it for our highlight-based evaluation.",Uses,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,1,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
819,1218,2357,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,4e346eb1628df6a12c1a121f862fb3a16c6fec60,"However, summarization datasets are limited to a single reference summary per document (Sandhaus, 2008; Hermann et al., 2015; Grusky et al., 2018; Narayan et al., 2018b) thus evaluations using them is prone to reference bias (Louis and Nenkova, 2013) , also a known issue in machine translation evaluation (Fomicheva and Specia, 2016) .",Motivation,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.","We present NEWSROOM, a summarization dataset of 1.3 million articles and summaries written by authors and editors in newsrooms of 38 major news publications. Extracted from search and social media metadata between 1998 and 2017, these high-quality summaries demonstrate high diversity of summarization styles. In particular, the summaries combine abstractive and extractive strategies, borrowing words and phrases from articles at varying rates. We analyze the extraction strategies used in NEWSROOM summaries against other datasets to quantify the diversity and difficulty of our new data, and train existing methods on the data to evaluate its utility and challenges. The dataset is available online at summari.es.",train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
820,1219,2358,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"When comparing the reference summaries against the original documents, both ROUGE and HROUGE confirm that the reference summaries are rather abstractive as reported by Narayan et al. (2018b) , and they in fact score below the system summaries.",Similar,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
821,1221,2360,ABC_45238fe9b493ccdf5921c8f5284097_13,ACL:P19-1330,59562be2cf8e01e8b7bb7560cef56158ea171227,"The superiority of TCONVS2S is expected; TCONVS2S is better than PTGEN for recognizing pertinent content and generating informative summaries due to its ability to represent high-level document knowledge in terms of topics and long-range dependencies (Narayan et al., 2018b) .",Similar,s2,"There has been substantial progress in summarization research enabled by the availability of novel, often large-scale, datasets and recent advances on neural network-based approaches. However, manual evaluation of the system generated summaries is inconsistent due to the difficulty the task poses to human non-expert readers. To address this issue, we propose a novel approach for manual evaluation, Highlight-based Reference-less Evaluation of Summarization (HighRES), in which summaries are assessed by multiple annotators against the source document via manually highlighted salient content in the latter. Thus summary assessment on the source document by human judges is facilitated, while the highlights can be used for evaluating multiple systems. To validate our approach we employ crowd-workers to augment with highlights a recently proposed dataset and compare two state-of-the-art systems. We demonstrate that HighRES improves inter-annotator agreement in comparison to using the source document directly, while they help emphasize differences among systems that would be ignored under other evaluation approaches.",Single document summarization is the task of producing a shorter version of a document while preserving its principal information content. In this paper we conceptualize extractive summarization as a sentence ranking task and propose a novel training algorithm which globally optimizes the ROUGE evaluation metric through a reinforcement learning objective. We use our algorithm to train a neural summarization model on the CNN and DailyMail datasets and demonstrate experimentally that it outperforms state-of-the-art extractive and abstractive systems when evaluated automatically and by humans.,train,0,"The research problem is the inconsistency in manual evaluation of summarization systems, primarily due to the difficulty for non-expert human readers to assess the summaries effectively. The motivation behind the research is to address the issue of inconsistent manual evaluation in summarization research by providing a more robust and reliable approach for evaluating system-generated summaries.",dev,"approach, inconsistent manual evaluation, manual evaluation, summarization research, summarization systems, system-generated summaries"
822,1297,2495,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field (Sproat and Jaitly, 2016) .",Background,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
823,1301,2499,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field (Sproat and Jaitly, 2016) . In this paper, we present our approach to nonstandard text normalization via machine translation techniques, where the source and target are written and spoken form text, respectively.",Motivation,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
824,1302,2500,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from Sproat and Jaitly (2016) .,Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
825,1303,2501,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the Sproat and Jaitly (2016) data release and split into training, validation, and test data.",Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
826,1304,2502,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Our labels are generated directly from the Google FST (Sproat and Jaitly, 2016) .",Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
827,1305,2503,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with Sproat and Jaitly (2016) , considering our training set is much smaller.",Difference,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
828,1306,2504,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"We follow Sproat and Jaitly (2016) in down-sampling window-based training data to constrain the proportion of ""<self>"" tokens to 10% of the data.",Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
829,1307,2505,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,Our first approach replicates the window-based seq2seq model of Sproat and Jaitly (2016) .,Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
830,1308,2506,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Data with TELEPHONE labels were not included in the initial analysis of Sproat and Jaitly (2016) , but were made available in the dataset release.",Difference,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
831,1309,2509,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with Sproat and Jaitly (2016) , considering our training set is much smaller.",Similar,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
832,1399,2680,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,abd91aca4d78799492256b406f5abc199d3802e4,"Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) [27] , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Sequence-to-sequence attention-based models integrate an acoustic, pronunciation and language model into a single neural network, which make them very suitable for multilingual automatic speech recognition (ASR). In this paper, we are concerned with multilingual speech recognition on low-resource languages by a single Transformer, one of sequence-to-sequence attention-based models. Sub-words are employed as the multilingual modeling unit without using any pronunciation lexicon. First, we show that a single multilingual ASR Transformer performs well on low-resource languages despite of some language confusion. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence under the condition of language information being known during training. Experiments on CALLHOME datasets demonstrate that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5\% average word error rate (WER) reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4\% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
833,1401,2682,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"One can also assign interpretations; for example, [27] argue their LAS self-attention heads are differentiated phoneme detectors.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
834,1402,2683,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences [27] to train self-attention for ASR.",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
835,1404,2685,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"A convolutional frontend is a typical downsampling strategy [8, 19] ; however, we leave integrating other layer types into SAN-CTC as future work. Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one [27] .",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
836,1405,2686,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"Wide contexts also enable incorporation of noise/speaker contexts, as [27] suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
837,1406,2687,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"We see that unlike self-attentional LAS [27] , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute).",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
838,1407,2688,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"The latter was found necessary for self-attentional LAS [27] , as additive encodings did not give convergence.",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
839,1408,2689,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"Inspired by [27] , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details.",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
840,1409,2690,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike [27] we still see mild differentiation of variances.",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
841,1410,2691,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",train,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
842,1411,2693,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",Uses,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",train,1,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
843,1412,2695,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"Inspired by [27] , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details.",Uses,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",train,1,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
844,1413,2697,ABC_18a44fac8d2f450aee62fc15c00c6f_15,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"A convolutional frontend is a typical downsampling strategy [8, 19] ; however, we leave integrating other layer types into SAN-CTC as future work. Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one [27] .",Extention,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",train,1,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
845,1621,3139,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Recently, [7, 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy.",Background,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
846,1622,3140,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"In our previous work [7] , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge.",Background,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
847,1624,3142,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Previous research used multi-modal information independently using neural network model by concatenating features from each modality [7, 21] .",Background,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
848,1625,3143,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"In our previous work [7] , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework.",Extention,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
849,1626,3144,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"As this research is extended work from previous research [7] , we use the same feature extraction method as done in our previous work.",Extention,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
850,1627,3145,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Motivated by the architecture used in [7, 17, 19] , we train a recurrent encoder to predict the categorical class of a given audio signal.",Motivation,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
851,1628,3146,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"For consistent comparison with previous works [7, 18] , all utterances labeled ""excitement"" are merged with those labeled ""happiness"".",Uses,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
852,1629,3147,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"We use the same dataset and features as other researchers [7, 18] .",Uses,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
853,1630,3148,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Recently, [7, 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy. However, none of these studies utilized attention method over audio and text modality in tandem for contextual understanding of the emotion in audio recording.",Motivation,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
854,1631,3149,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"To follow previous research [7] , we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o A t .",Uses,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
855,1632,3150,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Previous research used multi-modal information independently using neural network model by concatenating features from each modality [7, 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa).",Difference,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
856,1633,3151,ABC_2a84615479af66bbf875517a3a753b_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"In audio-BRE (Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of [7, 25] .",Similar,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",train,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
857,2489,5140,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,"Regarding massively multilingual models, Neubig and Hu (2018) explored methods for rapid adaptation of NMT to new languages by training multilingual models on the 59-language TED Talks corpus and fine-tuning them using data from the new languages.",Background,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,0,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
858,2490,5141,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,One recent exception is Neubig and Hu (2018) which trained many-to-one models from 58 languages into English.,Background,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,0,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
859,2492,5143,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,"Our experiments on the publicly available TED talks dataset (Qi et al., 2018) show that massively multilingual many-to-many models with up to 58 languages to-and-from English are very effective in low resource settings, allowing to use high-capacity models while avoiding overfitting and achieving superior results to the current stateof-the-art on this dataset (Neubig and Hu, 2018; Wang et al., 2019) when translating into English.",Uses,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,1,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
860,2493,5144,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,"Regarding the languages we evaluate on, we begin with the same four languages as Neubig and Hu (2018) -Azerbeijani (Az), Belarusian (Be), Galician (Gl) and Slovak (Sk).",Uses,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,1,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
861,2494,5145,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,We also compare our massively multilingual models to bilingual baselines and to two recently published results on this dataset (Neubig and Hu (2018) ; Wang et al. (2019) ).,Uses,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,1,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
862,2495,5146,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,"We first note that our many-to-many model outperforms all other models when translating into English, with 1.82 BLEU improvement (when av-eraged across the four language pairs) over the best fine-tuned many-to-one models of Neubig and Hu (2018) and 2.44 BLEU improvement over our many-to-one model when averaged across the four low-resource language pairs (Table 1) .",Difference,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,0,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
863,2496,5147,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,We also note that our many-to-one model is on average 0.75 BLEU behind the best many-to-one models in Neubig and Hu (2018) .,Difference,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,0,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
864,2500,5151,ABC_b335178d833e26190b7056469d3fa7_30,ARXIV:1903.00089,205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4,We use tokenized BLEU in order to be comparable with Neubig and Hu (2018) .,Uses,s2,"Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.","This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual “seed models”, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of “similar-language regularization”, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",train,1,"The research focuses on the challenges and effectiveness of massively multilingual Neural Machine Translation (NMT), specifically training a single model that can handle translations between numerous source and target languages, especially in low-resource settings. The motivation behind this research is to demonstrate the effectiveness of massively multilingual NMT models in low-resource settings, surpassing previous state-of-the-art methods and encouraging future research in this area. They also aim to explore the potential of these models by analyzing trade-offs between translation quality and various modeling decisions.",dev,"low-resource settings, massively multilingual Neural Machine Translation ( NMT ), massively multilingual NMT models, model, modeling decisions, models, state-of-the-art methods, translation quality"
865,3334,6730,ABC_15bacab4a8c520cfcdd7e7bd1e9ec5_46,ACL:N19-5001,73047a0f0192a35d3b5c6f5ebeadf3706b17e4dc,"(Yu et al., 2017; Li et al., 2017; Wang and Lee, 2018; Additionally, we will also introduce other technical focuses such as negative sampling and contrastive estimation (Cai and Wang, 2018; Bose et al., 2018 ), adversarial evaluation (Elliott, 2018 , and reward learning (Wang et al., 2018c) .",Unsure,s2,"Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. Adversarial learning is also a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming at providing some practical perspectives on the future of adversarial learning for solving real-world NLP problems.","We introduce KBGAN, an adversarial learning framework to improve the performances of a wide range of existing knowledge graph embedding models. Because knowledge graphs typically only contain positive facts, sampling useful negative training examples is a nontrivial task. Replacing the head or tail entity of a fact with a uniformly randomly selected entity is a conventional method for generating negative facts, but the majority of the generated negative facts can be easily discriminated from positive facts, and will contribute little towards the training. Inspired by generative adversarial networks (GANs), we use one knowledge graph embedding model as a negative sample generator to assist the training of our desired model, which acts as the discriminator in GANs. This framework is independent of the concrete form of generator and discriminator, and therefore can utilize a wide variety of knowledge graph embedding models as its building blocks. In experiments, we adversarially train two translation-based models, TRANSE and TRANSD, each with assistance from one of the two probability-based models, DISTMULT and COMPLEX. We evaluate the performances of KBGAN on the link prediction task, using three knowledge base completion datasets: FB15k-237, WN18 and WN18RR. Experimental results show that adversarial training substantially improves the performances of target embedding models under various settings.",train,0,"The research problem lies in the difficulty of designing adversarial learning models for NLP due to the discrete nature of language, despite its success in Computer Vision and other areas. The tutorial is motivated by the desire to bridge the gap between the successful application of adversarial learning in other fields and its challenges in NLP, aiming to provide practical solutions and insights for the future of adversarial learning in real-world NLP problems.",dev,"adversarial learning, adversarial learning models, discrete nature of language, NLP, real-world NLP problems"
866,3336,6732,ABC_15bacab4a8c520cfcdd7e7bd1e9ec5_46,ACL:N19-5001,176f1d608b918eec8dc4b75e7b6e0acaba84a447,"Finally, we provide an in-depth case study of deploying two-agent GAN models for conversational AI (Li et al., 2017) .",Uses,s2,"Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. Adversarial learning is also a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming at providing some practical perspectives on the future of adversarial learning for solving real-world NLP problems.","We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines",train,1,"The research problem lies in the difficulty of designing adversarial learning models for NLP due to the discrete nature of language, despite its success in Computer Vision and other areas. The tutorial is motivated by the desire to bridge the gap between the successful application of adversarial learning in other fields and its challenges in NLP, aiming to provide practical solutions and insights for the future of adversarial learning in real-world NLP problems.",dev,"adversarial learning, adversarial learning models, discrete nature of language, NLP, real-world NLP problems"
867,3337,6735,ABC_15bacab4a8c520cfcdd7e7bd1e9ec5_46,ACL:N19-5001,176f1d608b918eec8dc4b75e7b6e0acaba84a447,"We will introduce an in-depth case study of Generative Adversarial Networks for NLP, with a focus on dialogue generation (Li et al., 2017) .",Background,s2,"Adversarial learning is a game-theoretic learning paradigm, which has achieved huge successes in the field of Computer Vision recently. Adversarial learning is also a general framework that enables a variety of learning models, including the popular Generative Adversarial Networks (GANs). Due to the discrete nature of language, designing adversarial learning models is still challenging for NLP problems. In this tutorial, we provide a gentle introduction to the foundation of deep adversarial learning, as well as some practical problem formulations and solutions in NLP. We describe recent advances in deep adversarial learning for NLP, with a special focus on generation, adversarial examples & rules, and dialogue. We provide an overview of the research area, categorize different types of adversarial learning models, and discuss pros and cons, aiming at providing some practical perspectives on the future of adversarial learning for solving real-world NLP problems.","We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator—analagous to the human evaluator in the Turing test— to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines",train,0,"The research problem lies in the difficulty of designing adversarial learning models for NLP due to the discrete nature of language, despite its success in Computer Vision and other areas. The tutorial is motivated by the desire to bridge the gap between the successful application of adversarial learning in other fields and its challenges in NLP, aiming to provide practical solutions and insights for the future of adversarial learning in real-world NLP problems.",dev,"adversarial learning, adversarial learning models, discrete nature of language, NLP, real-world NLP problems"
868,4738,9833,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"We also apply the orthogonalization update to the mappers following <span style=""background: yellow; display: inline-block"">Conneau et al. (2018)</span> with β = 0.01.",Uses,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
869,4739,9834,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"For selecting the best model, we use the unsupervised validation criterion proposed by <span style=""background: yellow; display: inline-block"">Conneau et al. (2018)</span> , which correlates highly with the mapping quality.",Uses,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
870,4740,9835,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"Since our approach follows the same steps as <span style=""background: yellow; display: inline-block"">Conneau et al. (2018),</span> we first compare our proposed model with <span style=""background: yellow; display: inline-block"">their model</span> on European (Table 1) , non-European and low-resource languages (Table  2 ) on <span style=""background: yellow; display: inline-block"">their dataset</span>.",Uses,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
871,4741,9836,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"However, it is important Table 5 : Ablation study of our adversarial autoencoder model on the dataset of <span style=""background: yellow; display: inline-block"">Conneau et al. (2018)</span>.",Uses,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,1,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
872,4742,9837,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"Since our approach follows the same steps as <span style=""background: yellow; display: inline-block"">Conneau et al. (2018),</span> we first compare our proposed model with <span style=""background: yellow; display: inline-block"">their model</span> on European (Table 1) , non-European and low-resource languages (Table  2 ) on <span style=""background: yellow; display: inline-block"">their dataset</span>.",Difference,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
873,4743,9838,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"If we compare the results of ---Cycle with <span style=""background: yellow; display: inline-block"">Conneau-18,</span> we see sizeable gains for En-Es in both directions.",Difference,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
874,4744,9839,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"These improvements demonstrate that our model finds a better mapping compared to <span style=""background: yellow; display: inline-block"">Conneau et al. (2018)</span> .",Difference,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
875,4745,9840,ABC_5b14e259a557aa3cbfcbd6265f04c8_1,ACL:N19-1386,634b8216dcd15c19680d3790df117beb456e5daa,"Since our approach follows the same steps as <span style=""background: yellow; display: inline-block"">Conneau et al. (2018),</span> we first compare our proposed model with <span style=""background: yellow; display: inline-block"">their model</span> on European (Table 1) , non-European and low-resource languages (Table  2 ) on <span style=""background: yellow; display: inline-block"">their dataset</span>.",Similar,llm,"Adversarial training has shown impressive success in learning bilingual dictionary without any parallel data by mapping monolingual embeddings to a shared space. However, recent work has shown superior performance for non-adversarial methods in more challenging language pairs. In this work, we revisit adversarial autoencoder for unsupervised word translation and propose two novel extensions to it that yield more stable training and improved results. Our method includes regularization terms to enforce cycle consistency and input reconstruction, and puts the target encoders as an adversary against the corresponding discriminator. Extensive experimentations with European, non-European and low-resource languages show that our method is more robust and achieves better performance than recently proposed adversarial and non-adversarial approaches.","The study was aimed at observing the number conservation ability of EarlyChildhood (Pre-primary children) to test the Piaget’s theory of cognitivedevelopment. The major objective of the study was to investigate therelevance of curriculum with cognitive development of Early Childhood(Pre-primary) children. An empirical research design was used. All thegovernment and private Early Childhood (Pre-primary children) of KhyberPakhtunkhwa were population of the study. 240 students of pre-primarychildren were taken as a sample of the study. For data collection,observation sheets were developed. Statistical technique of chi-square wasapplied to analyze the collected data. It was found that primary schoolchildren of age 3 to 5 were not number conservers at any case. Hence itwas concluded that Early Childhood (Pre-primary children) were notfound to be number conservers which reflects that the level of numberconcepts (numeracy) given in National curriculum (Mathematics) ECE2002 (3 to 5 years) was higher than the cognitive level of Early Childhood(Pre-primary children) and hence it was not found to be suitable (withrespect to number conservancy) due to the non-conservancy of the EarlyChildhood (Pre-primary children). It is recommended that the numberconcepts (numeracy) given in National Curriculum for Mathematics ECE2002 (3 to 5 years) to be revised according to the cognitive level of EarlyChildhood (Pre-primary children). It should be reviewed on the basis ofindigenous research studies.",train,0,"The research addresses the limitations of adversarial autoencoder methods in unsupervised word translation, particularly in the context of challenging language pairs where non-adversarial methods have shown superior performance. The aim is to improve the stability and performance of adversarial autoencoder-based word translation. The research is driven by the desire to overcome the limitations of adversarial methods in unsupervised word translation, particularly in challenging language scenarios. The goal is to develop a more robust and efficient approach for bilingual dictionary learning without relying on parallel data.",dev,"adversarial autoencoder methods, adversarial autoencoder-based word translation, adversarial methods, approach, bilingual dictionary learning, challenging language pairs, challenging language scenarios, non-adversarial methods, parallel data, unsupervised word translation"
876,4956,10333,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"A window center might contain 1 or more words (e.g., ""8 AM"") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL<span style=""background: yellow; display: inline-block""> (Sproat and Jaitly, 2016)</span> .",Uses,llm,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
877,4957,10334,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Following <span style=""background: yellow; display: inline-block"">Sproat and Jaitly (2016)</span>, we implement a seq2seq model trained on window-based data.",Uses,llm,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
878,4958,10335,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"* TELEPHONE is not reported in <span style=""background: yellow; display: inline-block"">Sproat and Jaitly (2016)</span> but included in the dataset; ** we removed ELECTRONIC category.",Extention,llm,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
879,4959,10336,ABC_73d7831596bfe6d6861f360042048f_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"* TELEPHONE is not reported in <span style=""background: yellow; display: inline-block"">Sproat and Jaitly (2016)</span> but included in the dataset; ** we removed ELECTRONIC category.",Difference,llm,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",train,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
880,127,887,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; Belinkov and Bisk, 2018) .",Background,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
881,128,888,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,Belinkov and Bisk (2018) report significant degradations in performance after applying noise to only a small fraction of input tokens.,Background,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
882,129,889,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"Belinkov and Bisk (2018) experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token.",Background,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
883,130,890,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"Table 2 shows the performance of the model on data with varying amounts of natural orthographical errors (see Section 2.2). As observed in prior art (Heigold et al., 2017; Belinkov and Bisk, 2018) , when there are significant amounts of natural noise, the model's performance drops significantly.",Background,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
884,131,891,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,4343d3edd00a85171db0351baca54e8678feffdd,"Most relevant for us is the work of Belinkov and Bisk (2018) , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . They find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text.",Background,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic text processing. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), a new freely-available resource built by automatically mining Wikipedias revision history. The WiCoPaCo corpus focuses on local modifications made by human revisors and include various types of corrections (such as spelling error or typographical corrections) and rewritings, which can be categorized broadly into meaning-preserving and meaning-altering revisions. We present an initial hand-built typology of these revisions, but the resource allows for any possible annotation scheme. We discuss the main motivations for building such a resource and describe the main technical details guiding its construction. We also present applications and data analysis on French and report initial results on spelling error correction and morphosyntactic rewriting. The WiCoPaCo corpus can be freely downloaded from http://wicopaco.limsi.fr.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
885,132,892,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; Belinkov and Bisk, 2018) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length.",Background,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
886,133,893,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"Belinkov and Bisk (2018) experiment with a bag of characters, while Sakaguchi et al. (2017) use character RNNs combined with special representations for the first and last characters of each token. These models are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise.",Motivation,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
887,134,894,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"Substitutions and swaps were experimented with extensively in previous work (Heigold et al., 2017; Belinkov and Bisk, 2018) , but deletion and insertion were not. Deletion and insertion pose a different challenge to character encoders, since they alter the distances between character sequences in the word, as well as its overall length.",Motivation,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
888,135,895,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,3e85fbde18cd4d9bb36aa7f227b84f78a2390cd2,"Systems that are trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; Belinkov and Bisk, 2018) . One potential solution is to introduce noise at training time, an approach that is similar in spirit to the use of adversarial examples in other areas of machine learning (Goodfellow et al., 2014) and natural language processing (Ebrahimi et al., 2018) . So far, using synthetic noise at training time has been found to only improve performance on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; Belinkov and Bisk, 2018) . We desire methods that yield good performance on both clean text as well as naturally-occurring noise, but this is beyond the reach of current techniques.",Motivation,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Small perturbations in the input can severely distort intermediate representations and thus impact translation quality of neural machine translation (NMT) models. In this paper, we propose to improve the robustness of NMT models with adversarial stability training. The basic idea is to make both the encoder and decoder in NMT models robust against input perturbations by enabling them to behave similarly for the original input and its perturbed counterpart. Experimental results on Chinese-English, English-German and English-French translation tasks show that our approaches can not only achieve significant improvements over strong NMT systems but also improve the robustness of NMT models.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
889,136,896,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,4343d3edd00a85171db0351baca54e8678feffdd,"Most relevant for us is the work of Belinkov and Bisk (2018) , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . They find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by Belinkov and Bisk -are essential to achieving robustness to natural noise.",Difference,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic text processing. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), a new freely-available resource built by automatically mining Wikipedias revision history. The WiCoPaCo corpus focuses on local modifications made by human revisors and include various types of corrections (such as spelling error or typographical corrections) and rewritings, which can be categorized broadly into meaning-preserving and meaning-altering revisions. We present an initial hand-built typology of these revisions, but the resource allows for any possible annotation scheme. We discuss the main motivations for building such a resource and describe the main technical details guiding its construction. We also present applications and data analysis on French and report initial results on spelling error correction and morphosyntactic rewriting. The WiCoPaCo corpus can be freely downloaded from http://wicopaco.limsi.fr.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
890,137,897,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"As observed in prior art (Heigold et al., 2017; Belinkov and Bisk, 2018) , when there are significant amounts of natural noise, the model's performance drops significantly. However, training on our synthetic noise cocktail greatly improves performance, regaining between 20% (Czech) and 50% (German) of the BLEU score that was lost to natural noise.",Difference,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
891,138,898,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"The fact that we use deletion and insertion also explains why our model was able to regain a significant portion of its original performance when confronted with natural noise at test time, while previous work that trained only on substitutions and swaps was not able to do so (Belinkov and Bisk, 2018) .",Extention,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,1,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
892,139,899,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,4343d3edd00a85171db0351baca54e8678feffdd,"Most relevant for us is the work of Belinkov and Bisk (2018) , who evaluated on natural noise obtained from Wikipedia edit histories (e.g., Max and Wisniewski, 2010) . They find that robustness to natural noise can be obtained by training on the same noise model, but that (a) training on synthetic noise does not yield robustness to natural noise at test time, and (b) training on natural noise significantly impairs performance on clean text. In contrast, we show that training on the right kind and the right amount of synthetic noise can yield substantial improvements on natural noise at test time, without significantly impairing performance on clean data. Our ablation results suggest that deletion and insertion noise -which were not included by Belinkov and Bisk -are essential to achieving robustness to natural noise.",Extention,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic text processing. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), a new freely-available resource built by automatically mining Wikipedias revision history. The WiCoPaCo corpus focuses on local modifications made by human revisors and include various types of corrections (such as spelling error or typographical corrections) and rewritings, which can be categorized broadly into meaning-preserving and meaning-altering revisions. We present an initial hand-built typology of these revisions, but the resource allows for any possible annotation scheme. We discuss the main motivations for building such a resource and describe the main technical details guiding its construction. We also present applications and data analysis on French and report initial results on spelling error correction and morphosyntactic rewriting. The WiCoPaCo corpus can be freely downloaded from http://wicopaco.limsi.fr.",dev,1,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
893,140,900,ABC_13091dd4d06e11957a5cb7785b92d4_5,ARXIV:1902.01509,765bdcf27ebc1eb03a14f1e47aefa4dda1e03073,"The fact that we use deletion and insertion also explains why our model was able to regain a significant portion of its original performance when confronted with natural noise at test time, while previous work that trained only on substitutions and swaps was not able to do so (Belinkov and Bisk, 2018) .",Difference,s2,"Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natural noise data or distribution.","Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.",dev,0,"Current machine translation systems, while achieving broad coverage, suffer from a significant weakness: vulnerability to orthographic variations, particularly spelling errors. The motivation behind this research is to address the problem of sensitivity to spelling mistakes in machine translation systems. The goal is to develop a robust translation method that can handle real-world typos effectively, even without access to specific typo data.",dev,"machine translation systems, orthographic variations, real-world typos, robust translation method, spelling errors, spelling mistakes"
894,551,2660,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"One can also assign interpretations; for example, [27] argue their LAS self-attention heads are differentiated phoneme detectors.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
895,552,2661,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"We see that unlike self-attentional LAS [27] , SAN-CTC works respectably even with no position en- coding; in fact, the contribution of position is relatively minor (compare with [21] , where location in an encoder-decoder system improved CER by 3% absolute).",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
896,553,2662,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,abd91aca4d78799492256b406f5abc199d3802e4,"Hybrid self-attention/LSTM encoders were studied in the context of listenattend-spell (LAS) [27] , and the Transformer was directly adapted to speech in [19, 28, 29] ; both are encoder-decoder systems.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Sequence-to-sequence attention-based models integrate an acoustic, pronunciation and language model into a single neural network, which make them very suitable for multilingual automatic speech recognition (ASR). In this paper, we are concerned with multilingual speech recognition on low-resource languages by a single Transformer, one of sequence-to-sequence attention-based models. Sub-words are employed as the multilingual modeling unit without using any pronunciation lexicon. First, we show that a single multilingual ASR Transformer performs well on low-resource languages despite of some language confusion. We then look at incorporating language information into the model by inserting the language symbol at the beginning or at the end of the original sub-words sequence under the condition of language information being known during training. Experiments on CALLHOME datasets demonstrate that the multilingual ASR Transformer with the language symbol at the end performs better and can obtain relatively 10.5\% average word error rate (WER) reduction compared to SHL-MLSTM with residual learning. We go on to show that, assuming the language information being known during training and testing, about relatively 12.4\% average WER reduction can be observed compared to SHL-MLSTM with residual learning through giving the language symbol as the sentence start token.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
897,554,2663,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"In the first layers, we similarly observe a differentiation of variances, along with wide-context heads; in later layers, unlike [27] we still see mild differentiation of variances.",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
898,555,2664,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences [27] to train self-attention for ASR.",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
899,556,2665,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"In this work, we propose and evaluate fully self-attentional networks for CTC (SAN-CTC). Unlike past works, we do not require convolutional frontends [19] or interleaved recurrences [27] to train self-attention for ASR.",Motivation,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
900,557,2666,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
901,558,2667,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"The latter was found necessary for self-attentional LAS [27] , as additive encodings did not give convergence.",Motivation,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
902,559,2668,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"Inspired by [27] , we plot the standard deviation of attention weights for each head as training progresses; see Figure 2 for details.",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
903,560,2669,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"While in theory, a relatively local context could suffices for ASR, this is complicated by alphabets L which violate the conditional independence assumption of CTC (e.g., English characters [36] ). Wide contexts also enable incorporation of noise/speaker contexts, as [27] suggest regarding the broad-context attention heads in the first layer of their self-attentional LAS model.",Motivation,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",dev,0,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
904,561,2671,ABC_154fd8e6b625eb93da21c09906ee90_14,ARXIV:1901.10055,c52ac453e154953abdb06fc041023e327ea609a4,"Instead, we consider three fixed approaches, from least-to most-preserving of the input data: subsampling, which only takes every k-th frame; pooling, which aggregates every k consecutive frames via a statistic (average, maximum); reshaping, where one concatenates k consecutive frames into one [27] .",Uses,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",dev,1,"The research addresses the problem of improving end-to-end speech recognition, specifically aiming to develop a more effective and efficient model than existing CTC and encoder-decoder architectures. The motivation for this research stems from the successful application of self-attention in NLP and the desire to leverage its benefits for end-to-end speech recognition. The authors aim to improve upon traditional CTC models and encoder-decoder architectures for better speech recognition performance.",dev,"CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, model, NLP, self-attention, speech recognition"
905,1019,4369,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"For En-De we replicate the setup of Vaswani et al. (2017) which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.",Similar,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
906,1020,4370,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"All results use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. 2017 .",Similar,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
907,1021,4371,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"We provide several popular schedulers, e.g., the inverse square-root scheduler from Vaswani et al. (2017) and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016) .",Uses,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,1,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
908,1022,4372,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"We provide several popular schedulers, e.g., the inverse square-root scheduler from Vaswani et al. (2017) and cyclical schedulers based on warm restarts (Loshchilov and Hutter, 2016) .",Similar,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
909,1023,4373,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"For En-De we replicate the setup of Vaswani et al. (2017) which relies on WMT'16 for training with 4.5M sentence pairs, we validate on newstest13 and test on newstest14.",Uses,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,1,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
910,1024,4374,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,fea820b7d953d32069e189af2961c28fd213470b,"FAIRSEQ provides fast inference for non-recurrent models (Gehring et al., 2017; Vaswani et al., 2017; Fan et al., 2018b; Wu et al., 2019) through incremental decoding, where the model states of previously generated tokens are cached in each active beam and re-used.",Background,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.",dev,0,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
911,1025,4375,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,fea820b7d953d32069e189af2961c28fd213470b,"We provide reference implementations of several popular sequence-to-sequence models which can be used for machine translation, including LSTM (Luong et al., 2015) , convolutional models (Gehring et al., 2017; Wu et al., 2019) and Transformer (Vaswani et al., 2017) .",Background,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.",dev,0,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
912,1026,4376,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"All results use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. 2017 .",Uses,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,1,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
913,1027,4377,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,bf8fe437f779f2098f9af82b534aa51dc9edb06f,We reported improved BLEU scores over Vaswani et al. (2017) by training with a bigger batch size and an increased learning rate .,Difference,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT’14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT’14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",dev,0,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
914,1028,4378,ABC_242aacd35fb92d836fea9eb33961a3_24,ACL:N19-4009,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"FAIRSEQ supports language modeling with gated convolutional models and Transformer models (Vaswani et al., 2017) .",Background,s2,"fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",dev,0,"The research problem is to facilitate the development and training of custom models for various text generation tasks, such as translation, summarization, and language modeling. The challenge lies in providing a toolkit that enables researchers and developers to efficiently create and train these models. The motivation behind fairseq is to provide a user-friendly and efficient toolkit that empowers researchers and developers to build and train custom sequence models for various text generation tasks. This aims to advance research and development in the field of sequence modeling.",dev,"custom models, fairseq, language modeling, models, sequence modeling, sequence models, summarization, text generation tasks, This, translation"
915,676,2483,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field (Sproat and Jaitly, 2016) .",Background,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
916,679,2487,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,The data for the window-based seq2seq model and full sentence seq2seq were generated from the publicly available release of parallel written/speech formatted text from Sproat and Jaitly (2016) .,Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
917,680,2488,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Constructing such grammars is time consuming and error-prone and requires extensive linguistic knowledge and programming proficiency. Recently, with the rise of machine learning and especially deep learning techniques, researchers are starting to bring more data-driven approaches to this field (Sproat and Jaitly, 2016) .",Motivation,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
918,681,2489,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"We follow Sproat and Jaitly (2016) in down-sampling window-based training data to constrain the proportion of ""<self>"" tokens to 10% of the data.",Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
919,682,2490,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,Our first approach replicates the window-based seq2seq model of Sproat and Jaitly (2016) .,Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
920,683,2491,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Our datasets were randomly sampled from a set of 4.9M sentences in the training data portion of the Sproat and Jaitly (2016) data release and split into training, validation, and test data.",Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
921,684,2492,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Data with TELEPHONE labels were not included in the initial analysis of Sproat and Jaitly (2016) , but were made available in the dataset release.",Difference,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
922,685,2493,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"As shown in Figure 2 , our replicated windowbased model achieves reasonable performance compared with Sproat and Jaitly (2016) , considering our training set is much smaller.",Similar,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
923,686,2494,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Our labels are generated directly from the Google FST (Sproat and Jaitly, 2016) .",Uses,s2,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
924,860,3197,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Recently, [7, 18] combined acoustic information and conversation transcripts using a neural network-based model to improve emotion classification accuracy.",Background,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
925,861,3199,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Previous research used multi-modal information independently using neural network model by concatenating features from each modality [7, 21] .",Background,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
926,862,3200,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"In our previous work [7] , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge. In this paper, we improve upon our earlier work by incorporating an attention mechanism in the emotion recognition framework.",Extention,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
927,863,3201,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"In our previous work [7] , we applied a dual RNN in order to obtain a richer representation by blending the content and acoustic knowledge.",Background,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
928,864,3202,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Motivated by the architecture used in [7, 17, 19] , we train a recurrent encoder to predict the categorical class of a given audio signal.",Motivation,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
929,865,3203,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"To follow previous research [7] , we also add another prosodic feature vector, p, with each ot to generate a more informative vector representation of the signal, o A t .",Uses,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
930,866,3204,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"We use the same dataset and features as other researchers [7, 18] .",Uses,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
931,867,3205,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"For consistent comparison with previous works [7, 18] , all utterances labeled ""excitement"" are merged with those labeled ""happiness"".",Uses,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
932,868,3206,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"As this research is extended work from previous research [7] , we use the same feature extraction method as done in our previous work.",Extention,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,1,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
933,869,3207,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"Previous research used multi-modal information independently using neural network model by concatenating features from each modality [7, 21] . As opposed to this approach, we propose a neural network architecture that exploits information in each modality by extracting relevant segments of the speech data using information from the lexical content (and vice-versa).",Difference,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
934,870,3213,ABC_0fd26c6dffab3fba2d120d2c58dff6_17,ARXIV:1904.10788,d70ba7cb3872b3df5abadbb230bfaf57b02e7cca,"In audio-BRE ( Fig. 2(a) ), most of the emotion labels are frequently misclassified as neutral class, supporting the claims of [7, 25] .",Similar,s2,"In this paper, we are interested in exploiting textual and acoustic data of an utterance for the speech emotion classification task. The baseline approach models the information from audio and text independently using two deep neural networks (DNNs). The outputs from both the DNNs are then fused for classification. As opposed to using knowledge from both the modalities separately, we propose a framework to exploit acoustic information in tandem with lexical data. The proposed framework uses two bi-directional long short-term memory (BLSTM) for obtaining hidden representations of the utterance. Furthermore, we propose an attention mechanism, referred to as the multi-hop, which is trained to automatically infer the correlation between the modalities. The multi-hop attention first computes the relevant segments of the textual data corresponding to the audio signal. The relevant textual data is then applied to attend parts of the audio signal. To evaluate the performance of the proposed system, experiments are performed in the IEMOCAP dataset. Experimental results show that the proposed technique outperforms the state-of-the-art system by 6.5% relative improvement in terms of weighted accuracy.","Speech emotion recognition is a challenging task, and extensive reliance has been placed on models that use audio features in building well-performing classifiers. In this paper, we propose a novel deep dual recurrent encoder model that utilizes text data and audio signals simultaneously to obtain a better understanding of speech data. As emotional dialogue is composed of sound and spoken content, our model encodes the information from audio and text sequences using dual recurrent neural networks (RNNs) and then combines the information from these sources to predict the emotion class. This architecture analyzes speech data from the signal level to the language level, and it thus utilizes the information within the data more comprehensively than models that focus on audio features. Extensive experiments are conducted to investigate the efficacy and properties of the proposed model. Our proposed model outperforms previous state-of-the-art methods in assigning data to one of four emotion categories (i.e., angry, happy, sad and neutral) when the model is applied to the IEMOCAP dataset, as reflected by accuracies ranging from 68.8% to 71.8%.",test,0,"The research problem is to improve the accuracy of speech emotion classification by leveraging both textual and acoustic data from utterances. The motivation behind this research is to develop a more effective method for speech emotion classification by integrating textual and acoustic data, as opposed to treating them independently. The research aims to overcome the limitations of traditional approaches that separate these modalities, thereby achieving improved performance in speech emotion recognition.",dev,"accuracy, approaches, method, modalities, speech emotion classification, speech emotion recognition, textual and acoustic data, them"
935,1235,4326,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,adc276e6eae7051a027a4c269fb21dae43cadfed,"Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","
 
 Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely used on NLP tasks to capture the long-term and local dependencies, respectively. Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation, significantly less training time, and flexibility in modeling dependencies. We propose a novel attention mechanism in which the attention between elements from input sequence(s) is directional and multi-dimensional (i.e., feature-wise). A light-weight neural net, ""Directional Self-Attention Network (DiSAN),"" is then proposed to learn sentence embedding, based solely on the proposed attention without any RNN/CNN structure. DiSAN is only composed of a directional self-attention with temporal order encoded, followed by a multi-dimensional attention that compresses the sequence into a vector representation. Despite its simple form, DiSAN outperforms complicated RNN models on both prediction quality and time efficiency. It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02% on the Stanford Natural Language Inference (SNLI) dataset, and shows state-of-the-art test accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK), Customer Review, MPQA, TREC question-type classification and Subjectivity (SUBJ) datasets.
 
",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
936,1237,4328,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"The first sublayer performs multi-head, scaled dot-product, self-attention [22] .",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
937,1238,4329,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Self-attention is inherently content-based [22] , and so one often encodes position into the post-embedding vectors.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
938,1239,4330,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
939,1240,4331,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,41a78e2885b5dc8c719495a33985b5f4880f5b48,"As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, 22] .",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
940,1241,4332,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,The second sublayer is a position-wise feed-forward network [22] FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters,Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
941,1242,4333,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",Uses,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",test,1,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
942,1243,4334,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
943,1244,4336,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Maximum path length Table 1 : Operation complexity of each layer type, based on [22] .",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
944,1245,4337,ABC_642aa9fe999d0b2b3793cb1603c04c_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Maximum path length Table 1 : Operation complexity of each layer type, based on [22] .",Uses,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,1,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
945,1246,4383,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,adc276e6eae7051a027a4c269fb21dae43cadfed,"Its use in both encoder-decoder and feedforward contexts has led to faster training and state-of-the-art results in translation (via the Transformer [22] ), sentiment analysis [25] , and other tasks.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","
 
 Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely used on NLP tasks to capture the long-term and local dependencies, respectively. Attention mechanisms have recently attracted enormous interest due to their highly parallelizable computation, significantly less training time, and flexibility in modeling dependencies. We propose a novel attention mechanism in which the attention between elements from input sequence(s) is directional and multi-dimensional (i.e., feature-wise). A light-weight neural net, ""Directional Self-Attention Network (DiSAN),"" is then proposed to learn sentence embedding, based solely on the proposed attention without any RNN/CNN structure. DiSAN is only composed of a directional self-attention with temporal order encoded, followed by a multi-dimensional attention that compresses the sequence into a vector representation. Despite its simple form, DiSAN outperforms complicated RNN models on both prediction quality and time efficiency. It achieves the best test accuracy among all sentence encoding methods and improves the most recent best result by 1.02% on the Stanford Natural Language Inference (SNLI) dataset, and shows state-of-the-art test accuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language inference (MultiNLI), Sentences Involving Compositional Knowledge (SICK), Customer Review, MPQA, TREC question-type classification and Subjectivity (SUBJ) datasets.
 
",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
946,1247,4384,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,The second sublayer is a position-wise feed-forward network [22] FFN(H) = ReLU(HW1 + b1)W2 + b2 where parameters,Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
947,1248,4385,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Self-attention is inherently content-based [22] , and so one often encodes position into the post-embedding vectors.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
948,1249,4386,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Recently, the mechanism of self-attention [22, 24] was proposed, which uses the whole sequence at once to model feature interactions that are arbitrarily distant in time.",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
949,1250,4387,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",Uses,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",test,1,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
950,1251,4388,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"The first sublayer performs multi-head, scaled dot-product, self-attention [22] .",Background,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
951,1252,4389,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,41a78e2885b5dc8c719495a33985b5f4880f5b48,"Our proposed framework ( Figure 1a ) is built around self-attention layers, as used in the Transformer encoder [22] , previous explorations of self-attention in ASR [19, 27] , and defined in Section 2.3.",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
952,1253,4390,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Maximum path length Table 1 : Operation complexity of each layer type, based on [22] .",Uses,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,1,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
953,1254,4391,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,41a78e2885b5dc8c719495a33985b5f4880f5b48,"As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, 22] .",Difference,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
954,1255,4392,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,41a78e2885b5dc8c719495a33985b5f4880f5b48,"As self-attention architectures can be unstable in early training, we clip gradients to a global norm of 1 and use the standard linear warmup period before inverse square decay associated with these architectures [19, 22] .",Extention,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","Recurrent sequence-to-sequence models using encoder-decoder architecture have made great progress in speech recognition task. However, they suffer from the drawback of slow training speed because the internal recurrence limits the training parallelization. In this paper, we present the Speech-Transformer, a no-recurrence sequence-to-sequence model entirely relies on attention mechanisms to learn the positional dependencies, which can be trained faster with more efficiency. We also propose a 2D-Attention mechanism, which can jointly attend to the time and frequency axes of the 2-dimensional speech inputs, thus providing more expressive representations for the Speech-Transformer. Evaluated on the Wall Street Journal (WSJ) speech recognition dataset, our best model achieves competitive word error rate (WER) of 10.9%, while the whole training process only takes 1.2 days on 1 GPU, significantly faster than the published results of recurrent sequence-to-sequence models.",test,1,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
955,1256,4396,ABC_0e4deda746127b97f68080bc8f13c8_24,CorpusID:59336132,204e3073870fae3d05bcbc2f6a8e263d9b72e776,"Maximum path length Table 1 : Operation complexity of each layer type, based on [22] .",Similar,s2,"The success of self-attention in NLP has led to recent applications in end-to-end encoder-decoder architectures for speech recognition. Separately, connectionist temporal classification (CTC) has matured as an alignment-free, non-autoregressive approach to sequence transduction, either by itself or in various multitask and decoding frameworks. We propose SAN-CTC, a deep, fully self-attentional network for CTC, and show it is tractable and competitive for end-to-end speech recognition. SAN-CTC trains quickly and outperforms existing CTC models and most encoder-decoder models, with character error rates (CERs) of 4.7% in 1 day on WSJ eval92 and 2.8% in 1 week on LibriSpeech test-clean, with a fixed architecture and one GPU. Similar improvements hold for WERs after LM decoding. We motivate the architecture for speech, evaluate position and down-sampling approaches, and explore how label alphabets (character, phoneme, subword) affect attention heads and performance.","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",test,0,"The research aims to improve end-to-end speech recognition by proposing a novel, fully self-attentional network for CTC (Connectionist Temporal Classification) called SAN-CTC. The goal is to enhance the performance of speech recognition systems, surpassing existing CTC models and encoder-decoder architectures. The research is motivated by the success of self-attention in NLP and its potential for speech recognition. It also seeks to leverage the advantages of CTC, an alignment-free and non-autoregressive approach, for sequence transduction. The goal is to create a faster and more accurate end-to-end speech recognition system by exploring the potential of self-attention in the context of CTC.",dev,"alignment-free and non-autoregressive approach, CTC, CTC models, encoder-decoder architectures, end-to-end speech recognition, end-to-end speech recognition system, fully self-attentional network, It, NLP, SAN-CTC, self-attention, sequence transduction, speech recognition, speech recognition systems"
956,3016,10330,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Table 1 illustrates the window-based model's training examples corresponding to one sentence ""wake me up at 8 AM ."" which is broken down into 6 pairs. <n> and </n> indicate the center of the window. A window center might contain 1 or more words (e.g., ""8 AM"") and the grouping is provided by the dataset where each input sentence is segmented into chunks corresponding to labels such as TIME, DATE, ORDINAL<span style=""background: yellow; display: inline-block""> (Sproat and Jaitly, 2016)</span> .",Uses,llm,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
957,3017,10331,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"Following <span style=""background: yellow; display: inline-block"">Sproat and Jaitly (2016)</span>, we implement a seq2seq model trained on window-based data.",Uses,llm,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,1,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
958,3018,10332,ABC_1c89c8f4849d1c8214a3e5f6b9ff1a_14,ACL:N19-2024,eb5280aff90135c4c3a14f0bf6d6d298260a9887,"* TELEPHONE is not reported in <span style=""background: yellow; display: inline-block"">Sproat and Jaitly (2016)</span> but included in the dataset; ** we removed ELECTRONIC category.",Difference,llm,"Text normalization (TN) is an important step in conversational systems. It converts written text to its spoken form to facilitate speech recognition, natural language understanding and text-to-speech synthesis. Finite state transducers (FSTs) are commonly used to build grammars that handle text normalization. However, translating linguistic knowledge into grammars requires extensive effort. In this paper, we frame TN as a machine translation task and tackle it with sequence-to-sequence (seq2seq) models. Previous research focuses on normalizing a word (or phrase) with the help of limited word-level context, while our approach directly normalizes full sentences. We find subword models with additional linguistic features yield the best performance (with a word error rate of 0.17%).","This paper presents a challenge to the community: given a large corpus of written text aligned to its normalized spoken form, train an RNN to learn the correct normalization function. We present a data set of general text where the normalizations were generated using an existing text normalization component of a text-to-speech system. This data set will be released open-source in the near future. 
We also present our own experiments with this data set with a variety of different RNN architectures. While some of the architectures do in fact produce very good results when measured in terms of overall accuracy, the errors that are produced are problematic, since they would convey completely the wrong message if such a system were deployed in a speech application. On the other hand, we show that a simple FST-based filter can mitigate those errors, and achieve a level of accuracy not achievable by the RNN alone. 
Though our conclusions are largely negative on this point, we are actually not arguing that the text normalization problem is intractable using an pure RNN approach, merely that it is not going to be something that can be solved merely by having huge amounts of annotated text data and feeding that to a general RNN model. And when we open-source our data, we will be providing a novel data set for sequence-to-sequence modeling in the hopes that the the community can find better solutions. 
The data used in this work have been released and are available at: this https URL",test,0,"The research problem is the inefficiency of current methods, specifically Finite State Transducers (FSTs), for text normalization (TN) in conversational systems.  The existing methods require extensive effort to translate linguistic knowledge into grammars, and they often rely on limited word-level context for normalization. The motivation for this research stems from the need for efficient and accurate text normalization in conversational systems. This is crucial for facilitating speech recognition, natural language understanding, and text-to-speech synthesis.",dev,"conversational systems, Finite State Transducers ( FSTs ), grammars, limited word-level context, linguistic knowledge, methods, natural language understanding, normalization, speech recognition, text normalization, text normalization ( TN ), text-to-speech synthesis, they"
959,444,787,ABC_55f67c918001335974608200a87cfc_5,ARXIV:1910.08249,2b2090eab4abe27e6e5e4ca94afaf82e511b63bd,Table 1 shows the performance of our model compared to other models that also feature early fusion of the knowledge graph and text. These include Key-Value Memory Networks (KVMN) [3] and GRAFT-Net [17] . The results suggest that our model outperforms GRAFT-Net with an absolute increase in all metrics.,Difference,s2,"We introduce a relational graph neural network with bi-directional attention mechanism and hierarchical representation learning for open-domain question answering task. Our model can learn contextual representation by jointly learning and updating the query, knowledge graph, and document representations. The experiments suggest that our model achieves state-of-the-art on the WebQuestionsSP benchmark.","Existing question answering methods infer answers either from a knowledge base or from raw text. While knowledge base (KB) methods are good at answering compositional questions, their performance is often affected by the incompleteness of the KB. Au contraire, web text contains millions of facts that are absent in the KB, however in an unstructured form. Universal schema can support reasoning on the union of both structured KBs and unstructured text by aligning them in a common embedded space. In this paper we extend universal schema to natural language question answering, employing Memory networks to attend to the large body of facts in the combination of text and KB. Our models can be trained in an end-to-end fashion on question-answer pairs. Evaluation results on Spades fill-in-the-blank question answering dataset show that exploiting universal schema for question answering is better than using either a KB or text alone. This model also outperforms the current state-of-the-art by 8.5 F1 points.",train,0,"The research problem is addressing the challenge of open-domain question answering, which involves finding answers to questions from a vast and unstructured knowledge base. The research is motivated by the need for better methods to address the challenge of open-domain question answering, aiming to achieve state-of-the-art performance on relevant benchmarks like WebQuestionsSP.",train,"contextual representation, hierarchical representation learning, open-domain question answering task, relational graph neural network, WebQuestionsSP benchmark"
960,1747,3382,ABC_759c1c892361f62ad8f2c46e569e8a_18,ACL:W19-5908,fb32191ec07ba4d7badc76ca428c816995b5785a,"It has been widely used in the context of dialog policy learning (Fatemi et al., 2016; Dhingra et al., 2017; Casanueva et al., 2017) .",Background,s2,"In this paper, we explore state-of-the-art deep reinforcement learning methods for dialog policy training such as prioritized experience replay, double deep Q-Networks, dueling network architectures and distributional learning. Our main findings show that each individual method improves the rewards and the task success rate but combining these methods in a Rainbow agent, which performs best across tasks and environments, is a non-trivial task. We, therefore, provide insights about the influence of each method on the combination and how to combine them to form a Rainbow agent.","This paper proposes KB-InfoBot - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced “soft” posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents.",train,0,"The research focuses on improving dialog policy training by combining state-of-the-art deep reinforcement learning methods. Specifically, the challenge lies in effectively combining these methods into a Rainbow agent to achieve optimal performance across tasks and environments. The motivation behind the research is to improve the performance of dialog policy training by exploring the potential of combining advanced deep reinforcement learning methods into a single, effective Rainbow agent.",train,"deep reinforcement learning methods, dialog policy training, distributional learning, double deep Q-Networks, dueling network architectures, prioritized experience replay, Rainbow agent, Rainbow agent"
961,824,3732,ABC_9c8c5da4cdd13efb187690e7d3aa20_20,ACL:D19-1611,5a5a1076c3a3d4f7cbcc48a1e3f14307deec6b7a,"Previous work on paraphrase generation that used these datasets (Wang et al., 2019; Gupta et al., 2018; Li et al., 1 https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs 2018; Prakash et al., 2016) chose BLEU (Papineni et al., 2002) , METEOR (Lavie and Agarwal, 2007) and TER (Snover et al., 2006) as evaluation metrics.",Background,s2,"Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.","
 
 Paraphrase generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. In this paper, we address the problem of generating paraphrases automatically. Our proposed method is based on a combination of deep generative models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases, given an input sentence. Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence. We address this problem by conditioning the both, encoder and decoder sides of VAE, on the original sentence, so that it can generate the given sentence's paraphrases. Unlike most existing models, our model is simple, modular and can generate multiple paraphrases, for a given sentence. Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy, and its performance improvement over the state-of-the-art methods by a significant margin, whereas qualitative human evaluation indicate that the generated paraphrases are well-formed, grammatically correct, and are relevant to the input sentence. Furthermore, we evaluate our method on a newly released question paraphrase dataset, and establish a new baseline for future research.
 
",dev,0,"The research problem lies in the potential for paraphrase generation models to appear successful while actually producing trivial or no changes to the input sentence, leading to a concern about the evaluation metrics and the actual quality of generated paraphrases. The motivation stems from the practical applications of paraphrase generation and the need to critically evaluate the quality of generated paraphrases, ensuring meaningful change and not just superficial modifications.",train,"NLP task, Paraphrase generation, paraphrase generation research"
962,825,3733,ABC_9c8c5da4cdd13efb187690e7d3aa20_20,ACL:D19-1611,5a5a1076c3a3d4f7cbcc48a1e3f14307deec6b7a,"Gupta et al. (2018) sampled 4K sentences as their test set, but did not specify which sentences they used.",Background,s2,"Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.","
 
 Paraphrase generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. In this paper, we address the problem of generating paraphrases automatically. Our proposed method is based on a combination of deep generative models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases, given an input sentence. Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence. We address this problem by conditioning the both, encoder and decoder sides of VAE, on the original sentence, so that it can generate the given sentence's paraphrases. Unlike most existing models, our model is simple, modular and can generate multiple paraphrases, for a given sentence. Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy, and its performance improvement over the state-of-the-art methods by a significant margin, whereas qualitative human evaluation indicate that the generated paraphrases are well-formed, grammatically correct, and are relevant to the input sentence. Furthermore, we evaluate our method on a newly released question paraphrase dataset, and establish a new baseline for future research.
 
",dev,0,"The research problem lies in the potential for paraphrase generation models to appear successful while actually producing trivial or no changes to the input sentence, leading to a concern about the evaluation metrics and the actual quality of generated paraphrases. The motivation stems from the practical applications of paraphrase generation and the need to critically evaluate the quality of generated paraphrases, ensuring meaningful change and not just superficial modifications.",train,"NLP task, Paraphrase generation, paraphrase generation research"
963,826,3734,ABC_9c8c5da4cdd13efb187690e7d3aa20_20,ACL:D19-1611,5a5a1076c3a3d4f7cbcc48a1e3f14307deec6b7a,"However, relevance scores for captions of the same image score only 3.38 out of 5 under human evaluation (in contrast, the score is 4.82 for QUORA) (Gupta et al., 2018) , due to the fact that different captions for the same image often vary in the semantic information conveyed.",Background,s2,"Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.","
 
 Paraphrase generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. In this paper, we address the problem of generating paraphrases automatically. Our proposed method is based on a combination of deep generative models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases, given an input sentence. Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence. We address this problem by conditioning the both, encoder and decoder sides of VAE, on the original sentence, so that it can generate the given sentence's paraphrases. Unlike most existing models, our model is simple, modular and can generate multiple paraphrases, for a given sentence. Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy, and its performance improvement over the state-of-the-art methods by a significant margin, whereas qualitative human evaluation indicate that the generated paraphrases are well-formed, grammatically correct, and are relevant to the input sentence. Furthermore, we evaluate our method on a newly released question paraphrase dataset, and establish a new baseline for future research.
 
",dev,0,"The research problem lies in the potential for paraphrase generation models to appear successful while actually producing trivial or no changes to the input sentence, leading to a concern about the evaluation metrics and the actual quality of generated paraphrases. The motivation stems from the practical applications of paraphrase generation and the need to critically evaluate the quality of generated paraphrases, ensuring meaningful change and not just superficial modifications.",train,"NLP task, Paraphrase generation, paraphrase generation research"
964,827,3735,ABC_9c8c5da4cdd13efb187690e7d3aa20_20,ACL:D19-1611,5a5a1076c3a3d4f7cbcc48a1e3f14307deec6b7a,"There have been multiple works which use it as a paraphrase generation dataset by treating captions of the same image as paraphrases (Wang et al., 2019; Gupta et al., 2018; Prakash et al., 2016) .",Background,s2,"Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.","
 
 Paraphrase generation is an important problem in NLP, especially in question answering, information retrieval, information extraction, conversation systems, to name a few. In this paper, we address the problem of generating paraphrases automatically. Our proposed method is based on a combination of deep generative models (VAE) with sequence-to-sequence models (LSTM) to generate paraphrases, given an input sentence. Traditional VAEs when combined with recurrent neural networks can generate free text but they are not suitable for paraphrase generation for a given sentence. We address this problem by conditioning the both, encoder and decoder sides of VAE, on the original sentence, so that it can generate the given sentence's paraphrases. Unlike most existing models, our model is simple, modular and can generate multiple paraphrases, for a given sentence. Quantitative evaluation of the proposed method on a benchmark paraphrase dataset demonstrates its efficacy, and its performance improvement over the state-of-the-art methods by a significant margin, whereas qualitative human evaluation indicate that the generated paraphrases are well-formed, grammatically correct, and are relevant to the input sentence. Furthermore, we evaluate our method on a newly released question paraphrase dataset, and establish a new baseline for future research.
 
",dev,0,"The research problem lies in the potential for paraphrase generation models to appear successful while actually producing trivial or no changes to the input sentence, leading to a concern about the evaluation metrics and the actual quality of generated paraphrases. The motivation stems from the practical applications of paraphrase generation and the need to critically evaluate the quality of generated paraphrases, ensuring meaningful change and not just superficial modifications.",train,"NLP task, Paraphrase generation, paraphrase generation research"
965,828,3736,ABC_9c8c5da4cdd13efb187690e7d3aa20_20,ACL:D19-1611,a3be639d7e915b5f4e1499e52e1fcfd0940a31e5,"Consequently, although the exact test sets used by (Gupta et al., 2018) and (Li et al., 2018) are not available, it is logical to assume that parroting performance would still exceed or be on par with the state-of-the-art on those test sets.",Future Work,s2,"Paraphrase generation is an interesting and challenging NLP task which has numerous practical applications. In this paper, we analyze datasets commonly used for paraphrase generation research, and show that simply parroting input sentences surpasses state-of-the-art models in the literature when evaluated on standard metrics. Our findings illustrate that a model could be seemingly adept at generating paraphrases, despite only making trivial changes to the input sentence or even none at all.","Automatic generation of paraphrases from a given sentence is an important yet challenging task in natural language processing (NLP). In this paper, we present a deep reinforcement learning approach to paraphrase generation. Specifically, we propose a new framework for the task, which consists of a generator and an evaluator, both of which are learned from data. The generator, built as a sequence-to-sequence learning model, can produce paraphrases given a sentence. The evaluator, constructed as a deep matching model, can judge whether two sentences are paraphrases of each other. The generator is first trained by deep learning and then further fine-tuned by reinforcement learning in which the reward is given by the evaluator. For the learning of the evaluator, we propose two methods based on supervised learning and inverse reinforcement learning respectively, depending on the type of available training data. Experimental results on two datasets demonstrate the proposed models (the generators) can produce more accurate paraphrases and outperform the state-of-the-art methods in paraphrase generation in both automatic evaluation and human evaluation.",dev,0,"The research problem lies in the potential for paraphrase generation models to appear successful while actually producing trivial or no changes to the input sentence, leading to a concern about the evaluation metrics and the actual quality of generated paraphrases. The motivation stems from the practical applications of paraphrase generation and the need to critically evaluate the quality of generated paraphrases, ensuring meaningful change and not just superficial modifications.",train,"NLP task, Paraphrase generation, paraphrase generation research"
966,1469,6652,ABC_929020618e8e1daa6a769f552a4655_45,ARXIV:1912.05957,4a9dda304a41c5eb13066ce9ee0abdeee421709f,"Models proposed by Vajjala and Meurers [17] , Xia et al. [18] , and Mohammadi and Khasteh [19] are examples of state-of-the-art models for their target languages and target audience.",Background,s2,"Evaluating the readability of a text can significantly facilitate the precise expression of information in a written form. The formulation of text readability assessment demands the identification of meaningful properties of the text and correct conversion of features to the right readability level. Sophisticated features and models are being used to evaluate the comprehensibility of texts accurately. Still, these models are challenging to implement, heavily language-dependent, and do not perform well on short texts. Deep reinforcement learning models are demonstrated to be helpful in further improvement of state-of-the-art text readability assessment models. The main contributions of the proposed approach are the automation of feature extraction, loosening the tight language dependency of text readability assessment task, and efficient use of text by finding the minimum portion of a text required to assess its readability. The experiments on Weebit, Cambridge Exams, and Persian readability datasets display the model's state-of-the-art precision, efficiency, and the capability to be applied to other languages.","An automated approach to text readability assessment is essential to a language and can be a powerful tool for improving the understandability of texts written and published in that language. However, the Persian language, which is spoken by over 110 million speakers, lacks such a system. Unlike other languages such as English, French, and Chinese, minimal research studies have been conducted to develop an accurate and reliable text readability assessment system for the Persian language. In the present research, the first Persian dataset for text readability assessment was gathered, and the first model for Persian text readability assessment using machine learning was introduced. The experiments revealed that this model was accurate and could assess the readability of Persian texts with a high degree of confidence. The results of this study can be used in several applications such as medical and educational text readability evaluation and have the potential to be the cornerstone of future studies in Persian text readability assessment.",dev,0,"Current text readability assessment methods face challenges in implementation, language dependency, and performance on short texts. This research aims to address these limitations by improving the accuracy and efficiency of text readability assessment models. The motivation for this research stems from the need for accurate and efficient text readability assessment across various languages and text lengths. The goal is to improve the comprehensibility of written information and facilitate the precise expression of ideas.",train,"Cambridge Exams, feature extraction, Persian readability datasets, reinforcement learning models, short texts, text readability assessment, text readability assessment models, text readability assessment task, Weebit"
967,1265,4470,ABC_09493a62815b4b826248d6d9be47cb_25,ACL:P19-2055,b271605f2c26f5f37e7283a101b9e05d63be6cd9,"These tasks are devoted to the normalization of (1) (Limsopatham and Collier, 2016) 73.39 ----CNN (Limsopatham and Collier, 2016) 81.41 ----RNN (Limsopatham and Collier, 2016) 79.98 ----Attentional Char-CNN (Niu et al., 2018) 84.65 ----Hierarchical Char-CNN (Han et al., 2017) - Table 2 : The performance of the proposed models and the state-of-the-art methods in terms of accuracy.",Background,s2,"In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This is a challenging task since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful neural networks such as recurrent neural networks and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform existing state of the art models.","One effective way to improve the state of the art is through competitions. Following the success of the Critical Assessment of protein Structure Prediction (CASP) in bioinformatics research, a number of challenge evaluations have been organized by the text-mining research community to assess and advance natural language processing (NLP) research for biomedicine. In this article, we review the different community challenge evaluations held from 2002 to 2014 and their respective tasks. Furthermore, we examine these challenge tasks through their targeted problems in NLP research and biomedical applications, respectively. Next, we describe the general workflow of organizing a Biomedical NLP (BioNLP) challenge and involved stakeholders (task organizers, task data producers, task participants and end users). Finally, we summarize the impact and contributions by taking into account different BioNLP challenges as a whole, followed by a discussion of their limitations and difficulties. We conclude with future trends in BioNLP challenge evaluations.",test,0,"The research tackles the medical concept normalization problem. This problem involves accurately mapping health-related entities mentioned in free-form text, particularly social media, to corresponding concepts within the standardized Unified Medical Language System (UMLS) thesaurus. The challenge lies in the varying use of medical terminology by healthcare professionals and the general public on social media. The motivation behind this research is to address the challenge of medical concept normalization, specifically in the context of social media text. This is crucial for understanding and analyzing health-related information shared by the general public, bridging the gap between diverse terminology used by professionals and the public.",train,"contextualized word representation models, free-form text, health care professionals, medical concept normalization problem, medical terminology, neural architectures, neural networks, recurrent neural networks, semantic representations of social media expressions, sequence learning problem, social media texts"
968,1267,4473,ABC_09493a62815b4b826248d6d9be47cb_25,ACL:P19-2055,b271605f2c26f5f37e7283a101b9e05d63be6cd9,"Limsopatham and Collier (2016) utilized convolutional neural networks (CNNs) for phrase normalization in user reviews, while Tutubalina et al. (2018) , Han et al. (2017) , and Belousov et al. (2017) applied recurrent neural networks (RNNs) to UGTs, achieving similar results.",Background,s2,"In this work, we consider the medical concept normalization problem, i.e., the problem of mapping a health-related entity mention in a free-form text to a concept in a controlled vocabulary, usually to the standard thesaurus in the Unified Medical Language System (UMLS). This is a challenging task since medical terminology is very different when coming from health care professionals or from the general public in the form of social media texts. We approach it as a sequence learning problem with powerful neural networks such as recurrent neural networks and contextualized word representation models trained to obtain semantic representations of social media expressions. Our experimental evaluation over three different benchmarks shows that neural architectures leverage the semantic meaning of the entity mention and significantly outperform existing state of the art models.","One effective way to improve the state of the art is through competitions. Following the success of the Critical Assessment of protein Structure Prediction (CASP) in bioinformatics research, a number of challenge evaluations have been organized by the text-mining research community to assess and advance natural language processing (NLP) research for biomedicine. In this article, we review the different community challenge evaluations held from 2002 to 2014 and their respective tasks. Furthermore, we examine these challenge tasks through their targeted problems in NLP research and biomedical applications, respectively. Next, we describe the general workflow of organizing a Biomedical NLP (BioNLP) challenge and involved stakeholders (task organizers, task data producers, task participants and end users). Finally, we summarize the impact and contributions by taking into account different BioNLP challenges as a whole, followed by a discussion of their limitations and difficulties. We conclude with future trends in BioNLP challenge evaluations.",test,0,"The research tackles the medical concept normalization problem. This problem involves accurately mapping health-related entities mentioned in free-form text, particularly social media, to corresponding concepts within the standardized Unified Medical Language System (UMLS) thesaurus. The challenge lies in the varying use of medical terminology by healthcare professionals and the general public on social media. The motivation behind this research is to address the challenge of medical concept normalization, specifically in the context of social media text. This is crucial for understanding and analyzing health-related information shared by the general public, bridging the gap between diverse terminology used by professionals and the public.",train,"contextualized word representation models, free-form text, health care professionals, medical concept normalization problem, medical terminology, neural architectures, neural networks, recurrent neural networks, semantic representations of social media expressions, sequence learning problem, social media texts"
969,1587,5602,ABC_fe2f22d3d25358b23d0b75a6edee57_33,ARXIV:1909.03716,e9e737485329d233d78069b7b7677b65e035c15d,"In order to compare our models with the existing coarse-grained entity features (NER) being used in literature (Zhou et al., 2017; Harrison and Walker, 2018) , we also report the following experiments.",Unsure,s2,"In this paper, we propose a method for incorporating world knowledge (linked entities and fine-grained entity types) into a neural question generation model. This world knowledge helps to encode additional information related to the entities present in the passage required to generate human-like questions. We evaluate our models on both SQuAD and MS MARCO to demonstrate the usefulness of the world knowledge features. The proposed world knowledge enriched question generation model is able to outperform the vanilla neural question generation model by 1.37 and 1.59 absolute BLEU 4 score on SQuAD and MS MARCO test dataset respectively.","Question Generation is the task of automatically creating questions from textual input. In this work we present a new Attentional Encoder–Decoder Recurrent Neural Network model for automatic question generation. Our model incorporates linguistic features and an additional sentence embedding to capture meaning at both sentence and word levels. The linguistic features are designed to capture information related to named entity recognition, word case, and entity coreference resolution. In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence. Our model achieves state of the art results of 19.98 Bleu_4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin. A human evaluation also shows that the added features improve the quality of the generated questions.",test,0,"The research problem is to improve the generation of human-like questions from given passages using a neural question generation model. The motivation for the research is to enhance the quality of generated questions by providing the model with additional information about the entities present in the passage, leading to more human-like questions.",train,"MS MARCO test dataset, neural question generation model, SQuAD, vanilla neural question generation model, world knowledge enriched question generation model"
970,1589,5604,ABC_fe2f22d3d25358b23d0b75a6edee57_33,ARXIV:1909.03716,e9e737485329d233d78069b7b7677b65e035c15d,"Inspired by the success of using linguistic features in (Zhou et al., 2017; Harrison and Walker, 2018) , we exploit word knowledge in the form of entity linking and fine-grained entity typing in the encoder of the network.",Unsure,s2,"In this paper, we propose a method for incorporating world knowledge (linked entities and fine-grained entity types) into a neural question generation model. This world knowledge helps to encode additional information related to the entities present in the passage required to generate human-like questions. We evaluate our models on both SQuAD and MS MARCO to demonstrate the usefulness of the world knowledge features. The proposed world knowledge enriched question generation model is able to outperform the vanilla neural question generation model by 1.37 and 1.59 absolute BLEU 4 score on SQuAD and MS MARCO test dataset respectively.","Question Generation is the task of automatically creating questions from textual input. In this work we present a new Attentional Encoder–Decoder Recurrent Neural Network model for automatic question generation. Our model incorporates linguistic features and an additional sentence embedding to capture meaning at both sentence and word levels. The linguistic features are designed to capture information related to named entity recognition, word case, and entity coreference resolution. In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence. Our model achieves state of the art results of 19.98 Bleu_4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin. A human evaluation also shows that the added features improve the quality of the generated questions.",test,0,"The research problem is to improve the generation of human-like questions from given passages using a neural question generation model. The motivation for the research is to enhance the quality of generated questions by providing the model with additional information about the entities present in the passage, leading to more human-like questions.",train,"MS MARCO test dataset, neural question generation model, SQuAD, vanilla neural question generation model, world knowledge enriched question generation model"
971,1590,5605,ABC_fe2f22d3d25358b23d0b75a6edee57_33,ARXIV:1909.03716,e9e737485329d233d78069b7b7677b65e035c15d,"In previous works (Zhou et al., 2017; Harrison and Walker, 2018) , named entity type features have been used. These features, however, only allow for the encoding of coarse level information such as knowledge of if an entity belongs to a set of predefined categories such as 'PERSON', 'LOCATION' and 'ORGANI-ZATION'.",Motivation,s2,"In this paper, we propose a method for incorporating world knowledge (linked entities and fine-grained entity types) into a neural question generation model. This world knowledge helps to encode additional information related to the entities present in the passage required to generate human-like questions. We evaluate our models on both SQuAD and MS MARCO to demonstrate the usefulness of the world knowledge features. The proposed world knowledge enriched question generation model is able to outperform the vanilla neural question generation model by 1.37 and 1.59 absolute BLEU 4 score on SQuAD and MS MARCO test dataset respectively.","Question Generation is the task of automatically creating questions from textual input. In this work we present a new Attentional Encoder–Decoder Recurrent Neural Network model for automatic question generation. Our model incorporates linguistic features and an additional sentence embedding to capture meaning at both sentence and word levels. The linguistic features are designed to capture information related to named entity recognition, word case, and entity coreference resolution. In addition our model uses a copying mechanism and a special answer signal that enables generation of numerous diverse questions on a given sentence. Our model achieves state of the art results of 19.98 Bleu_4 on a benchmark Question Generation dataset, outperforming all previously published results by a significant margin. A human evaluation also shows that the added features improve the quality of the generated questions.",test,0,"The research problem is to improve the generation of human-like questions from given passages using a neural question generation model. The motivation for the research is to enhance the quality of generated questions by providing the model with additional information about the entities present in the passage, leading to more human-like questions.",train,"MS MARCO test dataset, neural question generation model, SQuAD, vanilla neural question generation model, world knowledge enriched question generation model"
972,1784,6235,ABC_4a093e0ca9a499c19e4721366d88f6_39,ACL:P19-1162,5966d7c7f60898d610812e24c64d4d57855ad86a,"(Bolukbasi et al., 2016b ) defines a useful metric for identifying gender bias and (Caliskan-Islam et al., 2016) defines a metric called the WEAT score for evaluating unfair correlations with sentiment for various demographics in text.",Background,s2,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",test,0,The research addresses the problem of unintended demographic biases in word embedding models and the lack of interpretable methods for evaluating how these biases might lead to discrimination in downstream NLP applications. The motivation stems from the widespread use of word embeddings in NLP and the need to develop interpretable methods for evaluating biases in these models. The researchers aim to contribute to a better understanding of how biases in word embeddings can lead to discrimination and to provide tools for mitigating these biases.,train,"discrimination, discrimination in downstream NLP applications, embedding models, Natural Language Processing community, word embeddings, word embeddings"
973,1785,6236,ABC_4a093e0ca9a499c19e4721366d88f6_39,ACL:P19-1162,5966d7c7f60898d610812e24c64d4d57855ad86a,"Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names (Caliskan-Islam et al., 2016) .",Background,s2,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",test,0,The research addresses the problem of unintended demographic biases in word embedding models and the lack of interpretable methods for evaluating how these biases might lead to discrimination in downstream NLP applications. The motivation stems from the widespread use of word embeddings in NLP and the need to develop interpretable methods for evaluating biases in these models. The researchers aim to contribute to a better understanding of how biases in word embeddings can lead to discrimination and to provide tools for mitigating these biases.,train,"discrimination, discrimination in downstream NLP applications, embedding models, Natural Language Processing community, word embeddings, word embeddings"
974,1786,6237,ABC_4a093e0ca9a499c19e4721366d88f6_39,ACL:P19-1162,5d4af8c9321168f9ba7a501f33fb019fa2deaa22,"Much of the focus for mitigating unintended bias in NLP is either targeted at reducing gender stereotypes in text (Bolukbasi et al., 2016b,a; Zhao et al., 2017; Zhang et al., 2018) , or inequality of sentiment or toxicity for various protected groups (Caliskan-Islam et al., 2016; Bakarov, 2018; Dixon et al.; Garg et al., 2018; Kiritchenko and Mohammad, 2018) .",Background,s2,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Automatic machine learning systems can inadvertently accentuate and perpetuate inappropriate human biases. Past work on examining inappropriate biases has largely focused on just individual systems. Further, there is no benchmark dataset for examining inappropriate biases in systems. Here for the first time, we present the Equity Evaluation Corpus (EEC), which consists of 8,640 English sentences carefully chosen to tease out biases towards certain races and genders. We use the dataset to examine 219 automatic sentiment analysis systems that took part in a recent shared task, SemEval-2018 Task 1 ‘Affect in Tweets’. We find that several of the systems show statistically significant bias; that is, they consistently provide slightly higher sentiment intensity predictions for one race or one gender. We make the EEC freely available.",test,0,The research addresses the problem of unintended demographic biases in word embedding models and the lack of interpretable methods for evaluating how these biases might lead to discrimination in downstream NLP applications. The motivation stems from the widespread use of word embeddings in NLP and the need to develop interpretable methods for evaluating biases in these models. The researchers aim to contribute to a better understanding of how biases in word embeddings can lead to discrimination and to provide tools for mitigating these biases.,train,"discrimination, discrimination in downstream NLP applications, embedding models, Natural Language Processing community, word embeddings, word embeddings"
975,1787,6238,ABC_4a093e0ca9a499c19e4721366d88f6_39,ACL:P19-1162,5966d7c7f60898d610812e24c64d4d57855ad86a,"GloVe and Word2vec embeddings have been shown to contain unintended bias in (Bolukbasi et al., 2016a; Caliskan-Islam et al., 2016) .",Background,s2,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",test,0,The research addresses the problem of unintended demographic biases in word embedding models and the lack of interpretable methods for evaluating how these biases might lead to discrimination in downstream NLP applications. The motivation stems from the widespread use of word embeddings in NLP and the need to develop interpretable methods for evaluating biases in these models. The researchers aim to contribute to a better understanding of how biases in word embeddings can lead to discrimination and to provide tools for mitigating these biases.,train,"discrimination, discrimination in downstream NLP applications, embedding models, Natural Language Processing community, word embeddings, word embeddings"
976,1788,6239,ABC_4a093e0ca9a499c19e4721366d88f6_39,ACL:P19-1162,5966d7c7f60898d610812e24c64d4d57855ad86a,"Although the RNSB metric is not directly comparable to WEAT scores, these results are still consistent with some of the bias predicted by (Caliskan-Islam et al., 2016) .",Similar,s2,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",test,0,The research addresses the problem of unintended demographic biases in word embedding models and the lack of interpretable methods for evaluating how these biases might lead to discrimination in downstream NLP applications. The motivation stems from the widespread use of word embeddings in NLP and the need to develop interpretable methods for evaluating biases in these models. The researchers aim to contribute to a better understanding of how biases in word embeddings can lead to discrimination and to provide tools for mitigating these biases.,train,"discrimination, discrimination in downstream NLP applications, embedding models, Natural Language Processing community, word embeddings, word embeddings"
977,1789,6240,ABC_4a093e0ca9a499c19e4721366d88f6_39,ACL:P19-1162,5966d7c7f60898d610812e24c64d4d57855ad86a,"Unfortunately metrics like these leverage vector space arguments between only two identities at a time like man vs woman (Bolukbasi et al., 2016a) , or European American names vs. African American names (Caliskan-Islam et al., 2016) . Our framework and RNSB metric enable a clear evaluation of discrimination with respect to word embedding bias for a whole class of demographics.",Difference,s2,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",test,0,The research addresses the problem of unintended demographic biases in word embedding models and the lack of interpretable methods for evaluating how these biases might lead to discrimination in downstream NLP applications. The motivation stems from the widespread use of word embeddings in NLP and the need to develop interpretable methods for evaluating biases in these models. The researchers aim to contribute to a better understanding of how biases in word embeddings can lead to discrimination and to provide tools for mitigating these biases.,train,"discrimination, discrimination in downstream NLP applications, embedding models, Natural Language Processing community, word embeddings, word embeddings"
978,1790,6241,ABC_4a093e0ca9a499c19e4721366d88f6_39,ACL:P19-1162,5966d7c7f60898d610812e24c64d4d57855ad86a,"First, we compare the RNSB metric for 3 pretrained word embeddings, showing that our metric is consistent with other word embedding analysis like WEAT (Caliskan-Islam et al., 2016) .",Similar,s2,"Word embedding models have gained a lot of traction in the Natural Language Processing community, however, they suffer from unintended demographic biases. Most approaches to evaluate these biases rely on vector space based metrics like the Word Embedding Association Test (WEAT). While these approaches offer great geometric insights into unintended biases in the embedding vector space, they fail to offer an interpretable meaning for how the embeddings could cause discrimination in downstream NLP applications. In this work, we present a transparent framework and metric for evaluating discrimination across protected groups with respect to their word embedding bias. Our metric (Relative Negative Sentiment Bias, RNSB) measures fairness in word embeddings via the relative negative sentiment associated with demographic identity terms from various protected groups. We show that our framework and metric enable useful analysis into the bias in word embeddings.","Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",test,0,The research addresses the problem of unintended demographic biases in word embedding models and the lack of interpretable methods for evaluating how these biases might lead to discrimination in downstream NLP applications. The motivation stems from the widespread use of word embeddings in NLP and the need to develop interpretable methods for evaluating biases in these models. The researchers aim to contribute to a better understanding of how biases in word embeddings can lead to discrimination and to provide tools for mitigating these biases.,train,"discrimination, discrimination in downstream NLP applications, embedding models, Natural Language Processing community, word embeddings, word embeddings"
979,1029,4379,ABC_817576dbe36f79ac3e0031211f400d_24,ACL:2020.findings-emnlp.292,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"This architecture has been shown to be a general language model that could be fine-tuned with little data in a relatively efficient way for a very distinct range of tasks and still outperform previous architectures (Devlin et al., 2019) .",Background,s2,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",dev,0,"The research addresses the need for a high-performing Dutch BERT model, as previous approaches using multilingual BERT or earlier implementations of BERT for Dutch have shown limited success.  The research focuses on the challenge of creating a model that outperforms existing Dutch BERT models, particularly when dealing with smaller datasets. The motivation stems from the recognition that single-language BERT models outperform multilingual versions, creating a need for specialized models for languages like Dutch. The research aims to contribute a powerful pre-trained model that can support a wide range of Dutch NLP tasks and facilitate further research and development in the field.",test,"Dutch BERT model, Dutch language model, Dutch language tasks, Dutch NLP applications, Dutch NLP tasks, multilingual BERT, natural language tasks, pre-trained and fine-tuned models, Pre-trained language models, pre-trained model, RobBERT, RobBERT, robustly optimized BERT approach"
980,1030,4380,ABC_817576dbe36f79ac3e0031211f400d_24,ACL:2020.findings-emnlp.292,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"BERT (Devlin et al., 2019) improved over previous transformer models and recurrent networks by allowing the system to learn from input text in a bidirectional way, rather than only from left-to-right or the other way around.",Background,s2,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",dev,0,"The research addresses the need for a high-performing Dutch BERT model, as previous approaches using multilingual BERT or earlier implementations of BERT for Dutch have shown limited success.  The research focuses on the challenge of creating a model that outperforms existing Dutch BERT models, particularly when dealing with smaller datasets. The motivation stems from the recognition that single-language BERT models outperform multilingual versions, creating a need for specialized models for languages like Dutch. The research aims to contribute a powerful pre-trained model that can support a wide range of Dutch NLP tasks and facilitate further research and development in the field.",test,"Dutch BERT model, Dutch language model, Dutch language tasks, Dutch NLP applications, Dutch NLP tasks, multilingual BERT, natural language tasks, pre-trained and fine-tuned models, Pre-trained language models, pre-trained model, RobBERT, RobBERT, robustly optimized BERT approach"
981,1031,4381,ABC_817576dbe36f79ac3e0031211f400d_24,ACL:2020.findings-emnlp.292,a4d5e425cac0bf84c86c0c9f720b6339d6288ffa,"ZeroR (majority class) 66.70 mBERT (Devlin et al., 2019) 90.21 BERTje (de Vries et al., 2019) 94.94 RobBERT (ours) 98.03 RobBERT outperforms previous models as well as other BERT models both with as well as without fine-tuning (see Table 1 and Table 2 ).",Background,s2,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","The transformer-based pre-trained language model BERT has helped to improve state-of-the-art performance on many natural language processing (NLP) tasks. Using the same architecture and parameters, we developed and evaluated a monolingual Dutch BERT model called BERTje. Compared to the multilingual BERT model, which includes Dutch but is only based on Wikipedia text, BERTje is based on a large and diverse dataset of 2.4 billion tokens. BERTje consistently outperforms the equally-sized multilingual BERT model on downstream NLP tasks (part-of-speech tagging, named-entity recognition, semantic role labeling, and sentiment analysis). Our pre-trained Dutch BERT model is made available at this https URL.",dev,0,"The research addresses the need for a high-performing Dutch BERT model, as previous approaches using multilingual BERT or earlier implementations of BERT for Dutch have shown limited success.  The research focuses on the challenge of creating a model that outperforms existing Dutch BERT models, particularly when dealing with smaller datasets. The motivation stems from the recognition that single-language BERT models outperform multilingual versions, creating a need for specialized models for languages like Dutch. The research aims to contribute a powerful pre-trained model that can support a wide range of Dutch NLP tasks and facilitate further research and development in the field.",test,"Dutch BERT model, Dutch language model, Dutch language tasks, Dutch NLP applications, Dutch NLP tasks, multilingual BERT, natural language tasks, pre-trained and fine-tuned models, Pre-trained language models, pre-trained model, RobBERT, RobBERT, robustly optimized BERT approach"
982,1032,4382,ABC_817576dbe36f79ac3e0031211f400d_24,ACL:2020.findings-emnlp.292,809cc93921e4698bde891475254ad6dfba33d03b,"For example, Multilingual-BERT is trained on a collection of corpora in 104 different languages (Devlin et al., 2019) , and generalizes language components well across languages (Pires et al., 2019) .",Background,s2,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",dev,0,"The research addresses the need for a high-performing Dutch BERT model, as previous approaches using multilingual BERT or earlier implementations of BERT for Dutch have shown limited success.  The research focuses on the challenge of creating a model that outperforms existing Dutch BERT models, particularly when dealing with smaller datasets. The motivation stems from the recognition that single-language BERT models outperform multilingual versions, creating a need for specialized models for languages like Dutch. The research aims to contribute a powerful pre-trained model that can support a wide range of Dutch NLP tasks and facilitate further research and development in the field.",test,"Dutch BERT model, Dutch language model, Dutch language tasks, Dutch NLP applications, Dutch NLP tasks, multilingual BERT, natural language tasks, pre-trained and fine-tuned models, Pre-trained language models, pre-trained model, RobBERT, RobBERT, robustly optimized BERT approach"
983,1438,6521,ABC_cc5927700475b7abc0482a28ab209a_43,ACL:2020.repl4nlp-1.19,a2d407962bb1f5fcd209114f5687d4c11bf9dfad,"Subsequently, there has been emphasis on post-processing the embeddings to improve their performance on downstream tasks (Mu and Viswanath, 2018) or to induce linguistic properties (Mrkšic et al.; Faruqui et al., 2015) .",Background,s2,"Word embeddings have become a staple of several natural language processing tasks, yet much remains to be understood about their properties. In this work, we analyze word embeddings in terms of their principal components and arrive at a number of novel and counterintuitive observations. In particular, we characterize the utility of variance explained by the principal components as a proxy for downstream performance. Furthermore, through syntactic probing of the principal embedding space, we show that the syntactic information captured by a principal component does not correlate with the amount of variance it explains. Consequently, we investigate the limitations of variance based embedding post-processing algorithms and demonstrate that such post-processing is counter-productive in sentence classification and machine translation tasks. Finally, we offer a few precautionary guidelines on applying variance based embedding post-processing and explain why non-isotropic geometry might be integral to word embedding performance.","Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a {\em very simple}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations {\em even stronger}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and { text classification}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.",dev,0,"The research addresses the lack of understanding about the properties of word embeddings, particularly their relationship to downstream performance and the effectiveness of variance-based post-processing algorithms. The motivation is to gain a deeper understanding of word embeddings, particularly their properties and the impact of post-processing algorithms on their performance. The goal is to provide insights and guidelines for effectively utilizing and improving word embedding models for natural language processing tasks.",test,"natural language processing tasks, principal component, principal components, syntactic probing of the principal embedding space, variance based embedding post-processing, variance based embedding post-processing algorithms, Word embeddings"
984,1439,6522,ABC_cc5927700475b7abc0482a28ab209a_43,ACL:2020.repl4nlp-1.19,151b459fd2f47de1f24af7380aa290e79f01b0b9,"In particular, the Principal Component Analysis (PCA) based post-processing algorithm proposed by (Mu and Viswanath, 2018) has led to significant gains in word and sentence similarity tasks, and has also proved useful in dimensionality reduction (Raunak, 2017) .",Background,s2,"Word embeddings have become a staple of several natural language processing tasks, yet much remains to be understood about their properties. In this work, we analyze word embeddings in terms of their principal components and arrive at a number of novel and counterintuitive observations. In particular, we characterize the utility of variance explained by the principal components as a proxy for downstream performance. Furthermore, through syntactic probing of the principal embedding space, we show that the syntactic information captured by a principal component does not correlate with the amount of variance it explains. Consequently, we investigate the limitations of variance based embedding post-processing algorithms and demonstrate that such post-processing is counter-productive in sentence classification and machine translation tasks. Finally, we offer a few precautionary guidelines on applying variance based embedding post-processing and explain why non-isotropic geometry might be integral to word embedding performance.","Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on improving the pretrained word vectors through post-processing algorithms. One improvement area is reducing the dimensionality of word embeddings. Reducing the size of word embeddings can improve their utility in memory constrained devices, benefiting several real world applications. In this work, we present a novel technique that efficiently combines PCA based dimensionality reduction with a recently proposed post-processing algorithm (Mu and Viswanath, 2018), to construct effective word embeddings of lower dimensions. Empirical evaluations on several benchmarks show that our algorithm efficiently reduces the embedding size while achieving similar or (more often) better performance than original embeddings. We have released the source code along with this paper.",dev,0,"The research addresses the lack of understanding about the properties of word embeddings, particularly their relationship to downstream performance and the effectiveness of variance-based post-processing algorithms. The motivation is to gain a deeper understanding of word embeddings, particularly their properties and the impact of post-processing algorithms on their performance. The goal is to provide insights and guidelines for effectively utilizing and improving word embedding models for natural language processing tasks.",test,"natural language processing tasks, principal component, principal components, syntactic probing of the principal embedding space, variance based embedding post-processing, variance based embedding post-processing algorithms, Word embeddings"
985,2386,10524,ABC_817576dbe36f79ac3e0031211f400d_24,ACL:2020.findings-emnlp.292,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"This cornerstone was used for BERT, a transformer model that obtained stateof-the-art results for eleven natural language processing tasks, such as question answering and natural language inference <span style=""background: yellow; display: inline-block"">(Devlin et al., 2019)</span> .",Background,llm,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",dev,0,"The research addresses the need for a high-performing Dutch BERT model, as previous approaches using multilingual BERT or earlier implementations of BERT for Dutch have shown limited success.  The research focuses on the challenge of creating a model that outperforms existing Dutch BERT models, particularly when dealing with smaller datasets. The motivation stems from the recognition that single-language BERT models outperform multilingual versions, creating a need for specialized models for languages like Dutch. The research aims to contribute a powerful pre-trained model that can support a wide range of Dutch NLP tasks and facilitate further research and development in the field.",test,"Dutch BERT model, Dutch language model, Dutch language tasks, Dutch NLP applications, Dutch NLP tasks, multilingual BERT, natural language tasks, pre-trained and fine-tuned models, Pre-trained language models, pre-trained model, RobBERT, RobBERT, robustly optimized BERT approach"
986,2387,10525,ABC_817576dbe36f79ac3e0031211f400d_24,ACL:2020.findings-emnlp.292,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"That<span style=""background: yellow; display: inline-block""> Devlin et al. (2019)</span> trained a better model when using NSP than without NSP is likely due to the model learning long-range dependencies in text from its inputs, which are longer than just the single sentence on itself.",Background,llm,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",dev,0,"The research addresses the need for a high-performing Dutch BERT model, as previous approaches using multilingual BERT or earlier implementations of BERT for Dutch have shown limited success.  The research focuses on the challenge of creating a model that outperforms existing Dutch BERT models, particularly when dealing with smaller datasets. The motivation stems from the recognition that single-language BERT models outperform multilingual versions, creating a need for specialized models for languages like Dutch. The research aims to contribute a powerful pre-trained model that can support a wide range of Dutch NLP tasks and facilitate further research and development in the field.",test,"Dutch BERT model, Dutch language model, Dutch language tasks, Dutch NLP applications, Dutch NLP tasks, multilingual BERT, natural language tasks, pre-trained and fine-tuned models, Pre-trained language models, pre-trained model, RobBERT, RobBERT, robustly optimized BERT approach"
987,2388,10526,ABC_817576dbe36f79ac3e0031211f400d_24,ACL:2020.findings-emnlp.292,df2b0e26d0599ce3e70df8a9da02e51594e0e992,"The architecture of our language model is thus equal to the original BERT model with 12 self-attention layers with 12 heads <span style=""background: yellow; display: inline-block"">(Devlin et al., 2019)</span> .",Similar,llm,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",dev,0,"The research addresses the need for a high-performing Dutch BERT model, as previous approaches using multilingual BERT or earlier implementations of BERT for Dutch have shown limited success.  The research focuses on the challenge of creating a model that outperforms existing Dutch BERT models, particularly when dealing with smaller datasets. The motivation stems from the recognition that single-language BERT models outperform multilingual versions, creating a need for specialized models for languages like Dutch. The research aims to contribute a powerful pre-trained model that can support a wide range of Dutch NLP tasks and facilitate further research and development in the field.",test,"Dutch BERT model, Dutch language model, Dutch language tasks, Dutch NLP applications, Dutch NLP tasks, multilingual BERT, natural language tasks, pre-trained and fine-tuned models, Pre-trained language models, pre-trained model, RobBERT, RobBERT, robustly optimized BERT approach"
988,1192,4202,ABC_4b65a59fc2331b9771ea09a12f32de_23,ACL:D19-1677,74157ae408173bf713f1e94f15aca1475c43bd74,"Recently, Locascio et al. (2016) designed the Deep-Regex model based on the sequence-to-sequence (Seq2Seq) model (Sutskever et al., 2014) using minimal domain knowledge during the learning phase while still accurately predicting regular expressions from NLs.",Background,s2,"We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.","This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",test,0,"The research problem is the complexity involved in generating semantically correct regular expressions from natural language descriptions. Specifically, the PSPACE-complete nature of regular expression equivalence poses a challenge for efficient training and generation. The research is motivated by the limitations of existing methods, like SemRegex, which rely on semantic equivalence checking for regular expressions. This complexity leads to long training times and hinders the development of more efficient models. The research aims to overcome this limitation by introducing a new approach that utilizes similarity computation instead of strict equivalence, leading to faster training and better performance.",test,"EQ_Reg mod-el, EQ_Reg model, EQ_Reg model, generating se-mantically correct regular expressions, natural language descriptions ( NL ), regex generation model, regular expression equivalence problem, reinforced learning, SemRegex, SoftRegex, SoftRegex"
989,1193,4204,ABC_4b65a59fc2331b9771ea09a12f32de_23,ACL:D19-1677,74157ae408173bf713f1e94f15aca1475c43bd74,Datasets: Locascio et al. (2016) created a set of NL-RX pair data by arbitrarily creating and combining data in a tree form.,Background,s2,"We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.","This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",test,0,"The research problem is the complexity involved in generating semantically correct regular expressions from natural language descriptions. Specifically, the PSPACE-complete nature of regular expression equivalence poses a challenge for efficient training and generation. The research is motivated by the limitations of existing methods, like SemRegex, which rely on semantic equivalence checking for regular expressions. This complexity leads to long training times and hinders the development of more efficient models. The research aims to overcome this limitation by introducing a new approach that utilizes similarity computation instead of strict equivalence, leading to faster training and better performance.",test,"EQ_Reg mod-el, EQ_Reg model, EQ_Reg model, generating se-mantically correct regular expressions, natural language descriptions ( NL ), regex generation model, regular expression equivalence problem, reinforced learning, SemRegex, SoftRegex, SoftRegex"
990,1194,4205,ABC_4b65a59fc2331b9771ea09a12f32de_23,ACL:D19-1677,74157ae408173bf713f1e94f15aca1475c43bd74,"On the other hand, NL-RX-Synth is data generated automatically and NL-RX-Turk is made from ordinary people by paraphrasing NL descriptions in NL-RX-Synth using Mechanical Turk (Locascio et al., 2016) .",Background,s2,"We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.","This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",test,0,"The research problem is the complexity involved in generating semantically correct regular expressions from natural language descriptions. Specifically, the PSPACE-complete nature of regular expression equivalence poses a challenge for efficient training and generation. The research is motivated by the limitations of existing methods, like SemRegex, which rely on semantic equivalence checking for regular expressions. This complexity leads to long training times and hinders the development of more efficient models. The research aims to overcome this limitation by introducing a new approach that utilizes similarity computation instead of strict equivalence, leading to faster training and better performance.",test,"EQ_Reg mod-el, EQ_Reg model, EQ_Reg model, generating se-mantically correct regular expressions, natural language descriptions ( NL ), regex generation model, regular expression equivalence problem, reinforced learning, SemRegex, SoftRegex, SoftRegex"
991,1195,4206,ABC_4b65a59fc2331b9771ea09a12f32de_23,ACL:D19-1677,74157ae408173bf713f1e94f15aca1475c43bd74,"Specifically, there are some ambiguities since Locascio et al. (2016) tried to obtain data from machine-generated sentences.",Background,s2,"We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.","This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",test,0,"The research problem is the complexity involved in generating semantically correct regular expressions from natural language descriptions. Specifically, the PSPACE-complete nature of regular expression equivalence poses a challenge for efficient training and generation. The research is motivated by the limitations of existing methods, like SemRegex, which rely on semantic equivalence checking for regular expressions. This complexity leads to long training times and hinders the development of more efficient models. The research aims to overcome this limitation by introducing a new approach that utilizes similarity computation instead of strict equivalence, leading to faster training and better performance.",test,"EQ_Reg mod-el, EQ_Reg model, EQ_Reg model, generating se-mantically correct regular expressions, natural language descriptions ( NL ), regex generation model, regular expression equivalence problem, reinforced learning, SemRegex, SoftRegex, SoftRegex"
992,1196,4207,ABC_4b65a59fc2331b9771ea09a12f32de_23,ACL:D19-1677,74157ae408173bf713f1e94f15aca1475c43bd74,We now describe how Locascio et al. (2016) generated their synthetic regular expression data.,Background,s2,"We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.","This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",test,0,"The research problem is the complexity involved in generating semantically correct regular expressions from natural language descriptions. Specifically, the PSPACE-complete nature of regular expression equivalence poses a challenge for efficient training and generation. The research is motivated by the limitations of existing methods, like SemRegex, which rely on semantic equivalence checking for regular expressions. This complexity leads to long training times and hinders the development of more efficient models. The research aims to overcome this limitation by introducing a new approach that utilizes similarity computation instead of strict equivalence, leading to faster training and better performance.",test,"EQ_Reg mod-el, EQ_Reg model, EQ_Reg model, generating se-mantically correct regular expressions, natural language descriptions ( NL ), regex generation model, regular expression equivalence problem, reinforced learning, SemRegex, SoftRegex, SoftRegex"
993,1197,4208,ABC_4b65a59fc2331b9771ea09a12f32de_23,ACL:D19-1677,74157ae408173bf713f1e94f15aca1475c43bd74,"Locascio et al. (2016) proposed the Deep-Regex model based on Seq2Seq for generating regular expressions from natural language descriptions together with a dataset of 10,000 NL-RX pairs.",Background,s2,"We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.","This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",test,0,"The research problem is the complexity involved in generating semantically correct regular expressions from natural language descriptions. Specifically, the PSPACE-complete nature of regular expression equivalence poses a challenge for efficient training and generation. The research is motivated by the limitations of existing methods, like SemRegex, which rely on semantic equivalence checking for regular expressions. This complexity leads to long training times and hinders the development of more efficient models. The research aims to overcome this limitation by introducing a new approach that utilizes similarity computation instead of strict equivalence, leading to faster training and better performance.",test,"EQ_Reg mod-el, EQ_Reg model, EQ_Reg model, generating se-mantically correct regular expressions, natural language descriptions ( NL ), regex generation model, regular expression equivalence problem, reinforced learning, SemRegex, SoftRegex, SoftRegex"
994,1198,4209,ABC_4b65a59fc2331b9771ea09a12f32de_23,ACL:D19-1677,74157ae408173bf713f1e94f15aca1475c43bd74,"Similar to Locascio et al. (2016) , we randomly generate regular expression pairs up to depth three and label the equivalence between each pair.",Similar,s2,"We continue the study of generating se-mantically correct regular expressions from natural language descriptions (NL). The current state-of-the-art model SemRegex produces regular expressions from NLs by rewarding the reinforced learning based on the semantic (rather than syntactic) equivalence between two regular expressions. Since the regular expression equivalence problem is PSPACE-complete, we introduce the EQ_Reg model for computing the simi-larity of two regular expressions using deep neural networks. Our EQ_Reg mod-el essentially softens the equivalence of two regular expressions when used as a reward function. We then propose a new regex generation model, SoftRegex, us-ing the EQ_Reg model, and empirically demonstrate that SoftRegex substantially reduces the training time (by a factor of at least 3.6) and produces state-of-the-art results on three benchmark datasets.","This paper explores the task of translating natural language queries into regular expressions which embody their meaning. In contrast to prior work, the proposed neural model does not utilize domain-specific crafting, learning to translate directly from a parallel corpus. To fully explore the potential of neural models, we propose a methodology for collecting a large corpus of regular expression, natural language pairs. Our resulting model achieves a performance gain of 19.6% over previous state-of-the-art models.",test,0,"The research problem is the complexity involved in generating semantically correct regular expressions from natural language descriptions. Specifically, the PSPACE-complete nature of regular expression equivalence poses a challenge for efficient training and generation. The research is motivated by the limitations of existing methods, like SemRegex, which rely on semantic equivalence checking for regular expressions. This complexity leads to long training times and hinders the development of more efficient models. The research aims to overcome this limitation by introducing a new approach that utilizes similarity computation instead of strict equivalence, leading to faster training and better performance.",test,"EQ_Reg mod-el, EQ_Reg model, EQ_Reg model, generating se-mantically correct regular expressions, natural language descriptions ( NL ), regex generation model, regular expression equivalence problem, reinforced learning, SemRegex, SoftRegex, SoftRegex"
995,1735,3357,ABC_3356313ee5cdf186816cd6fecfce84_18,ARXIV:1911.02086,9c226faaaf155f97a6951a3adb8bbae2ef8d2c25,"In recent years, end-to-end architectures gained traction that directly classify keyword posterior probabilites based on the previously extracted features, e.g., [1, 2, 3, 4, 5] .",Background,s2,"Keyword Spotting (KWS) enables speech-based user interaction on smart devices. Always-on and battery-powered application scenarios for smart devices put constraints on hardware resources and power consumption, while also demanding high accuracy as well as real-time capability. Previous architectures first extracted acoustic features and then applied a neural network to classify keyword probabilities, optimizing towards memory footprint and execution time.Compared to previous publications, we took additional steps to reduce power and memory consumption without reducing classification accuracy. Power-consuming audio preprocessing and data transfer steps are eliminated by directly classifying from raw audio. For this, our end-to-end architecture extracts spectral features using parametrized Sinc-convolutions. Its memory footprint is further reduced by grouping depthwise separable convolutions. Our network achieves the competitive accuracy of 96.4% on Google’s Speech Commands test set with only 62k parameters.","Our application requires a keyword spotting system with a small memory footprint, low computational cost, and high precision. To meet these requirements, we propose a simple approach based on deep neural networks. A deep neural network is trained to directly predict the keyword(s) or subword units of the keyword(s) followed by a posterior handling method producing a final confidence score. Keyword recognition results achieve 45% relative improvement with respect to a competitive Hidden Markov Model-based system, while performance in the presence of babble noise shows 39% relative improvement.",train,0,"The research problem is to develop a Keyword Spotting (KWS) system that can operate efficiently on resource-constrained smart devices, specifically addressing the challenges of limited hardware resources, power consumption, and the need for high accuracy and real-time performance. The research is motivated by the limitations of previous KWS architectures, which prioritized memory and execution time at the expense of power consumption. The aim is to develop a more efficient KWS system for smart devices that addresses the need for both accuracy and resource efficiency.",dev,"accuracy, execution time, Keyword Spotting ( KWS ) system, KWS architectures, KWS system, limited hardware resources, memory, power consumption, real-time performance, resource efficiency, resource-constrained smart devices, smart devices"
996,1736,3358,ABC_3356313ee5cdf186816cd6fecfce84_18,ARXIV:1911.02086,a3d4dbd03355d6b4972d7cb9257ccccdd6d33923,"Regarding real-time capability, our model is designed to operate on a single-core microcontroller capable of 50 MOps per second [2] .",Unsure,s2,"Keyword Spotting (KWS) enables speech-based user interaction on smart devices. Always-on and battery-powered application scenarios for smart devices put constraints on hardware resources and power consumption, while also demanding high accuracy as well as real-time capability. Previous architectures first extracted acoustic features and then applied a neural network to classify keyword probabilities, optimizing towards memory footprint and execution time.Compared to previous publications, we took additional steps to reduce power and memory consumption without reducing classification accuracy. Power-consuming audio preprocessing and data transfer steps are eliminated by directly classifying from raw audio. For this, our end-to-end architecture extracts spectral features using parametrized Sinc-convolutions. Its memory footprint is further reduced by grouping depthwise separable convolutions. Our network achieves the competitive accuracy of 96.4% on Google’s Speech Commands test set with only 62k parameters.","Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4%, which is ~10% higher than the DNN model with similar number of parameters.",train,0,"The research problem is to develop a Keyword Spotting (KWS) system that can operate efficiently on resource-constrained smart devices, specifically addressing the challenges of limited hardware resources, power consumption, and the need for high accuracy and real-time performance. The research is motivated by the limitations of previous KWS architectures, which prioritized memory and execution time at the expense of power consumption. The aim is to develop a more efficient KWS system for smart devices that addresses the need for both accuracy and resource efficiency.",dev,"accuracy, execution time, Keyword Spotting ( KWS ) system, KWS architectures, KWS system, limited hardware resources, memory, power consumption, real-time performance, resource efficiency, resource-constrained smart devices, smart devices"
997,1737,3361,ABC_3356313ee5cdf186816cd6fecfce84_18,ARXIV:1911.02086,a3d4dbd03355d6b4972d7cb9257ccccdd6d33923,"Compared to the DSConv network in [2] , our network is more efficient in terms of accuracy for a given parameter count.",Difference,s2,"Keyword Spotting (KWS) enables speech-based user interaction on smart devices. Always-on and battery-powered application scenarios for smart devices put constraints on hardware resources and power consumption, while also demanding high accuracy as well as real-time capability. Previous architectures first extracted acoustic features and then applied a neural network to classify keyword probabilities, optimizing towards memory footprint and execution time.Compared to previous publications, we took additional steps to reduce power and memory consumption without reducing classification accuracy. Power-consuming audio preprocessing and data transfer steps are eliminated by directly classifying from raw audio. For this, our end-to-end architecture extracts spectral features using parametrized Sinc-convolutions. Its memory footprint is further reduced by grouping depthwise separable convolutions. Our network achieves the competitive accuracy of 96.4% on Google’s Speech Commands test set with only 62k parameters.","Keyword spotting (KWS) is a critical component for enabling speech based user interactions on smart devices. It requires real-time response and high accuracy for good user experience. Recently, neural networks have become an attractive choice for KWS architecture because of their superior accuracy compared to traditional speech processing algorithms. Due to its always-on nature, KWS application has highly constrained power budget and typically runs on tiny microcontrollers with limited memory and compute capability. The design of neural network architecture for KWS must consider these constraints. In this work, we perform neural network architecture evaluation and exploration for running KWS on resource-constrained microcontrollers. We train various neural network architectures for keyword spotting published in literature to compare their accuracy and memory/compute requirements. We show that it is possible to optimize these neural network architectures to fit within the memory and compute constraints of microcontrollers without sacrificing accuracy. We further explore the depthwise separable convolutional neural network (DS-CNN) and compare it against other neural network architectures. DS-CNN achieves an accuracy of 95.4%, which is ~10% higher than the DNN model with similar number of parameters.",train,0,"The research problem is to develop a Keyword Spotting (KWS) system that can operate efficiently on resource-constrained smart devices, specifically addressing the challenges of limited hardware resources, power consumption, and the need for high accuracy and real-time performance. The research is motivated by the limitations of previous KWS architectures, which prioritized memory and execution time at the expense of power consumption. The aim is to develop a more efficient KWS system for smart devices that addresses the need for both accuracy and resource efficiency.",dev,"accuracy, execution time, Keyword Spotting ( KWS ) system, KWS architectures, KWS system, limited hardware resources, memory, power consumption, real-time performance, resource efficiency, resource-constrained smart devices, smart devices"
998,2710,5568,ABC_cbe9e36f371c072432ca25800c96d3_33,ARXIV:1907.04355,2f803165d054ee89bec2401368ceb9e75bad8b60,"Furthermore, this learning can take place jointly with ASR [15] , or separately with some tasks that have aligned objectives [16, 17, 18] .",Background,s2,"Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains.","This paper proposes a novel unsupervised autoregressive neural model for learning generic speech representations. In contrast to other speech representation learning methods that aim to remove noise or speaker variabilities, ours is designed to preserve information for a wide range of downstream tasks. In addition, the proposed model does not require any phonetic or word boundary labels, allowing the model to benefit from large quantities of unlabeled data. Speech representations learned by our model significantly improve performance on both phone classification and speaker verification over the surface features and other supervised and unsupervised approaches. Further analysis shows that different levels of speech information are captured by our model at different layers. In particular, the lower layers tend to be more discriminative for speakers, while the upper layers provide more phonetic content.",train,0,"This paper focuses on the problem of distilling robust phonetic features from grounding models trained on image-speech correlation, without using any textual transcripts, to enable transfer learning for speech recognition. The motivation lies in developing a more efficient transfer learning approach for speech recognition by leveraging grounding models that learn to extract robust phonetic features, independent of speaker and channel characteristics, without requiring specific speech recognition data during training. This approach aims to improve the universality and applicability of transfer learning for speech recognition across diverse domains.",dev,"approach, grounding models, image-speech correlation, robust phonetic features, speaker and channel characteristics, speech recognition, speech recognition data, textual transcripts, transfer learning, transfer learning approach"
999,2712,5570,ABC_cbe9e36f371c072432ca25800c96d3_33,ARXIV:1907.04355,cd30c240261fd61aaa5265cbb2a799f5adb9509a,"While previous work investigated usage of FHVAE for ASR by training FHVAE models on all domains of the target task (e.g., Aurora-4 with all four conditions) [17, 8] , we also evaluate FHVAE models trained on PlacesAudCap to test cross-dataset transferability, and on the subset of domains used for ASR training.",Background,s2,"Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains.","The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment -level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41 % and 27% absolute word error rate reductions respectively on mismatched domains.",train,0,"This paper focuses on the problem of distilling robust phonetic features from grounding models trained on image-speech correlation, without using any textual transcripts, to enable transfer learning for speech recognition. The motivation lies in developing a more efficient transfer learning approach for speech recognition by leveraging grounding models that learn to extract robust phonetic features, independent of speaker and channel characteristics, without requiring specific speech recognition data during training. This approach aims to improve the universality and applicability of transfer learning for speech recognition across diverse domains.",dev,"approach, grounding models, image-speech correlation, robust phonetic features, speaker and channel characteristics, speech recognition, speech recognition data, textual transcripts, transfer learning, transfer learning approach"
1000,2713,5571,ABC_cbe9e36f371c072432ca25800c96d3_33,ARXIV:1907.04355,cd30c240261fd61aaa5265cbb2a799f5adb9509a,"Other models capable of disentangling phonetic and domain information have recently been shown to learn acoustic features with a greater degree of domain invariance than traditional acoustic features [16, 7, 17] .",Background,s2,"Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains.","The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment -level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41 % and 27% absolute word error rate reductions respectively on mismatched domains.",train,0,"This paper focuses on the problem of distilling robust phonetic features from grounding models trained on image-speech correlation, without using any textual transcripts, to enable transfer learning for speech recognition. The motivation lies in developing a more efficient transfer learning approach for speech recognition by leveraging grounding models that learn to extract robust phonetic features, independent of speaker and channel characteristics, without requiring specific speech recognition data during training. This approach aims to improve the universality and applicability of transfer learning for speech recognition across diverse domains.",dev,"approach, grounding models, image-speech correlation, robust phonetic features, speaker and channel characteristics, speech recognition, speech recognition data, textual transcripts, transfer learning, transfer learning approach"
1001,2714,5572,ABC_cbe9e36f371c072432ca25800c96d3_33,ARXIV:1907.04355,cd30c240261fd61aaa5265cbb2a799f5adb9509a,"To evaluate transfer learning performance, we consider three criteria: (1) inclusion of phonetic content, (2) exclusion of nuisance factors, and (3) transferrability across datasets. The first two are evaluated using a protocol similar to [17] , where an ASR model is trained on a set of domains, and evaluated on both in-domain and out-of-domain speech (relative to the training data).",Similar,s2,"Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains.","The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment -level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41 % and 27% absolute word error rate reductions respectively on mismatched domains.",train,0,"This paper focuses on the problem of distilling robust phonetic features from grounding models trained on image-speech correlation, without using any textual transcripts, to enable transfer learning for speech recognition. The motivation lies in developing a more efficient transfer learning approach for speech recognition by leveraging grounding models that learn to extract robust phonetic features, independent of speaker and channel characteristics, without requiring specific speech recognition data during training. This approach aims to improve the universality and applicability of transfer learning for speech recognition across diverse domains.",dev,"approach, grounding models, image-speech correlation, robust phonetic features, speaker and channel characteristics, speech recognition, speech recognition data, textual transcripts, transfer learning, transfer learning approach"
1002,2715,5573,ABC_cbe9e36f371c072432ca25800c96d3_33,ARXIV:1907.04355,cd30c240261fd61aaa5265cbb2a799f5adb9509a,"FHVAE learns to encode sequence-level and segment-level information into separate latent variables without supervision by optimizing an evidence lower bound derived from a factorized graphical model, and has been shown effective for extracting domain invariant ASR features [17] .",Background,s2,"Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains.","The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment -level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41 % and 27% absolute word error rate reductions respectively on mismatched domains.",train,0,"This paper focuses on the problem of distilling robust phonetic features from grounding models trained on image-speech correlation, without using any textual transcripts, to enable transfer learning for speech recognition. The motivation lies in developing a more efficient transfer learning approach for speech recognition by leveraging grounding models that learn to extract robust phonetic features, independent of speaker and channel characteristics, without requiring specific speech recognition data during training. This approach aims to improve the universality and applicability of transfer learning for speech recognition across diverse domains.",dev,"approach, grounding models, image-speech correlation, robust phonetic features, speaker and channel characteristics, speech recognition, speech recognition data, textual transcripts, transfer learning, transfer learning approach"
1003,2719,5577,ABC_cbe9e36f371c072432ca25800c96d3_33,ARXIV:1907.04355,cd30c240261fd61aaa5265cbb2a799f5adb9509a,"Similar to [17] , we use the clean set (A) for training ASR systems, and test on the four groups separately.",Similar,s2,"Transfer learning aims to reduce the amount of data required to excel at a new task by re-using the knowledge acquired from learning other related tasks. This paper proposes a novel transfer learning scenario, which distills robust phonetic features from grounding models that are trained to tell whether a pair of image and speech are semantically correlated, without using any textual transcripts. As semantics of speech are largely determined by its lexical content, grounding models learn to preserve phonetic information while disregarding uncorrelated factors, such as speaker and channel. To study the properties of features distilled from different layers, we use them as input separately to train multiple speech recognition models. Empirical results demonstrate that layers closer to input retain more phonetic information, while following layers exhibit greater invariance to domain shift. Moreover, while most previous studies include training data for speech recognition for feature extractor training, our grounding models are not trained on any of those data, indicating more universal applicability to new domains.","The performance of automatic speech recognition (ASR) systems can be significantly compromised by previously unseen conditions, which is typically due to a mismatch between training and testing distributions. In this paper, we address robustness by studying domain invariant features, such that domain information becomes transparent to ASR systems, resolving the mismatch problem. Specifically, we investigate a recent model, called the Factorized Hierarchical Variational Autoencoder (FHVAE). FHVAEs learn to factorize sequence-level and segment-level attributes into different latent variables without supervision. We argue that the set of latent variables that contain segment -level information is our desired domain invariant feature for ASR. Experiments are conducted on Aurora-4 and CHiME-4, which demonstrate 41 % and 27% absolute word error rate reductions respectively on mismatched domains.",train,0,"This paper focuses on the problem of distilling robust phonetic features from grounding models trained on image-speech correlation, without using any textual transcripts, to enable transfer learning for speech recognition. The motivation lies in developing a more efficient transfer learning approach for speech recognition by leveraging grounding models that learn to extract robust phonetic features, independent of speaker and channel characteristics, without requiring specific speech recognition data during training. This approach aims to improve the universality and applicability of transfer learning for speech recognition across diverse domains.",dev,"approach, grounding models, image-speech correlation, robust phonetic features, speaker and channel characteristics, speech recognition, speech recognition data, textual transcripts, transfer learning, transfer learning approach"
